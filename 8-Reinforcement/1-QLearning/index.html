
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Machine-Learning">
      
      
        <meta name="author" content="Ean Yang">
      
      
        <link rel="canonical" href="https://eanyang7.github.io/Machine-Learning/8-Reinforcement/1-QLearning/">
      
      
        <link rel="prev" href="../README.zh-cn/">
      
      
        <link rel="next" href="README.zh-cn/">
      
      
      <link rel="icon" href="../../assets/favicon.jpg">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.10">
    
    
      
        <title>Introduction to Reinforcement Learning and Q-Learning - Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.fad675c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#introduction-to-reinforcement-learning-and-q-learning" class="md-skip">
          Ë∑≥ËΩ¨Ëá≥
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="È°µÁúâ">
    <a href="../.." title="Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã" class="md-header__button md-logo" aria-label="Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã" data-md-component="logo">
      
  <img src="../../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Introduction to Reinforcement Learning and Q-Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple"  aria-label="ÂàáÊç¢‰∏∫ÊöóÈªëÊ®°Âºè"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="ÂàáÊç¢‰∏∫ÊöóÈªëÊ®°Âºè" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="red"  aria-label="ÂàáÊç¢‰∏∫ÊµÖËâ≤Ê®°Âºè"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="ÂàáÊç¢‰∏∫ÊµÖËâ≤Ê®°Âºè" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="ÊêúÁ¥¢" placeholder="ÊêúÁ¥¢" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Êü•Êâæ">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="ÂàÜ‰∫´" aria-label="ÂàÜ‰∫´" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Ê∏ÖÁ©∫ÂΩìÂâçÂÜÖÂÆπ" aria-label="Ê∏ÖÁ©∫ÂΩìÂâçÂÜÖÂÆπ" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Ê≠£Âú®ÂàùÂßãÂåñÊêúÁ¥¢ÂºïÊìé
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EanYang7/Machine-Learning" title="ÂâçÂæÄ‰ªìÂ∫ì" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github‰ªìÂ∫ì
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="ÂØºËà™Ê†è" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã" class="md-nav__button md-logo" aria-label="Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã" data-md-component="logo">
      
  <img src="../../assets/logo.jpg" alt="logo">

    </a>
    Êú∫Âô®Â≠¶‰π† Âú®Á∫øÊïôÁ®ã
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EanYang7/Machine-Learning" title="ÂâçÂæÄ‰ªìÂ∫ì" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github‰ªìÂ∫ì
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Êú∫Âô®Â≠¶‰π†ËØæÁ®ã
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../1-Introduction/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    1 Introduction
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../2-Regression/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    2 Regression
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../3-Web-App/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    3 Web App
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../4-Classification/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    4 Classification
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../5-Clustering/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    5 Clustering
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../6-NLP/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    6 NLP
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../7-TimeSeries/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    7 TimeSeries
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    8 Reinforcement
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            8 Reinforcement
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to reinforcement learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../README.zh-cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Âº∫ÂåñÂ≠¶‰π†ÁÆÄ‰ªã
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9_3" checked>
        
          
          <label class="md-nav__link" for="__nav_9_3" id="__nav_9_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    1 QLearning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_9_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_9_3">
            <span class="md-nav__icon md-icon"></span>
            1 QLearning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Introduction to Reinforcement Learning and Q-Learning
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Introduction to Reinforcement Learning and Q-Learning
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pre-lecture-quiz" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-lecture quiz
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites-and-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites and Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      The environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#actions-and-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Actions and policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-walk" class="md-nav__link">
    <span class="md-ellipsis">
      Random walk
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-function" class="md-nav__link">
    <span class="md-ellipsis">
      Reward function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Q-Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#essence-of-q-learning-bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Essence of Q-Learning: Bellman Equation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploit-vs-explore" class="md-nav__link">
    <span class="md-ellipsis">
      Exploit vs. explore
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Python implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-the-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Checking the policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#challenge" class="md-nav__link">
    <span class="md-ellipsis">
      üöÄChallenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#navigation" class="md-nav__link">
    <span class="md-ellipsis">
      Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#investigating-the-learning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Investigating the learning process
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#post-lecture-quiz" class="md-nav__link">
    <span class="md-ellipsis">
      Post-lecture quiz
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assignment" class="md-nav__link">
    <span class="md-ellipsis">
      Assignment
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="README.zh-cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Âº∫ÂåñÂ≠¶‰π†Âíå Q-Learning ‰ªãÁªç
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="assignment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A More Realistic World
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="assignment.zh-cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ‰∏Ä‰∏™Êõ¥ÁúüÂÆûÁöÑ‰∏ñÁïå
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    
  
  
    <a href="solution/Julia/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Solution
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../2-Gym/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    2 Gym
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../9-Real-World/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    9 Real World
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pre-lecture-quiz" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-lecture quiz
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites-and-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites and Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      The environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#actions-and-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Actions and policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-walk" class="md-nav__link">
    <span class="md-ellipsis">
      Random walk
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reward-function" class="md-nav__link">
    <span class="md-ellipsis">
      Reward function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Q-Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#essence-of-q-learning-bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      Essence of Q-Learning: Bellman Equation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploit-vs-explore" class="md-nav__link">
    <span class="md-ellipsis">
      Exploit vs. explore
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#python-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Python implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-the-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Checking the policy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#challenge" class="md-nav__link">
    <span class="md-ellipsis">
      üöÄChallenge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#navigation" class="md-nav__link">
    <span class="md-ellipsis">
      Navigation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#investigating-the-learning-process" class="md-nav__link">
    <span class="md-ellipsis">
      Investigating the learning process
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#post-lecture-quiz" class="md-nav__link">
    <span class="md-ellipsis">
      Post-lecture quiz
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assignment" class="md-nav__link">
    <span class="md-ellipsis">
      Assignment
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/EanYang7/Machine-Learning/tree/main/docs/8-Reinforcement/1-QLearning/README.md" title="ÁºñËæëÊ≠§È°µ" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/EanYang7/Machine-Learning/tree/main/docs/8-Reinforcement/1-QLearning/README.md" title="Êü•ÁúãÊú¨È°µÁöÑÊ∫ê‰ª£Á†Å" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<h1 id="introduction-to-reinforcement-learning-and-q-learning">Introduction to Reinforcement Learning and Q-Learning<a class="headerlink" href="#introduction-to-reinforcement-learning-and-q-learning" title="Permanent link">‚öìÔ∏é</a></h1>
<p><img alt="Summary of reinforcement in machine learning in a sketchnote" src="../../sketchnotes/ml-reinforcement.png" /></p>
<blockquote>
<p>Sketchnote by <a href="https://www.twitter.com/girlie_mac">Tomomi Imura</a></p>
</blockquote>
<p>Reinforcement learning involves three important concepts: the agent, some states, and a set of actions per state. By executing an action in a specified state, the agent is given a reward. Again imagine the computer game Super Mario. You are Mario, you are in a game level, standing next to a cliff edge. Above you is a coin. You being Mario, in a game level, at a specific position ... that's your state. Moving one step to the right (an action) will take you over the edge, and that would give you a low numerical score. However, pressing the jump button would let you score a point and you would stay alive. That's a positive outcome and that should award you a positive numerical score.</p>
<p>By using reinforcement learning and a simulator (the game), you can learn how to play the game to maximize the reward which is staying alive and scoring as many points as possible.</p>
<p><a href="https://www.youtube.com/watch?v=lDq_en8RNOo"><img alt="Intro to Reinforcement Learning" src="https://img.youtube.com/vi/lDq_en8RNOo/0.jpg" /></a></p>
<blockquote>
<p>üé• Click the image above to hear Dmitry discuss Reinforcement Learning</p>
</blockquote>
<h2 id="pre-lecture-quiz"><a href="https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/">Pre-lecture quiz</a><a class="headerlink" href="#pre-lecture-quiz" title="Permanent link">‚öìÔ∏é</a></h2>
<h2 id="prerequisites-and-setup">Prerequisites and Setup<a class="headerlink" href="#prerequisites-and-setup" title="Permanent link">‚öìÔ∏é</a></h2>
<p>In this lesson, we will be experimenting with some code in Python. You should be able to run the Jupyter Notebook code from this lesson, either on your computer or somewhere in the cloud.</p>
<p>You can open <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb">the lesson notebook</a> and walk through this lesson to build.</p>
<blockquote>
<p><strong>Note:</strong> If you are opening this code from the cloud, you also need to fetch the <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py"><code>rlboard.py</code></a> file, which is used in the notebook code. Add it to the same directory as the notebook.</p>
</blockquote>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">‚öìÔ∏é</a></h2>
<p>In this lesson, we will explore the world of <strong><a href="https://en.wikipedia.org/wiki/Peter_and_the_Wolf">Peter and the Wolf</a></strong>, inspired by a musical fairy tale by a Russian composer, <a href="https://en.wikipedia.org/wiki/Sergei_Prokofiev">Sergei Prokofiev</a>. We will use <strong>Reinforcement Learning</strong> to let Peter explore his environment, collect tasty apples and avoid meeting the wolf.</p>
<p><strong>Reinforcement Learning</strong> (RL) is a learning technique that allows us to learn an optimal behavior of an <strong>agent</strong> in some <strong>environment</strong> by running many experiments. An agent in this environment should have some <strong>goal</strong>, defined by a <strong>reward function</strong>.</p>
<h2 id="the-environment">The environment<a class="headerlink" href="#the-environment" title="Permanent link">‚öìÔ∏é</a></h2>
<p>For simplicity, let's consider Peter's world to be a square board of size <code>width</code> x <code>height</code>, like this:</p>
<p><img alt="Peter's Environment" src="images/environment.png" /></p>
<p>Each cell in this board can either be:</p>
<ul>
<li><strong>ground</strong>, on which Peter and other creatures can walk.</li>
<li><strong>water</strong>, on which you obviously cannot walk.</li>
<li>a <strong>tree</strong> or <strong>grass</strong>, a place where you can rest.</li>
<li>an <strong>apple</strong>, which represents something Peter would be glad to find in order to feed himself.</li>
<li>a <strong>wolf</strong>, which is dangerous and should be avoided.</li>
</ul>
<p>There is a separate Python module, <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py"><code>rlboard.py</code></a>, which contains the code to work with this environment. Because this code is not important for understanding our concepts, we will import the module and use it to create the sample board (code block 1):</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">rlboard</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span><span class="mi">8</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Board</span><span class="p">(</span><span class="n">width</span><span class="p">,</span><span class="n">height</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">randomize</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</code></pre></div>
<p>This code should print a picture of the environment similar to the one above.</p>
<h2 id="actions-and-policy">Actions and policy<a class="headerlink" href="#actions-and-policy" title="Permanent link">‚öìÔ∏é</a></h2>
<p>In our example, Peter's goal would be able to find an apple, while avoiding the wolf and other obstacles. To do this, he can essentially walk around until he finds an apple.</p>
<p>Therefore, at any position, he can choose between one of the following actions: up, down, left and right.</p>
<p>We will define those actions as a dictionary, and map them to pairs of corresponding coordinate changes. For example, moving right (<code>R</code>) would correspond to a pair <code>(1,0)</code>. (code block 2):</p>
<div class="highlight"><pre><span></span><code><span class="n">actions</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;U&quot;</span> <span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;D&quot;</span> <span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;L&quot;</span> <span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;R&quot;</span> <span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="p">}</span>
<span class="n">action_idx</span> <span class="o">=</span> <span class="p">{</span> <span class="n">a</span> <span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">actions</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="p">}</span>
</code></pre></div>
<p>To sum up, the strategy and goal of this scenario are as follows:</p>
<ul>
<li>
<p><strong>The strategy</strong>, of our agent (Peter) is defined by a so-called <strong>policy</strong>. A policy is a function that returns the action at any given state. In our case, the state of the problem is represented by the board, including the current position of the player.</p>
</li>
<li>
<p><strong>The goal</strong>, of reinforcement learning is to eventually learn a good policy that will allow us to solve the problem efficiently. However, as a baseline, let's consider the simplest policy called <strong>random walk</strong>.</p>
</li>
</ul>
<h2 id="random-walk">Random walk<a class="headerlink" href="#random-walk" title="Permanent link">‚öìÔ∏é</a></h2>
<p>Let's first solve our problem by implementing a random walk strategy. With random walk, we will randomly choose the next action from the allowed actions, until we reach the apple (code block 3).</p>
<ol>
<li>
<p>Implement the random walk with the below code:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">random_policy</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">actions</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">walk</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">policy</span><span class="p">,</span><span class="n">start_position</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># number of steps</span>
    <span class="c1"># set initial position</span>
    <span class="k">if</span> <span class="n">start_position</span><span class="p">:</span>
        <span class="n">m</span><span class="o">.</span><span class="n">human</span> <span class="o">=</span> <span class="n">start_position</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="n">m</span><span class="o">.</span><span class="n">random_start</span><span class="p">()</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">at</span><span class="p">()</span> <span class="o">==</span> <span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">apple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">n</span> <span class="c1"># success!</span>
        <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">at</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">wolf</span><span class="p">,</span> <span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">water</span><span class="p">]:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># eaten by wolf or drowned</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="n">policy</span><span class="p">(</span><span class="n">m</span><span class="p">)]</span>
            <span class="n">new_pos</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">move_pos</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">human</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">is_valid</span><span class="p">(</span><span class="n">new_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">new_pos</span><span class="p">)</span><span class="o">!=</span><span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">water</span><span class="p">:</span>
                <span class="n">m</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># do the actual move</span>
                <span class="k">break</span>
        <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>

<span class="n">walk</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">random_policy</span><span class="p">)</span>
</code></pre></div>
<p>The call to <code>walk</code> should return the length of the corresponding path, which can vary from one run to another. </p>
</li>
<li>
<p>Run the walk experiment a number of times (say, 100), and print the resulting statistics (code block 4):</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_statistics</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
    <span class="n">s</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">walk</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">z</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">w</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">z</span>
            <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average path length = </span><span class="si">{</span><span class="n">s</span><span class="o">/</span><span class="n">n</span><span class="si">}</span><span class="s2">, eaten by wolf: </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2"> times&quot;</span><span class="p">)</span>

<span class="n">print_statistics</span><span class="p">(</span><span class="n">random_policy</span><span class="p">)</span>
</code></pre></div>
<p>Note that the average length of a path is around 30-40 steps, which is quite a lot, given the fact that the average distance to the nearest apple is around 5-6 steps.</p>
<p>You can also see what Peter's movement looks like during the random walk:</p>
<p><img alt="Peter's Random Walk" src="images/random_walk.gif" /></p>
</li>
</ol>
<h2 id="reward-function">Reward function<a class="headerlink" href="#reward-function" title="Permanent link">‚öìÔ∏é</a></h2>
<p>To make our policy more intelligent, we need to understand which moves are "better" than others. To do this, we need to define our goal.</p>
<p>The goal can be defined in terms of a <strong>reward function</strong>, which will return some score value for each state. The higher the number, the better the reward function. (code block 5)</p>
<div class="highlight"><pre><span></span><code><span class="n">move_reward</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.1</span>
<span class="n">goal_reward</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">end_reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>

<span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">pos</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span> <span class="ow">or</span> <span class="n">m</span><span class="o">.</span><span class="n">human</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="o">.</span><span class="n">is_valid</span><span class="p">(</span><span class="n">pos</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">end_reward</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">water</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">wolf</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">end_reward</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="n">Board</span><span class="o">.</span><span class="n">Cell</span><span class="o">.</span><span class="n">apple</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">goal_reward</span>
    <span class="k">return</span> <span class="n">move_reward</span>
</code></pre></div>
<p>An interesting thing about reward functions is that in most cases, <em>we are only given a substantial reward at the end of the game</em>. This means that our algorithm should somehow remember "good" steps that lead to a positive reward at the end, and increase their importance. Similarly, all moves that lead to bad results should be discouraged.</p>
<h2 id="q-learning">Q-Learning<a class="headerlink" href="#q-learning" title="Permanent link">‚öìÔ∏é</a></h2>
<p>An algorithm that we will discuss here is called <strong>Q-Learning</strong>. In this algorithm, the policy is defined by a function (or a data structure) called a <strong>Q-Table</strong>. It records the "goodness" of each of the actions in a given state.</p>
<p>It is called a Q-Table because it is often convenient to represent it as a table, or multi-dimensional array. Since our board has dimensions <code>width</code> x <code>height</code>, we can represent the Q-Table using a numpy array with shape <code>width</code> x <code>height</code> x <code>len(actions)</code>: (code block 6)</p>
<div class="highlight"><pre><span></span><code><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">width</span><span class="p">,</span><span class="n">height</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">*</span><span class="mf">1.0</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
</code></pre></div>
<p>Notice that we initialize all the values of the Q-Table with an equal value, in our case - 0.25. This corresponds to the "random walk" policy, because all moves in each state are equally good. We can pass the Q-Table to the <code>plot</code> function in order to visualize the table on the board: <code>m.plot(Q)</code>.</p>
<p><img alt="Peter's Environment" src="images/env_init.png" /></p>
<p>In the center of each cell there is an "arrow" that indicates the preferred direction of movement. Since all directions are equal, a dot is displayed.</p>
<p>Now we need to run the simulation, explore our environment, and learn a better distribution of Q-Table values, which will allow us to find the path to the apple much faster.</p>
<h2 id="essence-of-q-learning-bellman-equation">Essence of Q-Learning: Bellman Equation<a class="headerlink" href="#essence-of-q-learning-bellman-equation" title="Permanent link">‚öìÔ∏é</a></h2>
<p>Once we start moving, each action will have a corresponding reward, i.e. we can theoretically select the next action based on the highest immediate reward. However, in most states, the move will not achieve our goal of reaching the apple, and thus we cannot immediately decide which direction is better.</p>
<blockquote>
<p>Remember that it is not the immediate result that matters, but rather the final result, which we will obtain at the end of the simulation.</p>
</blockquote>
<p>In order to account for this delayed reward, we need to use the principles of <strong><a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a></strong>, which allow us to think about out problem recursively.</p>
<p>Suppose we are now at the state <em>s</em>, and we want to move to the next state <em>s'</em>. By doing so, we will receive the immediate reward <em>r(s,a)</em>, defined by the reward function, plus some future reward. If we suppose that our Q-Table correctly reflects the "attractiveness" of each action, then at state <em>s'</em> we will chose an action <em>a</em> that corresponds to maximum value of <em>Q(s',a')</em>. Thus, the best possible future reward we could get at state <em>s</em> will be defined as <code>max</code><sub>a'</sub><em>Q(s',a')</em> (maximum here is computed over all possible actions <em>a'</em> at state <em>s'</em>).</p>
<p>This gives the <strong>Bellman formula</strong> for calculating the value of the Q-Table at state <em>s</em>, given action <em>a</em>:</p>
<p><img src="images/bellman-equation.png"/></p>
<p>Here Œ≥ is the so-called <strong>discount factor</strong> that determines to which extent you should prefer the current reward over the future reward and vice versa.</p>
<h2 id="learning-algorithm">Learning Algorithm<a class="headerlink" href="#learning-algorithm" title="Permanent link">‚öìÔ∏é</a></h2>
<p>Given the equation above, we can now write pseudo-code for our learning algorithm:</p>
<ul>
<li>Initialize Q-Table Q with equal numbers for all states and actions</li>
<li>Set learning rate Œ± ‚Üê 1</li>
<li>Repeat simulation many times</li>
<li>Start at random position</li>
<li>Repeat
        1. Select an action <em>a</em> at state <em>s</em>
        2. Execute action by moving to a new state <em>s'</em>
        3. If we encounter end-of-game condition, or total reward is too small - exit simulation<br />
        4. Compute reward <em>r</em> at the new state
        5. Update Q-Function according to Bellman equation: <em>Q(s,a)</em> ‚Üê <em>(1-Œ±)Q(s,a)+Œ±(r+Œ≥ max<sub>a'</sub>Q(s',a'))</em>
        6. <em>s</em> ‚Üê <em>s'</em>
        7. Update the total reward and decrease Œ±.</li>
</ul>
<h2 id="exploit-vs-explore">Exploit vs. explore<a class="headerlink" href="#exploit-vs-explore" title="Permanent link">‚öìÔ∏é</a></h2>
<p>In the algorithm above, we did not specify how exactly we should choose an action at step 2.1. If we are choosing the action randomly, we will randomly <strong>explore</strong> the environment, and we are quite likely to die often as well as explore areas where we would not normally go. An alternative approach would be to <strong>exploit</strong> the Q-Table values that we already know, and thus to choose the best action (with higher Q-Table value) at state <em>s</em>. This, however, will prevent us from exploring other states, and it's likely we might not find the optimal solution.</p>
<p>Thus, the best approach is to strike a balance between exploration and exploitation. This can be done by choosing the action at state <em>s</em> with probabilities proportional to values in the Q-Table. In the beginning, when Q-Table values are all the same, it would correspond to a random selection, but as we learn more about our environment, we would be more likely to follow the optimal route while allowing the agent to choose the unexplored path once in a while.</p>
<h2 id="python-implementation">Python implementation<a class="headerlink" href="#python-implementation" title="Permanent link">‚öìÔ∏é</a></h2>
<p>We are now ready to implement the learning algorithm. Before we do that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions.</p>
<ol>
<li>
<p>Create a function <code>probs()</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">probs</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">-</span><span class="n">v</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">+</span><span class="n">eps</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">v</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">v</span>
</code></pre></div>
<p>We add a few <code>eps</code> to the original vector in order to avoid division by 0 in the initial case, when all components of the vector are identical.</p>
</li>
</ol>
<p>Run them learning algorithm through 5000 experiments, also called <strong>epochs</strong>: (code block 8)
<div class="highlight"><pre><span></span><code>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>

        <span class="c1"># Pick initial point</span>
        <span class="n">m</span><span class="o">.</span><span class="n">random_start</span><span class="p">()</span>

        <span class="c1"># Start travelling</span>
        <span class="n">n</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">cum_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">human</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">probs</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span><span class="n">weights</span><span class="o">=</span><span class="n">v</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">dpos</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            <span class="n">m</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">dpos</span><span class="p">,</span><span class="n">check_correctness</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># we allow player to move outside the board, which terminates episode</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">reward</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
            <span class="n">cum_reward</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="k">if</span> <span class="n">r</span><span class="o">==</span><span class="n">end_reward</span> <span class="ow">or</span> <span class="n">cum_reward</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1000</span><span class="p">:</span>
                <span class="n">lpath</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">n</span> <span class="o">/</span> <span class="mf">10e5</span><span class="p">)</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="n">ai</span> <span class="o">=</span> <span class="n">action_idx</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
            <span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ai</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">ai</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="o">+</span><span class="n">dpos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">+</span><span class="n">dpos</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
            <span class="n">n</span><span class="o">+=</span><span class="mi">1</span>
</code></pre></div></p>
<p>After executing this algorithm, the Q-Table should be updated with values that define the attractiveness of different actions at each step. We can try to visualize the Q-Table by plotting a vector at each cell that will point in the desired direction of movement. For simplicity, we draw a small circle instead of an arrow head.</p>
<p><img src="images/learned.png"/></p>
<h2 id="checking-the-policy">Checking the policy<a class="headerlink" href="#checking-the-policy" title="Permanent link">‚öìÔ∏é</a></h2>
<p>Since the Q-Table lists the "attractiveness" of each action at each state, it is quite easy to use it to define the efficient navigation in our world. In the simplest case, we can select the action corresponding to the highest Q-Table value: (code block 9)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">qpolicy_strict</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">human</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">probs</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">a</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">actions</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">a</span>

<span class="n">walk</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">qpolicy_strict</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p>If you try the code above several times, you may notice that sometimes it "hangs", and you need to press the STOP button in the notebook to interrupt it. This happens because there could be situations when two states "point" to each other in terms of optimal Q-Value, in which case the agents ends up moving between those states indefinitely.</p>
</blockquote>
<h2 id="challenge">üöÄChallenge<a class="headerlink" href="#challenge" title="Permanent link">‚öìÔ∏é</a></h2>
<blockquote>
<p><strong>Task 1:</strong> Modify the <code>walk</code> function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.</p>
<p><strong>Task 2:</strong> Modify the <code>walk</code> function so that it does not go back to the places where it has already been previously. This will prevent <code>walk</code> from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.</p>
</blockquote>
<h2 id="navigation">Navigation<a class="headerlink" href="#navigation" title="Permanent link">‚öìÔ∏é</a></h2>
<p>A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that <code>print_statistics</code> runs the simulation 100 times): (code block 10)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">qpolicy</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">human</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">probs</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span><span class="n">weights</span><span class="o">=</span><span class="n">v</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">a</span>

<span class="n">print_statistics</span><span class="p">(</span><span class="n">qpolicy</span><span class="p">)</span>
</code></pre></div>
<p>After running this code, you should get a much smaller average path length than before, in the range of 3-6.</p>
<h2 id="investigating-the-learning-process">Investigating the learning process<a class="headerlink" href="#investigating-the-learning-process" title="Permanent link">‚öìÔ∏é</a></h2>
<p>As we have mentioned, the learning process is a balance between exploration and exploration of gained knowledge about the structure of problem space. We have seen that the results of learning (the ability to help an agent to find a short path to the goal) has improved, but it is also interesting to observe how the average path length behaves during the learning process:</p>
<p><img src="images/lpathlen1.png"/></p>
<p>The learnings can be summarized as:</p>
<ul>
<li>
<p><strong>Average path length increases</strong>. What we see here is that at first, the average path length increases. This is probably due to the fact that when we know nothing about the environment, we are likely to get trapped in bad states, water or wolf. As we learn more and start using this knowledge, we can explore the environment for longer, but we still do not know where the apples are very well.</p>
</li>
<li>
<p><strong>Path length decrease, as we learn more</strong>. Once we learn enough, it becomes easier for the agent to achieve the goal, and the path length starts to decrease. However, we are still open to exploration, so we often diverge away from the best path, and explore new options, making the path longer than optimal.</p>
</li>
<li>
<p><strong>Length increase abruptly</strong>. What we also observe on this graph is that at some point, the length increased abruptly. This indicates the stochastic nature of the process, and that we can at some point "spoil" the Q-Table coefficients by overwriting them with new values. This ideally should be minimized by decreasing learning rate (for example, towards the end of training, we only adjust Q-Table values by a small value).</p>
</li>
</ul>
<p>Overall, it is important to remember that the success and quality of the learning process significantly depends on parameters, such as learning rate, learning rate decay, and discount factor. Those are often called <strong>hyperparameters</strong>, to distinguish them from <strong>parameters</strong>, which we optimize during training (for example, Q-Table coefficients). The process of finding the best hyperparameter values is called <strong>hyperparameter optimization</strong>, and it deserves a separate topic.</p>
<h2 id="post-lecture-quiz"><a href="https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/">Post-lecture quiz</a><a class="headerlink" href="#post-lecture-quiz" title="Permanent link">‚öìÔ∏é</a></h2>
<h2 id="assignment">Assignment<a class="headerlink" href="#assignment" title="Permanent link">‚öìÔ∏é</a></h2>
<p><a href="assignment/">A More Realistic World</a></p>

  <hr>
<div class="md-source-file">
  <small>
    
      ÊúÄÂêéÊõ¥Êñ∞:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 22, 2023</span>
      
        <br>
        ÂàõÂª∫Êó•Êúü:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 22, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  ÂõûÂà∞È°µÈù¢È°∂ÈÉ®
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="È°µËÑö" >
        
          
          <a href="../README.zh-cn/" class="md-footer__link md-footer__link--prev" aria-label="‰∏ä‰∏ÄÈ°µ: Âº∫ÂåñÂ≠¶‰π†ÁÆÄ‰ªã">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                ‰∏ä‰∏ÄÈ°µ
              </span>
              <div class="md-ellipsis">
                Âº∫ÂåñÂ≠¶‰π†ÁÆÄ‰ªã
              </div>
            </div>
          </a>
        
        
          
          <a href="README.zh-cn/" class="md-footer__link md-footer__link--next" aria-label="‰∏ã‰∏ÄÈ°µ: Âº∫ÂåñÂ≠¶‰π†Âíå Q-Learning ‰ªãÁªç">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                ‰∏ã‰∏ÄÈ°µ
              </span>
              <div class="md-ellipsis">
                Âº∫ÂåñÂ≠¶‰π†Âíå Q-Learning ‰ªãÁªç
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Ean Yang
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/YQisme" target="_blank" rel="noopener" title="github‰∏ªÈ°µ" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://space.bilibili.com/244185393?spm_id_from=333.788.0.0" target="_blank" rel="noopener" title="bÁ´ô‰∏ªÈ°µ" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://eanyang7.com" target="_blank" rel="noopener" title="‰∏™‰∫∫‰∏ªÈ°µ" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M112 48a48 48 0 1 1 96 0 48 48 0 1 1-96 0zm40 304v128c0 17.7-14.3 32-32 32s-32-14.3-32-32V256.9l-28.6 47.6c-9.1 15.1-28.8 20-43.9 10.9s-20-28.8-10.9-43.9l58.3-97c17.4-28.9 48.6-46.6 82.3-46.6h29.7c33.7 0 64.9 17.7 82.3 46.6l58.3 97c9.1 15.1 4.2 34.8-10.9 43.9s-34.8 4.2-43.9-10.9L232 256.9V480c0 17.7-14.3 32-32 32s-32-14.3-32-32V352h-16z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.prune", "navigation.top", "toc.follow", "header.autohide", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6c14ae12.min.js"></script>
      
    
  </body>
</html>