{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b","text":"<p>\u5728microsoft/ML-For-Beginners\u7684\u57fa\u7840\u4e0a\uff0c\u521b\u5efa\u4e86\u4e2d\u6587\u5728\u7ebf\u5b66\u4e60\u7f51\u7ad9\u673a\u5668\u5b66\u4e60 \u5728\u7ebf\u6559\u7a0b </p> <p>12\u5468\u300126\u8282\u8bfe</p> <p>\u5728\u8fd9\u5957\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5173\u4e8e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u5185\u5bb9\uff0c\u4e3b\u8981\u5c06\u4f7f\u7528 Scikit-learn \u8fd9\u4e00\u5e93\u3002\u5173\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5185\u5bb9\u5c06\u4f1a\u5c3d\u91cf\u907f\u514d \u2014\u2014 \u5b83\u4f1a\u88ab\u6211\u4eec\u5373\u5c06\u63a8\u51fa\u7684 \"AI for Beginners (\u9488\u5bf9\u521d\u5b66\u8005\u7684 AI \u6559\u7a0b)\" \u6240\u6db5\u76d6\u3002\u4f60\u4e5f\u53ef\u4ee5\u628a\u8fd9\u4e9b\u8bfe\u548c\u6211\u4eec\u5df2\u63a8\u51fa\u7684 Data Science for Beginners\uff08\u9488\u5bf9\u521d\u5b66\u8005\u7684\u6570\u636e\u79d1\u5b66\u6559\u7a0b\uff09 \u76f8\u7ed3\u5408\uff01</p> <p>\u901a\u8fc7\u628a\u8fd9\u4e9b\u7ecf\u5178\u7684\u6280\u672f\u5e94\u7528\u5728\u6765\u81ea\u4e16\u754c\u5404\u5730\u7684\u6570\u636e\uff0c\u6211\u4eec\u5c06 \u201c\u73af\u6e38\u4e16\u754c\u201d\u3002\u6bcf\u4e00\u8282\u8bfe\u90fd\u5305\u62ec\u4e86\u8bfe\u524d\u548c\u8bfe\u540e\u6d4b\u9a8c\u3001\u8bfe\u7a0b\u5185\u5bb9\u7684\u6587\u5b57\u8bb2\u4e49\u8bf4\u660e\u3001\u793a\u4f8b\u4ee3\u7801\u3001\u4f5c\u4e1a\u7b49\u3002\u901a\u8fc7\u8fd9\u79cd\u57fa\u4e8e\u9879\u76ee\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u4f60\u5c06\u5728\u6784\u5efa\u4e2d\u5b66\u4e60\uff0c\u8fd9\u6837\u53ef\u4ee5\u628a\u6280\u80fd\u5b66\u5f97\u66f4\u7262\u9760\u3002</p>"},{"location":"#_2","title":"\u51c6\u5907\u5f00\u59cb","text":"<p>\u5bf9\u4e8e\u5b66\u751f\u4eec\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u4f7f\u7528\u8fd9\u5957\u8bfe\u7a0b\uff0c\u628a\u6574\u4e2a\u4ed3\u5e93 fork \u5230\u4f60\u81ea\u5df1\u7684 GitHub \u8d26\u6237\u4e2d\uff0c\u5e76\u81ea\u884c\uff08\u6216\u548c\u4e00\u4e2a\u5c0f\u7ec4\u4e00\u8d77\uff09\u5b8c\u6210\u4ee5\u4e0b\u7ec3\u4e60\uff1a</p> <ul> <li>\u4ece\u8bfe\u524d\u6d4b\u9a8c\u5f00\u59cb</li> <li>\u9605\u8bfb\u8bfe\u7a0b\u5185\u5bb9\uff0c\u5b8c\u6210\u6240\u6709\u7684\u6d3b\u52a8\uff0c\u5728\u6bcf\u6b21 knowledge check \u65f6\u6682\u505c\u5e76\u601d\u8003</li> <li>\u6211\u4eec\u5efa\u8bae\u4f60\u57fa\u4e8e\u7406\u89e3\u6765\u521b\u5efa\u9879\u76ee\uff08\u800c\u4e0d\u662f\u4ec5\u4ec5\u8dd1\u4e00\u904d\u793a\u4f8b\u4ee3\u7801\uff09\u3002\u793a\u4f8b\u4ee3\u7801\u7684\u4f4d\u7f6e\u5728\u6bcf\u4e00\u4e2a\u9879\u76ee\u7684 <code>/solution</code> \u6587\u4ef6\u5939\u4e2d\u3002</li> <li>\u8fdb\u884c\u8bfe\u540e\u6d4b\u9a8c</li> <li>\u5b8c\u6210\u8bfe\u7a0b\u6311\u6218</li> <li>\u5b8c\u6210\u4f5c\u4e1a</li> <li>\u4e00\u8282\u8bfe\u5b8c\u6210\u540e, \u8bbf\u95ee\u8ba8\u8bba\u7248\uff0c\u901a\u8fc7\u586b\u5199\u76f8\u5e94\u7684 PAT Rubric (\u8bfe\u7a0b\u76ee\u6807) \u6765\u6df1\u5316\u81ea\u5df1\u7684\u5b66\u4e60\u6210\u679c\u3002\u4f60\u4e5f\u53ef\u4ee5\u56de\u5e94\u5176\u5b83\u7684 PAT\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u4e00\u8d77\u5b66\u4e60\u3002</li> </ul> <p>\u5982\u679c\u5e0c\u671b\u8fdb\u4e00\u6b65\u5b66\u4e60\uff0c\u6211\u4eec\u63a8\u8350\u8ddf\u968f Microsoft Learn \u7684\u6a21\u5757\u548c\u5b66\u4e60\u8def\u5f84\u3002</p>"},{"location":"#_3","title":"\u89c6\u9891\u6f14\u793a","text":"<p>\u6709\u4e9b\u8bfe\u7a0b\u4ee5\u77ed\u683c\u5f0f\u7684\u89c6\u9891\u63d0\u4f9b\u3002\u60a8\u53ef\u4ee5\u5728\u8bfe\u7a0b\u4e2d\u627e\u5230\u5b83\u4eec\u7684\u94fe\u63a5\uff0c\u6216\u70b9\u51fb\u4e0b\u65b9\u56fe\u7247\uff0c\u5728Microsoft Developer YouTube\u9891\u9053\u7684\u201cML for Beginners\u201d\u64ad\u653e\u5217\u8868\u4e2d\u627e\u5230\u5b83\u4eec\u3002</p> <p> </p>"},{"location":"#_4","title":"\u9879\u76ee\u56e2\u961f","text":"<p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u9762\u7684\u56fe\u7247\u83b7\u53d6\u5173\u4e8e\u8be5\u9879\u76ee\u548c\u521b\u5efa\u4eba\u5458\u7684\u89c6\u9891\uff01</p>"},{"location":"#_5","title":"\u6559\u5b66\u65b9\u5f0f","text":"<p>\u6b64\u8bfe\u7a0b\u57fa\u4e8e\u4e24\u4e2a\u6559\u5b66\u539f\u5219\uff1a\u5b66\u751f\u5e94\u8be5\u4e0a\u624b\u8fdb\u884c\u9879\u76ee\u5b9e\u8df5\uff0c\u5e76\u5b8c\u6210\u9891\u7e41\u7684\u6d4b\u9a8c\u3002 \u6b64\u5916\uff0c\u4e3a\u4e86\u4f7f\u6574\u4e2a\u8bfe\u7a0b\u66f4\u5177\u6709\u6574\u4f53\u6027\uff0c\u8bfe\u7a0b\u4eec\u6709\u4e00\u4e2a\u5171\u540c\u7684\u4e3b\u9898\u3002</p> <p>\u901a\u8fc7\u786e\u4fdd\u8bfe\u7a0b\u5185\u5bb9\u4e0e\u9879\u76ee\u5f3a\u76f8\u5173\uff0c\u6211\u4eec\u8ba9\u5b66\u4e60\u8fc7\u7a0b\u5bf9\u5b66\u751f\u66f4\u5177\u5438\u5f15\u529b\uff0c\u6982\u5ff5\u7684\u5b66\u4e60\u4e5f\u88ab\u6df1\u5316\u4e86\u3002\u96be\u5ea6\u8f83\u4f4e\u7684\u8bfe\u524d\u6d4b\u9a8c\u53ef\u4ee5\u5438\u5f15\u5b66\u751f\u5b66\u4e60\u8bfe\u7a0b\uff0c\u800c\u8bfe\u540e\u7684\u7b2c\u4e8c\u6b21\u6d4b\u9a8c\u4e5f\u8fdb\u4e00\u6b65\u91cd\u590d\u4e86\u8bfe\u5802\u4e2d\u7684\u6982\u5ff5\u3002\u8be5\u8bfe\u7a0b\u88ab\u8bbe\u8ba1\u5730\u7075\u6d3b\u6709\u8da3\uff0c\u53ef\u4ee5\u4e00\u6b21\u6027\u5168\u90e8\u5b66\u4e60\uff0c\u6216\u8005\u5206\u5f00\u6765\u4e00\u90e8\u5206\u4e00\u90e8\u5206\u5b66\u4e60\u3002\u8fd9\u4e9b\u9879\u76ee\u7531\u6d45\u5165\u6df1\uff0c\u4ece\u7b2c\u4e00\u5468\u7684\u5c0f\u9879\u76ee\u5f00\u59cb\uff0c\u5728\u7b2c\u5341\u4e8c\u5468\u7ed3\u675f\u65f6\u53d8\u5f97\u8f83\u4e3a\u590d\u6742\u3002\u672c\u8bfe\u7a0b\u8fd8\u5305\u62ec\u4e00\u4e2a\u5173\u4e8e\u673a\u5668\u5b66\u4e60\u5b9e\u9645\u5e94\u7528\u7684\u9644\u8a00\uff0c\u53ef\u7528\u4f5c\u989d\u5916\u5b66\u5206\u6216\u8fdb\u4e00\u6b65\u8ba8\u8bba\u7684\u57fa\u7840\u3002</p>"},{"location":"#_6","title":"\u6bcf\u4e00\u8282\u8bfe\u90fd\u5305\u542b\uff1a","text":"<ul> <li>\u53ef\u9009\u7684\u901f\u5199\u7b14\u8bb0</li> <li>\u53ef\u9009\u7684\u8865\u5145\u89c6\u9891</li> <li>\u89c6\u9891\u6f14\u793a\uff08\u4ec5\u9002\u7528\u4e8e\u90e8\u5206\u8bfe\u7a0b\uff09</li> <li>\u8bfe\u524d\u70ed\u8eab\u6d4b\u9a8c</li> <li>\u6587\u5b57\u8bfe\u7a0b</li> <li>\u5bf9\u4e8e\u57fa\u4e8e\u9879\u76ee\u7684\u8bfe\u7a0b\uff0c\u5305\u542b\u6784\u5efa\u9879\u76ee\u7684\u5206\u6b65\u6307\u5357</li> <li>\u77e5\u8bc6\u68c0\u6d4b</li> <li>\u4e00\u4e2a\u6311\u6218</li> <li>\u8865\u5145\u9605\u8bfb</li> <li>\u4f5c\u4e1a</li> <li>\u8bfe\u540e\u6d4b\u9a8c</li> </ul> <p>\u5173\u4e8e\u6d4b\u9a8c\uff1a\u6240\u6709\u7684\u6d4b\u9a8c\u90fd\u5728\u8fd9\u4e2a\u5e94\u7528\u91cc\uff0c\u603b\u5171 52 \u4e2a\u6d4b\u9a8c\uff0c\u6bcf\u4e2a\u6d4b\u9a8c\u4e09\u4e2a\u95ee\u9898\u3002\u5b83\u4eec\u7684\u94fe\u63a5\u5728\u6bcf\u8282\u8bfe\u4e2d\uff0c\u800c\u4e14\u8fd9\u4e2a\u6d4b\u9a8c\u5e94\u7528\u53ef\u4ee5\u5728\u672c\u5730\u8fd0\u884c\u3002\u8bf7\u53c2\u8003 <code>quiz-app</code> \u6587\u4ef6\u5939\u4e2d\u7684\u6307\u5357\u3002</p> \u8bfe\u7a0b\u7f16\u53f7 \u4e3b\u4f53 \u8bfe\u7a0b\u7ec4 \u5b66\u4e60\u76ee\u6807 \u8bfe\u7a0b\u94fe\u63a5 01 \u673a\u5668\u5b66\u4e60\u7b80\u4ecb \u7b80\u4ecb \u4e86\u89e3\u673a\u5668\u5b66\u4e60\u80cc\u540e\u7684\u57fa\u672c\u6982\u5ff5 \u8bfe\u7a0b 02 \u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2 \u7b80\u4ecb \u4e86\u89e3\u8be5\u9886\u57df\u7684\u5386\u53f2 \u8bfe\u7a0b 03 \u673a\u5668\u5b66\u4e60\u4e0e\u516c\u5e73 \u7b80\u4ecb \u5728\u6784\u5efa\u548c\u5e94\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u8003\u8651\u54ea\u4e9b\u6709\u5173\u516c\u5e73\u7684\u91cd\u8981\u54f2\u5b66\u95ee\u9898\uff1f \u8bfe\u7a0b 04 \u673a\u5668\u5b66\u4e60\u7684\u6280\u672f\u5de5\u5177 \u7b80\u4ecb \u673a\u5668\u5b66\u4e60\u7814\u7a76\u8005\u4f7f\u7528\u54ea\u4e9b\u6280\u672f\u6765\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1f \u8bfe\u7a0b 05 \u56de\u5f52\u7b80\u4ecb \u56de\u5f52 \u5f00\u59cb\u4f7f\u7528 Python \u548c Scikit-learn \u6784\u5efa\u56de\u5f52\u6a21\u578b \u8bfe\u7a0b 06 \u5317\u7f8e\u5357\u74dc\u4ef7\u683c \ud83c\udf83 \u56de\u5f52 \u53ef\u89c6\u5316\u3001\u8fdb\u884c\u6570\u636e\u6e05\u7406\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u505a\u51c6\u5907 \u8bfe\u7a0b 07 \u5317\u7f8e\u5357\u74dc\u4ef7\u683c \ud83c\udf83 \u56de\u5f52 \u5efa\u7acb\u7ebf\u6027\u548c\u591a\u9879\u5f0f\u56de\u5f52\u6a21\u578b \u8bfe\u7a0b 08 \u5317\u7f8e\u5357\u74dc\u4ef7\u683c \ud83c\udf83 \u56de\u5f52 \u6784\u5efa\u903b\u8f91\u56de\u5f52\u6a21\u578b \u8bfe\u7a0b 09 \u4e00\u4e2a\u7f51\u9875\u5e94\u7528 \ud83d\udd0c \u7f51\u9875\u5e94\u7528 \u6784\u5efa\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f\u4ee5\u4f7f\u7528\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b \u8bfe\u7a0b 10 \u5206\u7c7b\u7b80\u4ecb \u5206\u7c7b \u6e05\u7406\u3001\u51c6\u5907\u548c\u53ef\u89c6\u5316\u6570\u636e\uff1b \u5206\u7c7b\u7b80\u4ecb \u8bfe\u7a0b 11 \u7f8e\u5473\u7684\u4e9a\u6d32\u548c\u5370\u5ea6\u7f8e\u98df \ud83c\udf5c \u5206\u7c7b \u5206\u7c7b\u5668\u7b80\u4ecb \u8bfe\u7a0b 12 \u7f8e\u5473\u7684\u4e9a\u6d32\u548c\u5370\u5ea6\u7f8e\u98df \ud83c\udf5c \u5206\u7c7b \u5173\u4e8e\u5206\u7c7b\u5668\u7684\u66f4\u591a\u5185\u5bb9 \u8bfe\u7a0b 13 \u7f8e\u5473\u7684\u4e9a\u6d32\u548c\u5370\u5ea6\u7f8e\u98df \ud83c\udf5c \u5206\u7c7b \u4f7f\u7528\u60a8\u7684\u6a21\u578b\u6784\u5efa\u4e00\u4e2a\u53ef\u4ee5\u300c\u63a8\u8350\u300d\u7684 Web \u5e94\u7528 \u8bfe\u7a0b 14 \u805a\u7c7b\u7b80\u4ecb \u805a\u7c7b \u6e05\u7406\u3001\u51c6\u5907\u548c\u53ef\u89c6\u5316\u6570\u636e\uff1b \u805a\u7c7b\u7b80\u4ecb \u8bfe\u7a0b 15 \u63a2\u7d22\u5c3c\u65e5\u5229\u4e9a\u4eba\u7684\u97f3\u4e50\u54c1\u5473 \ud83c\udfa7 \u805a\u7c7b \u63a2\u7d22 K-Means \u805a\u7c7b\u65b9\u6cd5 \u8bfe\u7a0b 16 \u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u7b80\u4ecb  \u2615\ufe0f \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684 bot (\u673a\u5668\u4eba) \u6765\u4e86\u89e3 NLP \u7684\u57fa\u7840\u77e5\u8bc6 \u8bfe\u7a0b 17 \u5e38\u89c1\u7684 NLP \u4efb\u52a1 \u2615\ufe0f \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u901a\u8fc7\u7406\u89e3\u5904\u7406\u8bed\u8a00\u7ed3\u6784\u65f6\u6240\u9700\u7684\u5e38\u89c1\u4efb\u52a1\u6765\u52a0\u6df1\u5bf9\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u7684\u7406\u89e3 \u8bfe\u7a0b 18 \u7ffb\u8bd1\u548c\u60c5\u611f\u5206\u6790 \u2665\ufe0f \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5bf9\u7b80\u00b7\u5965\u65af\u6c40\u7684\u6587\u672c\u8fdb\u884c\u7ffb\u8bd1\u548c\u60c5\u611f\u5206\u6790 \u8bfe\u7a0b 19 \u6b27\u6d32\u7684\u6d6a\u6f2b\u9152\u5e97 \u2665\ufe0f \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5bf9\u4e8e\u9152\u5e97\u8bc4\u4ef7\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff08\u4e0a\uff09 \u8bfe\u7a0b 20 \u6b27\u6d32\u7684\u6d6a\u6f2b\u9152\u5e97 \u2665\ufe0f \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u5bf9\u4e8e\u9152\u5e97\u8bc4\u4ef7\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff08\u4e0b\uff09 \u8bfe\u7a0b 21 \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b80\u4ecb \u65f6\u95f4\u5e8f\u5217 \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b80\u4ecb forecasting \u8bfe\u7a0b 22 \u26a1\ufe0f \u4e16\u754c\u7528\u7535\u91cf \u26a1\ufe0f - \u4f7f\u7528 ARIMA \u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b \u65f6\u95f4\u5e8f\u5217 \u4f7f\u7528 ARIMA \u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b \u8bfe\u7a0b 23 \u26a1\ufe0f \u4e16\u754c\u7528\u7535\u91cf\u26a1\ufe0f - \u57fa\u4e8eSVR\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b \u65f6\u95f4\u5e8f\u5217 \u57fa\u4e8e\u652f\u6301\u5411\u91cf\u56de\u5f52\u5668\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b \u8bfe\u7a0b 23 \u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb \u5f3a\u5316\u5b66\u4e60 Q-Learning \u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb \u8bfe\u7a0b 24 \u5e2e\u52a9 Peter \u907f\u5f00\u72fc\uff01\ud83d\udc3a \u5f3a\u5316\u5b66\u4e60 \u5f3a\u5316\u5b66\u4e60\u7ec3\u4e60 \u8bfe\u7a0b \u9644\u8a00 \u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u573a\u666f\u548c\u5e94\u7528 \u81ea\u7136\u573a\u666f\u4e0b\u7684\u673a\u5668\u5b66\u4e60 \u63a2\u7d22\u6709\u8da3\u7684\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e86\u89e3\u73b0\u5b9e\u4e16\u754c\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528 \u8bfe\u7a0b \u9644\u8a00 \u4f7f\u7528RAI\u4eea\u8868\u677f\u5728ML\u4e2d\u8c03\u8bd5\u6a21\u578b \u81ea\u7136\u573a\u666f\u4e0b\u7684\u673a\u5668\u5b66\u4e60 \u4f7f\u7528\u8d1f\u8d23\u4efb\u7684AI\u4eea\u8868\u677f\u7ec4\u4ef6\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u8c03\u8bd5 \u8bfe\u7a0b"},{"location":"#_7","title":"\u79bb\u7ebf\u8bbf\u95ee","text":"<p>\u4f7f\u7528Material for MkDocs \u8fdb\u884c\u672c\u5730\u6784\u5efa</p>"},{"location":"#pdf","title":"PDF \u6587\u6863","text":"<p>\u70b9\u51fb\u8fd9\u91cc\u67e5\u627e\u8bfe\u7a0b\u7684 PDF \u6587\u6863\u3002</p>"},{"location":"#_8","title":"\u5176\u4ed6\u8bfe\u7a0b","text":"<ul> <li>AI</li> <li>\u751f\u6210\u5f0fAI</li> <li>\u6570\u636e\u79d1\u5b66</li> <li>Web \u5f00\u53d1</li> </ul>"},{"location":"1-Introduction/","title":"Introduction to machine learning","text":"<p>In this section of the curriculum, you will be introduced to the base concepts underlying the field of machine learning, what it is, and learn about its history and the techniques researchers use to work with it.  Let's explore this new world of ML together!</p> <p></p> <p>Photo by Bill Oxford on Unsplash</p>"},{"location":"1-Introduction/#lessons","title":"Lessons","text":"<ol> <li>Introduction to machine learning</li> <li>The History of machine learning and AI</li> <li>Fairness and machine learning</li> <li>Techniques of machine learning</li> </ol>"},{"location":"1-Introduction/#credits","title":"Credits","text":"<p>\"Introduction to Machine Learning\" was written with \u2665\ufe0f by a team of folks including Muhammad Sakib Khan Inan, Ornella Altunyan and Jen Looper</p> <p>\"The History of Machine Learning\" was written with \u2665\ufe0f by Jen Looper and Amy Boyd</p> <p>\"Fairness and Machine Learning\" was written with \u2665\ufe0f by Tomomi Imura </p> <p>\"Techniques of Machine Learning\" was written with \u2665\ufe0f by Jen Looper and Chris Noring </p>"},{"location":"1-Introduction/1-intro-to-ML/","title":"Introduction to machine learning","text":""},{"location":"1-Introduction/1-intro-to-ML/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>\ud83c\udfa5 Click the image above for a short video working through this lesson.</p> <p>Welcome to this course on classical machine learning for beginners! Whether you're completely new to this topic, or an experienced ML practitioner looking to brush up on an area, we're happy to have you join us! We want to create a friendly launching spot for your ML study and would be happy to evaluate, respond to, and incorporate your feedback.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: MIT's John Guttag introduces machine learning</p>"},{"location":"1-Introduction/1-intro-to-ML/#getting-started-with-machine-learning","title":"Getting started with machine learning","text":"<p>Before starting with this curriculum, you need to have your computer set up and ready to run notebooks locally.</p> <ul> <li>Configure your machine with these videos. Use the following links to learn how to install Python in your system and setup a text editor for development.</li> <li>Learn Python. It's also recommended to have a basic understanding of Python, a programming language useful for data scientists that we use in this course.</li> <li>Learn Node.js and JavaScript. We also use JavaScript a few times in this course when building web apps, so you will need to have node and npm installed, as well as Visual Studio Code available for both Python and JavaScript development.</li> <li>Create a GitHub account. Since you found us here on GitHub, you might already have an account, but if not, create one and then fork this curriculum to use on your own. (Feel free to give us a star, too \ud83d\ude0a)</li> <li>Explore Scikit-learn. Familiarize yourself with Scikit-learn, a set of ML libraries that we reference in these lessons.</li> </ul>"},{"location":"1-Introduction/1-intro-to-ML/#what-is-machine-learning","title":"What is machine learning?","text":"<p>The term 'machine learning' is one of the most popular and frequently used terms of today. There is a nontrivial possibility that you have heard this term at least once if you have some sort of familiarity with technology, no matter what domain you work in. The mechanics of machine learning, however, are a mystery to most people. For a machine learning beginner, the subject can sometimes feel overwhelming. Therefore, it is important to understand what machine learning actually is, and to learn about it step by step, through practical examples.</p>"},{"location":"1-Introduction/1-intro-to-ML/#the-hype-curve","title":"The hype curve","text":"<p>Google Trends shows the recent 'hype curve' of the term 'machine learning'</p>"},{"location":"1-Introduction/1-intro-to-ML/#a-mysterious-universe","title":"A mysterious universe","text":"<p>We live in a universe full of fascinating mysteries. Great scientists such as Stephen Hawking, Albert Einstein, and many more have devoted their lives to searching for meaningful information that uncovers the mysteries of the world around us. This is the human condition of learning: a human child learns new things and uncovers the structure of their world year by year as they grow to adulthood.</p>"},{"location":"1-Introduction/1-intro-to-ML/#the-childs-brain","title":"The child's brain","text":"<p>A child's brain and senses perceive the facts of their surroundings and gradually learn the hidden patterns of life which help the child to craft logical rules to identify learned patterns. The learning process of the human brain makes humans the most sophisticated living creature of this world. Learning continuously by discovering hidden patterns and then innovating on those patterns enables us to make ourselves better and better throughout our lifetime. This learning capacity and evolving capability is related to a concept called brain plasticity. Superficially, we can draw some motivational similarities between the learning process of the human brain and the concepts of machine learning.</p>"},{"location":"1-Introduction/1-intro-to-ML/#the-human-brain","title":"The human brain","text":"<p>The human brain perceives things from the real world, processes the perceived information, makes rational decisions, and performs certain actions based on circumstances. This is what we called behaving intelligently. When we program a facsimile of the intelligent behavioral process to a machine, it is called artificial intelligence (AI).</p>"},{"location":"1-Introduction/1-intro-to-ML/#some-terminology","title":"Some terminology","text":"<p>Although the terms can be confused, machine learning (ML) is an important subset of artificial intelligence. ML is concerned with using specialized algorithms to uncover meaningful information and find hidden patterns from perceived data to corroborate the rational decision-making process.</p>"},{"location":"1-Introduction/1-intro-to-ML/#ai-ml-deep-learning","title":"AI, ML, Deep Learning","text":"<p>A diagram showing the relationships between AI, ML, deep learning, and data science. Infographic by Jen Looper inspired by this graphic</p>"},{"location":"1-Introduction/1-intro-to-ML/#concepts-to-cover","title":"Concepts to cover","text":"<p>In this curriculum, we are going to cover only the core concepts of machine learning that a beginner must know. We cover what we call 'classical machine learning' primarily using Scikit-learn, an excellent library many students use to learn the basics.  To understand broader concepts of artificial intelligence or deep learning, a strong fundamental knowledge of machine learning is indispensable, and so we would like to offer it here.</p>"},{"location":"1-Introduction/1-intro-to-ML/#in-this-course-you-will-learn","title":"In this course you will learn:","text":"<ul> <li>core concepts of machine learning</li> <li>the history of ML</li> <li>ML and fairness</li> <li>regression ML techniques</li> <li>classification ML techniques</li> <li>clustering ML techniques</li> <li>natural language processing ML techniques</li> <li>time series forecasting ML techniques</li> <li>reinforcement learning</li> <li>real-world applications for ML</li> </ul>"},{"location":"1-Introduction/1-intro-to-ML/#what-we-will-not-cover","title":"What we will not cover","text":"<ul> <li>deep learning</li> <li>neural networks</li> <li>AI</li> </ul> <p>To make for a better learning experience, we will avoid the complexities of neural networks, 'deep learning' - many-layered model-building using neural networks - and AI, which we will discuss in a different curriculum. We also will offer a forthcoming data science curriculum to focus on that aspect of this larger field.</p>"},{"location":"1-Introduction/1-intro-to-ML/#why-study-machine-learning","title":"Why study machine learning?","text":"<p>Machine learning, from a systems perspective, is defined as the creation of automated systems that can learn hidden patterns from data to aid in making intelligent decisions.</p> <p>This motivation is loosely inspired by how the human brain learns certain things based on the data it perceives from the outside world.</p> <p>\u2705 Think for a minute why a business would want to try to use machine learning strategies vs. creating a hard-coded rules-based engine.</p>"},{"location":"1-Introduction/1-intro-to-ML/#applications-of-machine-learning","title":"Applications of machine learning","text":"<p>Applications of machine learning are now almost everywhere, and are as ubiquitous as the data that is flowing around our societies, generated by our smart phones, connected devices, and other systems. Considering the immense potential of state-of-the-art machine learning algorithms, researchers have been exploring their capability to solve multi-dimensional and multi-disciplinary real-life problems with great positive outcomes.</p>"},{"location":"1-Introduction/1-intro-to-ML/#examples-of-applied-ml","title":"Examples of applied ML","text":"<p>You can use machine learning in many ways:</p> <ul> <li>To predict the likelihood of disease from a patient's medical history or reports.</li> <li>To leverage weather data to predict weather events.</li> <li>To understand the sentiment of a text.</li> <li>To detect fake news to stop the spread of propaganda.</li> </ul> <p>Finance, economics, earth science, space exploration, biomedical engineering, cognitive science, and even fields in the humanities have adapted machine learning to solve the arduous, data-processing heavy problems of their domain.</p>"},{"location":"1-Introduction/1-intro-to-ML/#conclusion","title":"Conclusion","text":"<p>Machine learning automates the process of pattern-discovery by finding meaningful insights from real-world or generated data. It has proven itself to be highly valuable in business, health, and financial applications, among others.</p> <p>In the near future, understanding the basics of machine learning is going to be a must for people from any domain due to its widespread adoption.</p>"},{"location":"1-Introduction/1-intro-to-ML/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Sketch, on paper or using an online app like Excalidraw, your understanding of the differences between AI, ML, deep learning, and data science. Add some ideas of problems that each of these techniques are good at solving.</p>"},{"location":"1-Introduction/1-intro-to-ML/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"1-Introduction/1-intro-to-ML/#review-self-study","title":"Review &amp; Self Study","text":"<p>To learn more about how you can work with ML algorithms in the cloud, follow this Learning Path.</p> <p>Take a Learning Path about the basics of ML.</p>"},{"location":"1-Introduction/1-intro-to-ML/#assignment","title":"Assignment","text":"<p>Get up and running</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u4ecb\u7ecd","text":"<p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u9762\u7684\u56fe\u7247\u89c2\u770b\u8ba8\u8bba\u673a\u5668\u5b66\u4e60\u3001\u4eba\u5de5\u667a\u80fd\u548c\u6df1\u5ea6\u5b66\u4e60\u4e4b\u95f4\u533a\u522b\u7684\u89c6\u9891\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u6b22\u8fce\u6765\u5230\u8fd9\u4e2a\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u521d\u5b66\u8005\u8bfe\u7a0b\uff01\u65e0\u8bba\u4f60\u662f\u8fd9\u4e2a\u4e3b\u9898\u7684\u65b0\u624b\uff0c\u8fd8\u662f\u4e00\u4e2a\u6709\u7ecf\u9a8c\u7684 ML \u4ece\u4e1a\u8005\uff0c\u6211\u4eec\u90fd\u5f88\u9ad8\u5174\u4f60\u80fd\u52a0\u5165\u6211\u4eec\uff01\u6211\u4eec\u5e0c\u671b\u4e3a\u4f60\u7684 ML \u7814\u7a76\u521b\u5efa\u4e00\u4e2a\u597d\u7684\u5f00\u59cb\uff0c\u5e76\u5f88\u4e50\u610f\u8bc4\u4f30\u3001\u56de\u5e94\u548c\u63a5\u53d7\u4f60\u7684\u53cd\u9988\u3002</p> <p></p> <p>\ud83c\udfa5 \u5355\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1a\u9ebb\u7701\u7406\u5de5\u5b66\u9662\u7684 John Guttag \u4ecb\u7ecd\u673a\u5668\u5b66\u4e60</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_4","title":"\u673a\u5668\u5b66\u4e60\u5165\u95e8","text":"<p>\u5728\u5f00\u59cb\u672c\u8bfe\u7a0b\u4e4b\u524d\uff0c\u4f60\u9700\u8981\u8bbe\u7f6e\u8ba1\u7b97\u673a\u80fd\u5728\u672c\u5730\u8fd0\u884c Jupyter Notebooks\u3002</p> <ul> <li>\u6309\u7167\u8fd9\u4e9b\u89c6\u9891\u91cc\u7684\u8bb2\u89e3\u914d\u7f6e\u4f60\u7684\u8ba1\u7b97\u673a\u3002\u4e86\u89e3\u6709\u5173\u5982\u4f55\u5728\u6b64\u89c6\u9891\u96c6\u4e2d\u8bbe\u7f6e\u8ba1\u7b97\u673a\u7684\u66f4\u591a\u4fe1\u606f\u3002</li> <li>\u5b66\u4e60 Python\u3002 \u8fd8\u5efa\u8bae\u4f60\u5bf9 Python \u6709\u4e00\u4e2a\u57fa\u672c\u7684\u4e86\u89e3\u3002\u8fd9\u662f\u6211\u4eec\u5728\u672c\u8bfe\u7a0b\u4e2d\u4f7f\u7528\u7684\u4e00\u79cd\u5bf9\u6570\u636e\u79d1\u5b66\u5bb6\u6709\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u3002</li> <li>\u5b66\u4e60 Node.js \u548c JavaScript\u3002\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u5728\u6784\u5efa web \u5e94\u7528\u7a0b\u5e8f\u65f6\u4e5f\u4f7f\u7528\u8fc7\u51e0\u6b21 JavaScript\uff0c\u56e0\u6b64\u4f60\u9700\u8981\u6709 Node.js \u548c npm \u4ee5\u53ca Visual Studio Code \u7528\u4e8e Python \u548c JavaScript \u5f00\u53d1\u3002</li> <li>\u521b\u5efa GitHub \u5e10\u6237\u3002\u65e2\u7136\u4f60\u5728 GitHub \u4e0a\u627e\u5230\u6211\u4eec\uff0c\u4f60\u53ef\u80fd\u5df2\u7ecf\u6709\u4e86\u4e00\u4e2a\u5e10\u6237\uff0c\u4f46\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u521b\u5efa\u4e00\u4e2a\u5e10\u6237\uff0c\u7136\u540e fork \u6b64\u8bfe\u7a0b\u81ea\u5df1\u4f7f\u7528(\u4e5f\u7ed9\u6211\u4eec\u4e00\u9897\u661f\u661f\u5427\ud83d\ude0a) </li> <li>\u63a2\u7d22 Scikit-learn. \u719f\u6089 Scikit-learn\uff0c\u6211\u4eec\u5728\u8fd9\u4e9b\u8bfe\u7a0b\u4e2d\u5f15\u7528\u7684\u4e00\u7ec4 ML \u5e93\u3002</li> </ul>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_5","title":"\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff1f","text":"<p>\u672f\u8bed\u201c\u673a\u5668\u5b66\u4e60\u201d\u662f\u5f53\u4eca\u6700\u6d41\u884c\u548c\u6700\u5e38\u7528\u7684\u672f\u8bed\u4e4b\u4e00\u3002 \u5982\u679c\u4f60\u5bf9\u79d1\u6280\u6709\u67d0\u79cd\u7a0b\u5ea6\u7684\u719f\u6089\uff0c\u90a3\u4e48\u5f88\u53ef\u80fd\u4f60\u81f3\u5c11\u542c\u8bf4\u8fc7\u8fd9\u4e2a\u672f\u8bed\u4e00\u6b21\uff0c\u65e0\u8bba\u4f60\u5728\u54ea\u4e2a\u9886\u57df\u5de5\u4f5c\u3002\u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u7684\u673a\u5236\u5bf9\u5927\u591a\u6570\u4eba\u6765\u8bf4\u662f\u4e00\u4e2a\u8c1c\u3002 \u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u521d\u5b66\u8005\u6765\u8bf4\uff0c\u8fd9\u4e2a\u4e3b\u9898\u6709\u65f6\u4f1a\u8ba9\u4eba\u611f\u5230\u4e0d\u77e5\u6240\u63aa\u3002 \u56e0\u6b64\uff0c\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u8d28\u662f\u4ec0\u4e48\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u4e00\u6b65\u4e00\u6b65\u5730\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u662f\u5f88\u91cd\u8981\u7684\u3002</p> <p></p> <p>\u8c37\u6b4c\u8d8b\u52bf\u663e\u793a\u4e86\u201c\u673a\u5668\u5b66\u4e60\u201d\u4e00\u8bcd\u6700\u8fd1\u7684\u201c\u8d8b\u52bf\u66f2\u7ebf\u201d</p> <p>\u6211\u4eec\u751f\u6d3b\u5728\u4e00\u4e2a\u5145\u6ee1\u8ff7\u4eba\u5965\u79d8\u7684\u5b87\u5b99\u4e2d\u3002\u50cf\u53f2\u8482\u82ac\u00b7\u970d\u91d1\u3001\u963f\u5c14\u4f2f\u7279\u00b7\u7231\u56e0\u65af\u5766\u7b49\u4f1f\u5927\u7684\u79d1\u5b66\u5bb6\uff0c\u4ee5\u53ca\u66f4\u591a\u7684\u4eba\uff0c\u90fd\u81f4\u529b\u4e8e\u5bfb\u627e\u6709\u610f\u4e49\u7684\u4fe1\u606f\uff0c\u63ed\u793a\u6211\u4eec\u5468\u56f4\u4e16\u754c\u7684\u5965\u79d8\u3002\u8fd9\u5c31\u662f\u4eba\u7c7b\u5b66\u4e60\u7684\u6761\u4ef6\uff1a\u4e00\u4e2a\u4eba\u7c7b\u7684\u5b69\u5b50\u5728\u957f\u5927\u6210\u4eba\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u5e74\u53c8\u4e00\u5e74\u5730\u5b66\u4e60\u65b0\u4e8b\u7269\u5e76\u63ed\u793a\u4e16\u754c\u7684\u7ed3\u6784\u3002</p> <p>\u5b69\u5b50\u7684\u5927\u8111\u548c\u611f\u5b98\u611f\u77e5\u5230\u5468\u56f4\u7684\u4e8b\u5b9e\uff0c\u5e76\u9010\u6e10\u5b66\u4e60\u9690\u85cf\u7684\u751f\u6d3b\u6a21\u5f0f\uff0c\u8fd9\u6709\u52a9\u4e8e\u5b69\u5b50\u5236\u5b9a\u903b\u8f91\u89c4\u5219\u6765\u8bc6\u522b\u5b66\u4e60\u6a21\u5f0f\u3002\u4eba\u7c7b\u5927\u8111\u7684\u5b66\u4e60\u8fc7\u7a0b\u4f7f\u4eba\u7c7b\u6210\u4e3a\u4e16\u754c\u4e0a\u6700\u590d\u6742\u7684\u751f\u7269\u3002\u4e0d\u65ad\u5730\u5b66\u4e60\uff0c\u901a\u8fc7\u53d1\u73b0\u9690\u85cf\u7684\u6a21\u5f0f\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u6a21\u5f0f\u8fdb\u884c\u521b\u65b0\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u4f7f\u81ea\u5df1\u5728\u4e00\u751f\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u597d\u3002\u8fd9\u79cd\u5b66\u4e60\u80fd\u529b\u548c\u8fdb\u5316\u80fd\u529b\u4e0e\u4e00\u4e2a\u53eb\u505a\u5927\u8111\u53ef\u5851\u6027\u7684\u6982\u5ff5\u6709\u5173\u3002\u4ece\u8868\u9762\u4e0a\u770b\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u4eba\u8111\u7684\u5b66\u4e60\u8fc7\u7a0b\u548c\u673a\u5668\u5b66\u4e60\u7684\u6982\u5ff5\u4e4b\u95f4\u627e\u5230\u4e00\u4e9b\u52a8\u673a\u4e0a\u7684\u76f8\u4f3c\u4e4b\u5904\u3002</p> <p>\u4eba\u8111 \u4ece\u73b0\u5b9e\u4e16\u754c\u4e2d\u611f\u77e5\u4e8b\u7269\uff0c\u5904\u7406\u611f\u77e5\u5230\u7684\u4fe1\u606f\uff0c\u505a\u51fa\u7406\u6027\u7684\u51b3\u5b9a\uff0c\u5e76\u6839\u636e\u73af\u5883\u6267\u884c\u67d0\u4e9b\u884c\u52a8\u3002\u8fd9\u5c31\u662f\u6211\u4eec\u6240\u8bf4\u7684\u667a\u80fd\u884c\u4e3a\u3002\u5f53\u6211\u4eec\u5c06\u667a\u80fd\u884c\u4e3a\u8fc7\u7a0b\u7684\u590d\u5236\u54c1\u7f16\u7a0b\u5230\u8ba1\u7b97\u673a\u4e0a\u65f6\uff0c\u5b83\u88ab\u79f0\u4e3a\u4eba\u5de5\u667a\u80fd (AI)\u3002</p> <p>\u5c3d\u7ba1\u8fd9\u4e9b\u672f\u8bed\u53ef\u80fd\u4f1a\u6df7\u6dc6\uff0c\u4f46\u673a\u5668\u5b66\u4e60 (ML) \u662f\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u91cd\u8981\u5b50\u96c6\u3002 \u673a\u5668\u5b66\u4e60\u5173\u6ce8\u4f7f\u7528\u4e13\u95e8\u7684\u7b97\u6cd5\u6765\u53d1\u73b0\u6709\u610f\u4e49\u7684\u4fe1\u606f\uff0c\u5e76\u4ece\u611f\u77e5\u6570\u636e\u4e2d\u627e\u5230\u9690\u85cf\u7684\u6a21\u5f0f\uff0c\u4ee5\u8bc1\u5b9e\u7406\u6027\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002</p> <p></p> <p>\u663e\u793a AI\u3001ML\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u636e\u79d1\u5b66\u4e4b\u95f4\u5173\u7cfb\u7684\u56fe\u8868\u3002\u56fe\u7247\u4f5c\u8005 Jen Looper\uff0c\u7075\u611f\u6765\u81ea\u8fd9\u5f20\u56fe</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_6","title":"\u4f60\u5c06\u5728\u672c\u8bfe\u7a0b\u4e2d\u5b66\u5230\u4ec0\u4e48","text":"<p>\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ec5\u6db5\u76d6\u521d\u5b66\u8005\u5fc5\u987b\u4e86\u89e3\u7684\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u6211\u4eec\u4e3b\u8981\u4f7f\u7528 Scikit-learn \u6765\u4ecb\u7ecd\u6211\u4eec\u6240\u8c13\u7684\u201c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u201d\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bb8\u591a\u5b66\u751f\u7528\u6765\u5b66\u4e60\u57fa\u7840\u77e5\u8bc6\u7684\u4f18\u79c0\u5e93\u3002\u8981\u7406\u89e3\u66f4\u5e7f\u6cdb\u7684\u4eba\u5de5\u667a\u80fd\u6216\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7840\u77e5\u8bc6\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u60f3\u5728\u8fd9\u91cc\u63d0\u4f9b\u5b83\u3002</p> <p>\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\uff1a</p> <ul> <li>\u673a\u5668\u5b66\u4e60\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2</li> <li>\u673a\u5668\u5b66\u4e60\u548c\u516c\u5e73\u6027</li> <li>\u56de\u5f52</li> <li>\u5206\u7c7b</li> <li>\u805a\u7c7b</li> <li>\u81ea\u7136\u8bed\u8a00\u5904\u7406</li> <li>\u65f6\u5e8f\u9884\u6d4b</li> <li>\u5f3a\u5316\u5b66\u4e60</li> <li>\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528</li> </ul>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_7","title":"\u6211\u4eec\u4e0d\u4f1a\u6db5\u76d6\u7684\u5185\u5bb9","text":"<ul> <li>\u6df1\u5ea6\u5b66\u4e60</li> <li>\u795e\u7ecf\u7f51\u7edc</li> <li>AI</li> </ul> <p>\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u6211\u4eec\u5c06\u907f\u514d\u795e\u7ecf\u7f51\u7edc\u3001\u201c\u6df1\u5ea6\u5b66\u4e60\u201d\uff08\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u591a\u5c42\u6a21\u578b\u6784\u5efa\uff09\u548c\u4eba\u5de5\u667a\u80fd\u7684\u590d\u6742\u6027\uff0c\u6211\u4eec\u5c06\u5728\u4e0d\u540c\u7684\u8bfe\u7a0b\u4e2d\u8ba8\u8bba\u8fd9\u4e9b\u95ee\u9898\u3002 \u6211\u4eec\u8fd8\u5c06\u63d0\u4f9b\u5373\u5c06\u63a8\u51fa\u7684\u6570\u636e\u79d1\u5b66\u8bfe\u7a0b\uff0c\u4ee5\u4e13\u6ce8\u4e8e\u8fd9\u4e2a\u66f4\u5927\u9886\u57df\u7684\u8fd9\u4e00\u65b9\u9762\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_8","title":"\u4e3a\u4ec0\u4e48\u8981\u5b66\u4e60\u673a\u5668\u5b66\u4e60\uff1f","text":"<p>\u4ece\u7cfb\u7edf\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u673a\u5668\u5b66\u4e60\u88ab\u5b9a\u4e49\u4e3a\u521b\u5efa\u53ef\u4ee5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u9690\u85cf\u6a21\u5f0f\u4ee5\u5e2e\u52a9\u505a\u51fa\u667a\u80fd\u51b3\u7b56\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u3002</p> <p>\u8fd9\u79cd\u52a8\u673a\u5927\u81f4\u662f\u53d7\u4eba\u8111\u5982\u4f55\u6839\u636e\u5b83\u4ece\u5916\u90e8\u4e16\u754c\u611f\u77e5\u5230\u7684\u6570\u636e\u6765\u5b66\u4e60\u67d0\u4e9b\u4e1c\u897f\u7684\u542f\u53d1\u3002</p> <p>\u2705 \u60f3\u4e00\u60f3\u4e3a\u4ec0\u4e48\u4f01\u4e1a\u60f3\u8981\u5c1d\u8bd5\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b56\u7565\u800c\u4e0d\u662f\u521b\u5efa\u57fa\u4e8e\u786c\u7f16\u7801\u7684\u89c4\u5219\u5f15\u64ce\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_9","title":"\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528","text":"<p>\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u73b0\u5728\u51e0\u4e4e\u65e0\u5904\u4e0d\u5728\uff0c\u5c31\u50cf\u6211\u4eec\u7684\u667a\u80fd\u624b\u673a\u3001\u4e92\u8054\u8bbe\u5907\u548c\u5176\u4ed6\u7cfb\u7edf\u4ea7\u751f\u7684\u6570\u636e\u4e00\u6837\u65e0\u5904\u4e0d\u5728\u3002\u8003\u8651\u5230\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7814\u7a76\u4eba\u5458\u4e00\u76f4\u5728\u63a2\u7d22\u5176\u89e3\u51b3\u591a\u7ef4\u591a\u5b66\u79d1\u73b0\u5b9e\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u79ef\u6781\u6210\u679c\u3002</p> <p>\u4f60\u53ef\u4ee5\u5728\u5f88\u591a\u65b9\u9762\u4f7f\u7528\u673a\u5668\u5b66\u4e60:</p> <ul> <li>\u6839\u636e\u75c5\u4eba\u7684\u75c5\u53f2\u6216\u62a5\u544a\u6765\u9884\u6d4b\u60a3\u75c5\u7684\u53ef\u80fd\u6027\u3002</li> <li>\u5229\u7528\u5929\u6c14\u6570\u636e\u9884\u6d4b\u5929\u6c14\u3002</li> <li>\u7406\u89e3\u6587\u672c\u7684\u60c5\u611f\u3002</li> <li>\u68c0\u6d4b\u5047\u65b0\u95fb\u4ee5\u963b\u6b62\u5176\u4f20\u64ad\u3002</li> </ul> <p>\u91d1\u878d\u3001\u7ecf\u6d4e\u5b66\u3001\u5730\u7403\u79d1\u5b66\u3001\u592a\u7a7a\u63a2\u7d22\u3001\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u3001\u8ba4\u77e5\u79d1\u5b66\uff0c\u751a\u81f3\u4eba\u6587\u5b66\u79d1\u9886\u57df\u90fd\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6765\u89e3\u51b3\u5176\u9886\u57df\u4e2d\u8270\u5de8\u7684\u3001\u6570\u636e\u5904\u7406\u7e41\u91cd\u7684\u95ee\u9898\u3002</p> <p>\u673a\u5668\u5b66\u4e60\u901a\u8fc7\u4ece\u771f\u5b9e\u4e16\u754c\u6216\u751f\u6210\u7684\u6570\u636e\u4e2d\u53d1\u73b0\u6709\u610f\u4e49\u7684\u89c1\u89e3\uff0c\u81ea\u52a8\u5316\u4e86\u6a21\u5f0f\u53d1\u73b0\u7684\u8fc7\u7a0b\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u5b83\u5728\u5546\u4e1a\u3001\u5065\u5eb7\u548c\u91d1\u878d\u5e94\u7528\u7b49\u65b9\u9762\u5177\u6709\u5f88\u9ad8\u7684\u4ef7\u503c\u3002</p> <p>\u5728\u4e0d\u4e45\u7684\u5c06\u6765\uff0c\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7840\u77e5\u8bc6\u5c06\u6210\u4e3a\u4efb\u4f55\u9886\u57df\u7684\u4eba\u4eec\u7684\u5fc5\u4fee\u8bfe\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_10","title":"\ud83d\ude80 \u6311\u6218","text":"<p>\u5728\u7eb8\u4e0a\u6216\u4f7f\u7528 Excalidraw \u7b49\u5728\u7ebf\u5e94\u7528\u7a0b\u5e8f\u7ed8\u5236\u8349\u56fe\uff0c\u4e86\u89e3\u4f60\u5bf9 AI\u3001ML\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u636e\u79d1\u5b66\u4e4b\u95f4\u5dee\u5f02\u7684\u7406\u89e3\u3002\u6dfb\u52a0\u4e00\u4e9b\u5173\u4e8e\u8fd9\u4e9b\u6280\u672f\u64c5\u957f\u89e3\u51b3\u7684\u95ee\u9898\u7684\u60f3\u6cd5\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_11","title":"\u9605\u8bfb\u540e\u6d4b\u9a8c","text":""},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_12","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u8981\u4e86\u89e3\u6709\u5173\u5982\u4f55\u5728\u4e91\u4e2d\u4f7f\u7528 ML \u7b97\u6cd5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u9075\u5faa\u4ee5\u4e0b\u5b66\u4e60\u8def\u5f84\u3002</p>"},{"location":"1-Introduction/1-intro-to-ML/README.zh-cn/#_13","title":"\u4efb\u52a1","text":"<p>\u542f\u52a8\u5e76\u8fd0\u884c</p>"},{"location":"1-Introduction/1-intro-to-ML/assignment/","title":"Get Up and Running","text":""},{"location":"1-Introduction/1-intro-to-ML/assignment/#instructions","title":"Instructions","text":"<p>In this non-graded assignment, you should brush up on Python and get your environment up and running and able to run notebooks.</p> <p>Take this Python Learning Path, and then get your systems setup by going through these introductory videos:</p> <p>https://www.youtube.com/playlist?list=PLlrxD0HtieHhS8VzuMCfQD4uJ9yne1mE6</p>"},{"location":"1-Introduction/1-intro-to-ML/assignment.zh-cn/","title":"\u542f\u52a8\u548c\u8fd0\u884c","text":""},{"location":"1-Introduction/1-intro-to-ML/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u8fd9\u4e2a\u4e0d\u8bc4\u5206\u7684\u4f5c\u4e1a\u4e2d\uff0c\u4f60\u5e94\u8be5\u6e29\u4e60\u4e00\u4e0b Python\uff0c\u5c06 Python \u73af\u5883\u80fd\u591f\u8fd0\u884c\u8d77\u6765\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fd0\u884c notebooks\u3002</p> <p>\u5b66\u4e60\u8fd9\u4e2a Python \u5b66\u4e60\u8def\u5f84\uff0c\u7136\u540e\u901a\u8fc7\u8fd9\u4e9b\u4ecb\u7ecd\u6027\u7684\u89c6\u9891\u5c06\u4f60\u7684\u7cfb\u7edf\u73af\u5883\u8bbe\u7f6e\u597d\uff1a</p> <p>https://www.youtube.com/playlist?list=PLlrxD0HtieHhS8VzuMCfQD4uJ9yne1mE6</p>"},{"location":"1-Introduction/2-history-of-ML/","title":"History of machine learning","text":"<p>Sketchnote by Tomomi Imura</p>"},{"location":"1-Introduction/2-history-of-ML/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>\ud83c\udfa5 Click the image above for a short video working through this lesson.</p> <p>In this lesson, we will walk through the major milestones in the history of machine learning and artificial intelligence.</p> <p>The history of artificial intelligence (AI) as a field is intertwined with the history of machine learning, as the algorithms and computational advances that underpin ML fed into the development of AI. It is useful to remember that, while these fields as distinct areas of inquiry began to crystallize in the 1950s, important algorithmic, statistical, mathematical, computational and technical discoveries predated and overlapped this era. In fact, people have been thinking about these questions for hundreds of years: this article discusses the historical intellectual underpinnings of the idea of a 'thinking machine.'</p>"},{"location":"1-Introduction/2-history-of-ML/#notable-discoveries","title":"Notable discoveries","text":"<ul> <li>1763, 1812 Bayes Theorem and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge.</li> <li>1805 Least Square Theory by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting.</li> <li>1913 Markov Chains, named after Russian mathematician Andrey Markov, is used to describe a sequence of possible events based on a previous state.</li> <li>1957 Perceptron is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning.</li> </ul> <ul> <li>1967 Nearest Neighbor is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns.</li> <li>1970 Backpropagation is used to train feedforward neural networks.</li> <li>1982 Recurrent Neural Networks are artificial neural networks derived from feedforward neural networks that create temporal graphs.</li> </ul> <p>\u2705 Do a little research. What other dates stand out as pivotal in the history of ML and AI?</p>"},{"location":"1-Introduction/2-history-of-ML/#1950-machines-that-think","title":"1950: Machines that think","text":"<p>Alan Turing, a truly remarkable person who was voted by the public in 2019 as the greatest scientist of the 20th century, is credited as helping to lay the foundation for the concept of a 'machine that can think.' He grappled with naysayers and his own need for empirical evidence of this concept in part by creating the Turing Test, which you will explore in our NLP lessons.</p>"},{"location":"1-Introduction/2-history-of-ML/#1956-dartmouth-summer-research-project","title":"1956: Dartmouth Summer Research Project","text":"<p>\"The Dartmouth Summer Research Project on artificial intelligence was a seminal event for artificial intelligence as a field,\" and it was here that the term 'artificial intelligence' was coined (source).</p> <p>Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.</p> <p>The lead researcher, mathematics professor John McCarthy, hoped \"to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" The participants included another luminary in the field, Marvin Minsky.</p> <p>The workshop is credited with having initiated and encouraged several discussions including \"the rise of symbolic methods, systems focussed on limited domains (early expert systems), and deductive systems versus inductive systems.\" (source).</p>"},{"location":"1-Introduction/2-history-of-ML/#1956-1974-the-golden-years","title":"1956 - 1974: \"The golden years\"","text":"<p>From the 1950s through the mid '70s, optimism ran high in the hope that AI could solve many problems. In 1967, Marvin Minsky stated confidently that \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.\" (Minsky, Marvin (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.: Prentice-Hall)</p> <p>natural language processing research flourished, search was refined and made more powerful, and the concept of 'micro-worlds' was created, where simple tasks were completed using plain language instructions.</p> <p>Research was well funded by government agencies, advances were made in computation and algorithms, and prototypes of intelligent machines were built. Some of these machines include:</p> <ul> <li> <p>Shakey the robot, who could maneuver and decide how to perform tasks 'intelligently'.</p> <p></p> <p>Shakey in 1972</p> </li> </ul> <ul> <li> <p>Eliza, an early 'chatterbot', could converse with people and act as a primitive 'therapist'. You'll learn more about Eliza in the NLP lessons.</p> <p></p> <p>A version of Eliza, a chatbot</p> </li> </ul> <ul> <li> <p>\"Blocks world\" was an example of a micro-world where blocks could be stacked and sorted, and experiments in teaching machines to make decisions could be tested. Advances built with libraries such as SHRDLU helped propel language processing forward.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Blocks world with SHRDLU</p> </li> </ul>"},{"location":"1-Introduction/2-history-of-ML/#1974-1980-ai-winter","title":"1974 - 1980: \"AI Winter\"","text":""},{"location":"1-Introduction/2-history-of-ML/#by-the-mid-1970s-it-had-become-apparent-that-the-complexity-of-making-intelligent-machines-had-been-understated-and-that-its-promise-given-the-available-compute-power-had-been-overblown-funding-dried-up-and-confidence-in-the-field-slowed-some-issues-that-impacted-confidence-included","title":"By the mid 1970s, it had become apparent that the complexity of making 'intelligent machines' had been understated and that its promise, given the available compute power, had been overblown. Funding dried up and confidence in the field slowed. Some issues that impacted confidence included:","text":"<ul> <li>Limitations. Compute power was too limited.</li> <li>Combinatorial explosion. The amount of parameters needed to be trained grew exponentially as more was asked of computers, without a parallel evolution of compute power and capability.</li> <li>Paucity of data. There was a paucity of data that hindered the process of testing, developing, and refining algorithms.</li> <li>Are we asking the right questions?. The very questions that were being asked began to be questioned. Researchers began to field criticism about their approaches:</li> <li>Turing tests came into question by means, among other ideas, of the 'chinese room theory' which posited that, \"programming a digital computer may make it appear to understand language but could not produce real understanding.\" (source)</li> <li>The ethics of introducing artificial intelligences such as the \"therapist\" ELIZA into society was challenged.</li> </ul> <p>At the same time, various AI schools of thought began to form. A dichotomy was established between \"scruffy\" vs. \"neat AI\" practices. Scruffy labs tweaked programs for hours until they had the desired results. Neat labs \"focused on logic and formal problem solving\". ELIZA and SHRDLU were well-known scruffy systems. In the 1980s, as demand emerged to make ML systems reproducible, the neat approach gradually took the forefront as its results are more explainable.</p>"},{"location":"1-Introduction/2-history-of-ML/#1980s-expert-systems","title":"1980s Expert systems","text":"<p>As the field grew, its benefit to business became clearer, and in the 1980s so did the proliferation of 'expert systems'. \"Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\" (source).</p> <p>This type of system is actually hybrid, consisting partially of a rules engine defining business requirements, and an inference engine that leveraged the rules system to deduce new facts.</p> <p>This era also saw increasing attention paid to neural networks.</p>"},{"location":"1-Introduction/2-history-of-ML/#1987-1993-ai-chill","title":"1987 - 1993: AI 'Chill'","text":"<p>The proliferation of specialized expert systems hardware had the unfortunate effect of becoming too specialized. The rise of personal computers also competed with these large, specialized, centralized systems. The democratization of computing had begun, and it eventually paved the way for the modern explosion of big data.</p>"},{"location":"1-Introduction/2-history-of-ML/#1993-2011","title":"1993 - 2011","text":"<p>This epoch saw a new era for ML and AI to be able to solve some of the problems that had been caused earlier by the lack of data and compute power. The amount of data began to rapidly increase and become more widely available, for better and for worse, especially with the advent of the smartphone around 2007. Compute power expanded exponentially, and algorithms evolved alongside. The field began to gain maturity as the freewheeling days of the past began to crystallize into a true discipline.</p>"},{"location":"1-Introduction/2-history-of-ML/#now","title":"Now","text":"<p>Today machine learning and AI touch almost every part of our lives. This era calls for careful understanding of the risks and potentials effects of these algorithms on human lives. As Microsoft's Brad Smith has stated, \"Information technology raises issues that go to the heart of fundamental human-rights protections like privacy and freedom of expression. These issues heighten responsibility for tech companies that create these products. In our view, they also call for thoughtful government regulation and for the development of norms around acceptable uses\" (source).</p> <p>It remains to be seen what the future holds, but it is important to understand these computer systems and the software and algorithms that they run. We hope that this curriculum will help you to gain a better understanding so that you can decide for yourself.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Yann LeCun discusses the history of deep learning in this lecture</p>"},{"location":"1-Introduction/2-history-of-ML/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Dig into one of these historical moments and learn more about the people behind them. There are fascinating characters, and no scientific discovery was ever created in a cultural vacuum. What do you discover?</p>"},{"location":"1-Introduction/2-history-of-ML/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"1-Introduction/2-history-of-ML/#review-self-study","title":"Review &amp; Self Study","text":"<p>Here are items to watch and listen to:</p> <p>This podcast where Amy Boyd discusses the evolution of AI</p> <p></p>"},{"location":"1-Introduction/2-history-of-ML/#assignment","title":"Assignment","text":"<p>Create a timeline</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2","text":"<p>\u4f5c\u8005 Tomomi Imura</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b\u9a8c","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u8d70\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u5386\u53f2\u4e0a\u7684\u4e3b\u8981\u91cc\u7a0b\u7891\u3002 </p> <p>\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4f5c\u4e3a\u4e00\u4e2a\u9886\u57df\u7684\u5386\u53f2\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2\u4ea4\u7ec7\u5728\u4e00\u8d77\uff0c\u56e0\u4e3a\u652f\u6301\u673a\u5668\u5b66\u4e60\u7684\u7b97\u6cd5\u548c\u8ba1\u7b97\u80fd\u529b\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86AI\u7684\u53d1\u5c55\u3002\u8bb0\u4f4f\uff0c\u867d\u7136\u8fd9\u4e9b\u9886\u57df\u4f5c\u4e3a\u4e0d\u540c\u7814\u7a76\u9886\u57df\u5728 20 \u4e16\u7eaa 50 \u5e74\u4ee3\u624d\u5f00\u59cb\u5177\u4f53\u5316\uff0c\u4f46\u91cd\u8981\u7684\u7b97\u6cd5\u3001\u7edf\u8ba1\u3001\u6570\u5b66\u3001\u8ba1\u7b97\u548c\u6280\u672f\u53d1\u73b0 \u8981\u65e9\u4e8e\u548c\u91cd\u53e0\u4e86\u8fd9\u4e2a\u65f6\u4ee3\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u6570\u767e\u5e74\u6765\u4eba\u4eec\u4e00\u76f4\u5728\u601d\u8003\u8fd9\u4e9b\u95ee\u9898\uff1a\u672c\u6587\u8ba8\u8bba\u4e86\u201c\u601d\u7ef4\u673a\u5668\u201d\u8fd9\u4e00\u6982\u5ff5\u7684\u5386\u53f2\u77e5\u8bc6\u57fa\u7840\u3002 </p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_3","title":"\u4e3b\u8981\u53d1\u73b0","text":"<ul> <li>1763, 1812 \u8d1d\u53f6\u65af\u5b9a\u7406 \u53ca\u5176\u524d\u8eab\u3002\u8be5\u5b9a\u7406\u53ca\u5176\u5e94\u7528\u662f\u63a8\u7406\u7684\u57fa\u7840\uff0c\u63cf\u8ff0\u4e86\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\u3002</li> <li>1805 \u6700\u5c0f\u4e8c\u4e58\u7406\u8bba\u7531\u6cd5\u56fd\u6570\u5b66\u5bb6 Adrien-Marie Legendre \u63d0\u51fa\u3002 \u4f60\u5c06\u5728\u6211\u4eec\u7684\u56de\u5f52\u5355\u5143\u4e2d\u4e86\u89e3\u8fd9\u4e00\u7406\u8bba\uff0c\u5b83\u6709\u52a9\u4e8e\u6570\u636e\u62df\u5408\u3002</li> <li>1913 \u9a6c\u5c14\u53ef\u592b\u94fe\u4ee5\u4fc4\u7f57\u65af\u6570\u5b66\u5bb6 Andrey Markov \u7684\u540d\u5b57\u547d\u540d\uff0c\u7528\u4e8e\u63cf\u8ff0\u57fa\u4e8e\u5148\u524d\u72b6\u6001\u7684\u4e00\u7cfb\u5217\u53ef\u80fd\u4e8b\u4ef6\u3002</li> <li>1957 \u611f\u77e5\u5668\u662f\u7f8e\u56fd\u5fc3\u7406\u5b66\u5bb6 Frank Rosenblatt \u53d1\u660e\u7684\u4e00\u79cd\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u662f\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\u7684\u57fa\u7840\u3002</li> <li>1967 \u6700\u8fd1\u90bb\u662f\u4e00\u79cd\u6700\u521d\u8bbe\u8ba1\u7528\u4e8e\u6620\u5c04\u8def\u7ebf\u7684\u7b97\u6cd5\u3002 \u5728 ML \u4e2d\uff0c\u5b83\u7528\u4e8e\u68c0\u6d4b\u6a21\u5f0f\u3002</li> <li>1970 \u53cd\u5411\u4f20\u64ad\u7528\u4e8e\u8bad\u7ec3\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u3002</li> <li>1982 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc \u662f\u6e90\u81ea\u4ea7\u751f\u65f6\u95f4\u56fe\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u3002</li> </ul> <p>\u2705 \u505a\u70b9\u8c03\u67e5\u3002\u5728 ML \u548c AI \u7684\u5386\u53f2\u4e0a\uff0c\u8fd8\u6709\u54ea\u4e9b\u65e5\u671f\u662f\u91cd\u8981\u7684\uff1f</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1950","title":"1950: \u4f1a\u601d\u8003\u7684\u673a\u5668","text":"<p>Alan Turing\uff0c\u4e00\u4e2a\u771f\u6b63\u6770\u51fa\u7684\u4eba\uff0c\u5728 2019 \u5e74\u88ab\u516c\u4f17\u6295\u7968\u9009\u51fa \u4f5c\u4e3a 20 \u4e16\u7eaa\u6700\u4f1f\u5927\u7684\u79d1\u5b66\u5bb6\uff0c\u4ed6\u8ba4\u4e3a\u6709\u52a9\u4e8e\u4e3a\u201c\u4f1a\u601d\u8003\u7684\u673a\u5668\u201d\u7684\u6982\u5ff5\u6253\u4e0b\u57fa\u7840\u3002\u4ed6\u901a\u8fc7\u521b\u5efa \u56fe\u7075\u6d4b\u8bd5\u6765\u89e3\u51b3\u53cd\u5bf9\u8005\u548c\u4ed6\u81ea\u5df1\u5bf9\u8fd9\u4e00\u6982\u5ff5\u7684\u7ecf\u9a8c\u8bc1\u636e\u7684\u9700\u6c42\uff0c\u4f60\u5c06\u5728\u6211\u4eec\u7684 NLP \u8bfe\u7a0b\u4e2d\u8fdb\u884c\u63a2\u7d22\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1956","title":"1956: \u8fbe\u7279\u8305\u65af\u590f\u5b63\u7814\u7a76\u9879\u76ee","text":"<p>\u201c\u8fbe\u7279\u8305\u65af\u590f\u5b63\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9879\u76ee\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u4e2a\u5f00\u521b\u6027\u4e8b\u4ef6\uff0c\u201d\u6b63\u662f\u5728\u8fd9\u91cc\uff0c\u4eba\u4eec\u521b\u9020\u4e86\u201c\u4eba\u5de5\u667a\u80fd\u201d\u4e00\u8bcd\uff08\u6765\u6e90\uff09\u3002</p> <p>\u539f\u5219\u4e0a\uff0c\u5b66\u4e60\u7684\u6bcf\u4e2a\u65b9\u9762\u6216\u667a\u80fd\u7684\u4efb\u4f55\u5176\u4ed6\u7279\u5f81\u90fd\u53ef\u4ee5\u88ab\u7cbe\u786e\u5730\u63cf\u8ff0\uff0c\u4ee5\u81f3\u4e8e\u53ef\u4ee5\u7528\u673a\u5668\u6765\u6a21\u62df\u5b83\u3002 </p> <p>\u9996\u5e2d\u7814\u7a76\u5458\u3001\u6570\u5b66\u6559\u6388 John McCarthy \u5e0c\u671b\u201c\u57fa\u4e8e\u8fd9\u6837\u4e00\u79cd\u731c\u60f3\uff0c\u5373\u5b66\u4e60\u7684\u6bcf\u4e2a\u65b9\u9762\u6216\u667a\u80fd\u7684\u4efb\u4f55\u5176\u4ed6\u7279\u5f81\u539f\u5219\u4e0a\u90fd\u53ef\u4ee5\u5982\u6b64\u7cbe\u786e\u5730\u63cf\u8ff0\uff0c\u4ee5\u81f3\u4e8e\u53ef\u4ee5\u5236\u9020\u51fa\u4e00\u53f0\u673a\u5668\u6765\u6a21\u62df\u5b83\u3002\u201d \u53c2\u4e0e\u8005\u5305\u62ec\u8be5\u9886\u57df\u7684\u53e6\u4e00\u4f4d\u6770\u51fa\u4eba\u7269 Marvin Minsky\u3002</p> <p>\u7814\u8ba8\u4f1a\u88ab\u8ba4\u4e3a\u53d1\u8d77\u5e76\u9f13\u52b1\u4e86\u4e00\u4e9b\u8ba8\u8bba\uff0c\u5305\u62ec\u201c\u7b26\u53f7\u65b9\u6cd5\u7684\u5174\u8d77\u3001\u4e13\u6ce8\u4e8e\u6709\u9650\u9886\u57df\u7684\u7cfb\u7edf\uff08\u65e9\u671f\u4e13\u5bb6\u7cfb\u7edf\uff09\uff0c\u4ee5\u53ca\u6f14\u7ece\u7cfb\u7edf\u4e0e\u5f52\u7eb3\u7cfb\u7edf\u7684\u5bf9\u6bd4\u3002\u201d\uff08\u6765\u6e90\uff09\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1956-1974","title":"1956 - 1974: \u201c\u9ec4\u91d1\u5c81\u6708\u201d","text":"<p>\u4ece 20 \u4e16\u7eaa 50 \u5e74\u4ee3\u5230 70 \u5e74\u4ee3\u4e2d\u671f\uff0c\u4e50\u89c2\u60c5\u7eea\u9ad8\u6da8\uff0c\u5e0c\u671b\u4eba\u5de5\u667a\u80fd\u80fd\u591f\u89e3\u51b3\u8bb8\u591a\u95ee\u9898\u30021967 \u5e74\uff0cMarvin Minsky \u81ea\u4fe1\u5730\u8bf4\uff0c\u201c\u4e00\u4ee3\u4eba\u4e4b\u5185...\u521b\u9020\u2018\u4eba\u5de5\u667a\u80fd\u2019\u7684\u95ee\u9898\u5c06\u5f97\u5230\u5b9e\u8d28\u6027\u7684\u89e3\u51b3\u3002\u201d\uff08Minsky\uff0cMarvin\uff081967\uff09\uff0c\u300a\u8ba1\u7b97\uff1a\u6709\u9650\u548c\u65e0\u9650\u673a\u5668\u300b\uff0c\u65b0\u6cfd\u897f\u5dde\u6069\u683c\u4f0d\u5fb7\u514b\u5229\u592b\u65af\uff1aPrentice Hall\uff09</p> <p>\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u84ec\u52c3\u53d1\u5c55\uff0c\u641c\u7d22\u88ab\u63d0\u70bc\u5e76\u53d8\u5f97\u66f4\u52a0\u5f3a\u5927\uff0c\u521b\u9020\u4e86\u201c\u5fae\u89c2\u4e16\u754c\u201d\u7684\u6982\u5ff5\uff0c\u5728\u8fd9\u4e2a\u6982\u5ff5\u4e2d\uff0c\u7b80\u5355\u7684\u4efb\u52a1\u662f\u7528\u7b80\u5355\u7684\u8bed\u8a00\u6307\u4ee4\u5b8c\u6210\u7684\u3002</p> <p>\u8fd9\u9879\u7814\u7a76\u5f97\u5230\u4e86\u653f\u5e9c\u673a\u6784\u7684\u5145\u5206\u8d44\u52a9\uff0c\u5728\u8ba1\u7b97\u548c\u7b97\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u5e76\u5efa\u9020\u4e86\u667a\u80fd\u673a\u5668\u7684\u539f\u578b\u3002\u5176\u4e2d\u4e00\u4e9b\u673a\u5668\u5305\u62ec\uff1a</p> <ul> <li> <p>\u673a\u5668\u4eba Shakey\uff0c\u4ed6\u4eec\u53ef\u4ee5\u201c\u806a\u660e\u5730\u201d\u64cd\u7eb5\u548c\u51b3\u5b9a\u5982\u4f55\u6267\u884c\u4efb\u52a1\u3002</p> <p></p> <p>1972 \u5e74\u7684 Shakey</p> </li> <li> <p>Eliza\uff0c\u4e00\u4e2a\u65e9\u671f\u7684\u201c\u804a\u5929\u673a\u5668\u4eba\u201d\uff0c\u53ef\u4ee5\u4e0e\u4eba\u4ea4\u8c08\u5e76\u5145\u5f53\u539f\u59cb\u7684\u201c\u6cbb\u7597\u5e08\u201d\u3002 \u4f60\u5c06\u5728 NLP \u8bfe\u7a0b\u4e2d\u4e86\u89e3\u6709\u5173 Eliza \u7684\u66f4\u591a\u4fe1\u606f\u3002 </p> <p></p> <p>Eliza \u7684\u4e00\u4e2a\u7248\u672c\uff0c\u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba </p> </li> <li> <p>\u201c\u79ef\u6728\u4e16\u754c\u201d\u662f\u4e00\u4e2a\u5fae\u89c2\u4e16\u754c\u7684\u4f8b\u5b50\uff0c\u5728\u90a3\u91cc\u79ef\u6728\u53ef\u4ee5\u5806\u53e0\u548c\u5206\u7c7b\uff0c\u5e76\u4e14\u53ef\u4ee5\u6d4b\u8bd5\u6559\u673a\u5668\u505a\u51fa\u51b3\u7b56\u7684\u5b9e\u9a8c\u3002 \u4f7f\u7528 SHRDLU \u7b49\u5e93\u6784\u5efa\u7684\u9ad8\u7ea7\u529f\u80fd\u6709\u52a9\u4e8e\u63a8\u52a8\u8bed\u8a00\u5904\u7406\u5411\u524d\u53d1\u5c55\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1a \u79ef\u6728\u4e16\u754c\u4e0e SHRDLU</p> </li> </ul>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1974-1980-ai","title":"1974 - 1980: AI \u7684\u5bd2\u51ac","text":"<p>\u5230\u4e86 20 \u4e16\u7eaa 70 \u5e74\u4ee3\u4e2d\u671f\uff0c\u5f88\u660e\u663e\u5236\u9020\u201c\u667a\u80fd\u673a\u5668\u201d\u7684\u590d\u6742\u6027\u88ab\u4f4e\u4f30\u4e86\uff0c\u800c\u4e14\u8003\u8651\u5230\u53ef\u7528\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u5b83\u7684\u524d\u666f\u88ab\u5938\u5927\u4e86\u3002\u8d44\u91d1\u67af\u7aed\uff0c\u5e02\u573a\u4fe1\u5fc3\u653e\u7f13\u3002\u5f71\u54cd\u4fe1\u5fc3\u7684\u4e00\u4e9b\u95ee\u9898\u5305\u62ec\uff1a</p> <ul> <li>\u9650\u5236\u3002\u8ba1\u7b97\u80fd\u529b\u592a\u6709\u9650\u4e86</li> <li>\u7ec4\u5408\u7206\u70b8\u3002\u968f\u7740\u5bf9\u8ba1\u7b97\u673a\u7684\u8981\u6c42\u8d8a\u6765\u8d8a\u9ad8\uff0c\u9700\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u6570\u91cf\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u800c\u8ba1\u7b97\u80fd\u529b\u5374\u6ca1\u6709\u5e73\u884c\u53d1\u5c55\u3002</li> <li>\u7f3a\u4e4f\u6570\u636e\u3002 \u7f3a\u4e4f\u6570\u636e\u963b\u788d\u4e86\u6d4b\u8bd5\u3001\u5f00\u53d1\u548c\u6539\u8fdb\u7b97\u6cd5\u7684\u8fc7\u7a0b\u3002 </li> <li>\u6211\u4eec\u662f\u5426\u5728\u95ee\u6b63\u786e\u7684\u95ee\u9898\uff1f\u3002 \u88ab\u95ee\u5230\u7684\u95ee\u9898\u4e5f\u5f00\u59cb\u53d7\u5230\u8d28\u7591\u3002 \u7814\u7a76\u4eba\u5458\u5f00\u59cb\u5bf9\u4ed6\u4eec\u7684\u65b9\u6cd5\u63d0\u51fa\u6279\u8bc4\uff1a </li> <li>\u56fe\u7075\u6d4b\u8bd5\u53d7\u5230\u8d28\u7591\u7684\u65b9\u6cd5\u4e4b\u4e00\u662f\u201c\u4e2d\u56fd\u623f\u95f4\u7406\u8bba\u201d\uff0c\u8be5\u7406\u8bba\u8ba4\u4e3a\uff0c\u201c\u5bf9\u6570\u5b57\u8ba1\u7b97\u673a\u8fdb\u884c\u7f16\u7a0b\u53ef\u80fd\u4f7f\u5176\u770b\u8d77\u6765\u80fd\u7406\u89e3\u8bed\u8a00\uff0c\u4f46\u4e0d\u80fd\u4ea7\u751f\u771f\u6b63\u7684\u7406\u89e3\u3002\u201d (\u6765\u6e90)</li> <li>\u5c06\u201c\u6cbb\u7597\u5e08\u201dELIZA \u8fd9\u6837\u7684\u4eba\u5de5\u667a\u80fd\u5f15\u5165\u793e\u4f1a\u7684\u4f26\u7406\u53d7\u5230\u4e86\u6311\u6218\u3002</li> </ul> <p>\u4e0e\u6b64\u540c\u65f6\uff0c\u5404\u79cd\u4eba\u5de5\u667a\u80fd\u5b66\u6d3e\u5f00\u59cb\u5f62\u6210\u3002 \u5728 \u201cscruffy\u201d \u4e0e \u201cneat AI\u201d \u4e4b\u95f4\u5efa\u7acb\u4e86\u4e8c\u5206\u6cd5\u3002 Scruffy \u5b9e\u9a8c\u5ba4\u5bf9\u7a0b\u5e8f\u8fdb\u884c\u4e86\u6570\u5c0f\u65f6\u7684\u8c03\u6574\uff0c\u76f4\u5230\u83b7\u5f97\u6240\u9700\u7684\u7ed3\u679c\u3002 Neat \u5b9e\u9a8c\u5ba4\u201c\u4e13\u6ce8\u4e8e\u903b\u8f91\u548c\u5f62\u5f0f\u95ee\u9898\u7684\u89e3\u51b3\u201d\u3002 ELIZA \u548c SHRDLU \u662f\u4f17\u6240\u5468\u77e5\u7684 scruffy \u7cfb\u7edf\u3002 \u5728 1980 \u5e74\u4ee3\uff0c\u968f\u7740\u4f7f ML \u7cfb\u7edf\u53ef\u91cd\u73b0\u7684\u9700\u6c42\u51fa\u73b0\uff0cneat \u65b9\u6cd5\u9010\u6e10\u8d70\u4e0a\u524d\u6cbf\uff0c\u56e0\u4e3a\u5176\u7ed3\u679c\u66f4\u6613\u4e8e\u89e3\u91ca\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1980s","title":"1980s \u4e13\u5bb6\u7cfb\u7edf","text":"<p>\u968f\u7740\u8fd9\u4e2a\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5b83\u5bf9\u5546\u4e1a\u7684\u597d\u5904\u53d8\u5f97\u8d8a\u6765\u8d8a\u660e\u663e\uff0c\u5728 20 \u4e16\u7eaa 80 \u5e74\u4ee3\uff0c\u2018\u4e13\u5bb6\u7cfb\u7edf\u2019\u4e5f\u5f00\u59cb\u5e7f\u6cdb\u6d41\u884c\u8d77\u6765\u3002\u201c\u4e13\u5bb6\u7cfb\u7edf\u662f\u9996\u6279\u771f\u6b63\u6210\u529f\u7684\u4eba\u5de5\u667a\u80fd (AI) \u8f6f\u4ef6\u5f62\u5f0f\u4e4b\u4e00\u3002\u201d \uff08\u6765\u6e90\uff09\u3002</p> <p>\u8fd9\u79cd\u7c7b\u578b\u7684\u7cfb\u7edf\u5b9e\u9645\u4e0a\u662f\u6df7\u5408\u7cfb\u7edf\uff0c\u90e8\u5206\u7531\u5b9a\u4e49\u4e1a\u52a1\u9700\u6c42\u7684\u89c4\u5219\u5f15\u64ce\u548c\u5229\u7528\u89c4\u5219\u7cfb\u7edf\u63a8\u65ad\u65b0\u4e8b\u5b9e\u7684\u63a8\u7406\u5f15\u64ce\u7ec4\u6210\u3002</p> <p>\u5728\u8fd9\u4e2a\u65f6\u4ee3\uff0c\u795e\u7ecf\u7f51\u7edc\u4e5f\u8d8a\u6765\u8d8a\u53d7\u5230\u91cd\u89c6\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1987-1993-ai","title":"1987 - 1993: AI \u7684\u51b7\u9759\u671f","text":"<p>\u4e13\u4e1a\u7684\u4e13\u5bb6\u7cfb\u7edf\u786c\u4ef6\u7684\u6fc0\u589e\u9020\u6210\u4e86\u8fc7\u4e8e\u4e13\u4e1a\u5316\u7684\u4e0d\u5e78\u540e\u679c\u3002\u4e2a\u4eba\u7535\u8111\u7684\u5174\u8d77\u4e5f\u4e0e\u8fd9\u4e9b\u5927\u578b\u3001\u4e13\u4e1a\u5316\u3001\u96c6\u4e2d\u5316\u7cfb\u7edf\u5c55\u5f00\u4e86\u7ade\u4e89\u3002\u8ba1\u7b97\u673a\u7684\u5e73\u6c11\u5316\u5df2\u7ecf\u5f00\u59cb\uff0c\u5b83\u6700\u7ec8\u4e3a\u5927\u6570\u636e\u7684\u73b0\u4ee3\u7206\u70b8\u94fa\u5e73\u4e86\u9053\u8def\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#1993-2011","title":"1993 - 2011","text":"<p>\u8fd9\u4e2a\u65f6\u4ee3\u89c1\u8bc1\u4e86\u4e00\u4e2a\u65b0\u7684\u65f6\u4ee3\uff0cML \u548c AI \u80fd\u591f\u89e3\u51b3\u65e9\u671f\u7531\u4e8e\u7f3a\u4e4f\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u800c\u5bfc\u81f4\u7684\u4e00\u4e9b\u95ee\u9898\u3002\u6570\u636e\u91cf\u5f00\u59cb\u8fc5\u901f\u589e\u52a0\uff0c\u53d8\u5f97\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u65e0\u8bba\u597d\u574f\uff0c\u5c24\u5176\u662f 2007 \u5e74\u5de6\u53f3\u667a\u80fd\u624b\u673a\u7684\u51fa\u73b0\uff0c\u8ba1\u7b97\u80fd\u529b\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7b97\u6cd5\u4e5f\u968f\u4e4b\u53d1\u5c55\u3002\u8fd9\u4e2a\u9886\u57df\u5f00\u59cb\u53d8\u5f97\u6210\u719f\uff0c\u56e0\u4e3a\u8fc7\u53bb\u90a3\u4e9b\u968f\u5fc3\u6240\u6b32\u7684\u65e5\u5b50\u5f00\u59cb\u5177\u4f53\u5316\u4e3a\u4e00\u79cd\u771f\u6b63\u7684\u7eaa\u5f8b\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_4","title":"\u73b0\u5728","text":"<p>\u4eca\u5929\uff0c\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u51e0\u4e4e\u89e6\u53ca\u6211\u4eec\u751f\u6d3b\u7684\u6bcf\u4e00\u4e2a\u90e8\u5206\u3002\u8fd9\u4e2a\u65f6\u4ee3\u8981\u6c42\u4ed4\u7ec6\u4e86\u89e3\u8fd9\u4e9b\u7b97\u6cd5\u5bf9\u4eba\u7c7b\u751f\u6d3b\u7684\u98ce\u9669\u548c\u6f5c\u5728\u5f71\u54cd\u3002\u6b63\u5982\u5fae\u8f6f\u7684 Brad Smith \u6240\u8a00\uff0c\u201c\u4fe1\u606f\u6280\u672f\u5f15\u53d1\u7684\u95ee\u9898\u89e6\u53ca\u9690\u79c1\u548c\u8a00\u8bba\u81ea\u7531\u7b49\u57fa\u672c\u4eba\u6743\u4fdd\u62a4\u7684\u6838\u5fc3\u3002\u8fd9\u4e9b\u95ee\u9898\u52a0\u91cd\u4e86\u5236\u9020\u8fd9\u4e9b\u4ea7\u54c1\u7684\u79d1\u6280\u516c\u53f8\u7684\u8d23\u4efb\u3002\u5728\u6211\u4eec\u770b\u6765\uff0c\u5b83\u4eec\u8fd8\u547c\u5401\u653f\u5e9c\u8fdb\u884c\u6df1\u601d\u719f\u8651\u7684\u76d1\u7ba1\uff0c\u5e76\u56f4\u7ed5\u53ef\u63a5\u53d7\u7684\u7528\u9014\u5236\u5b9a\u89c4\u8303\u201d\uff08\u6765\u6e90\uff09\u3002</p> <p>\u672a\u6765\u7684\u60c5\u51b5\u8fd8\u6709\u5f85\u89c2\u5bdf\uff0c\u4f46\u4e86\u89e3\u8fd9\u4e9b\u8ba1\u7b97\u673a\u7cfb\u7edf\u4ee5\u53ca\u5b83\u4eec\u8fd0\u884c\u7684\u8f6f\u4ef6\u548c\u7b97\u6cd5\u662f\u5f88\u91cd\u8981\u7684\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u95e8\u8bfe\u7a0b\u80fd\u5e2e\u52a9\u4f60\u66f4\u597d\u7684\u7406\u89e3\uff0c\u4ee5\u4fbf\u4f60\u81ea\u5df1\u51b3\u5b9a\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1aYann LeCun \u5728\u672c\u6b21\u8bb2\u5ea7\u4e2d\u8ba8\u8bba\u6df1\u5ea6\u5b66\u4e60\u7684\u5386\u53f2 </p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_5","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u6df1\u5165\u4e86\u89e3\u8fd9\u4e9b\u5386\u53f2\u65f6\u523b\u4e4b\u4e00\uff0c\u5e76\u66f4\u591a\u5730\u4e86\u89e3\u5b83\u4eec\u80cc\u540e\u7684\u4eba\u3002\u8fd9\u91cc\u6709\u8bb8\u591a\u5f15\u4eba\u5165\u80dc\u7684\u4eba\u7269\uff0c\u6ca1\u6709\u4e00\u9879\u79d1\u5b66\u53d1\u73b0\u662f\u5728\u6587\u5316\u771f\u7a7a\u4e2d\u521b\u9020\u51fa\u6765\u7684\u3002\u4f60\u53d1\u73b0\u4e86\u4ec0\u4e48\uff1f</p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_6","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_7","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u4ee5\u4e0b\u662f\u8981\u89c2\u770b\u548c\u6536\u542c\u7684\u8282\u76ee\uff1a</p> <p>\u8fd9\u662f Amy Boyd \u8ba8\u8bba\u4eba\u5de5\u667a\u80fd\u8fdb\u5316\u7684\u64ad\u5ba2</p> <p></p>"},{"location":"1-Introduction/2-history-of-ML/README.zh-cn/#_8","title":"\u4efb\u52a1","text":"<p>\u521b\u5efa\u65f6\u95f4\u7ebf</p>"},{"location":"1-Introduction/2-history-of-ML/assignment/","title":"Create a timeline","text":""},{"location":"1-Introduction/2-history-of-ML/assignment/#instructions","title":"Instructions","text":"<p>Using this repo, create a timeline of some aspect of the history of algorithms, mathematics, statistics, AI, or ML, or a combination of these. You can focus on one person, one idea, or a long timespan of thought. Make sure to add multimedia elements.</p>"},{"location":"1-Introduction/2-history-of-ML/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A deployed timeline is presented as a GitHub page The code is incomplete and not deployed The timeline is incomplete, not well researched and not deployed"},{"location":"1-Introduction/2-history-of-ML/assignment.zh-cn/","title":"\u5efa\u7acb\u4e00\u4e2a\u65f6\u95f4\u8f74","text":""},{"location":"1-Introduction/2-history-of-ML/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u4f7f\u7528\u8fd9\u4e2a \u4ed3\u5e93\uff0c\u521b\u5efa\u4e00\u4e2a\u5173\u4e8e\u7b97\u6cd5\u3001\u6570\u5b66\u3001\u7edf\u8ba1\u5b66\u3001\u4eba\u5de5\u667a\u80fd\u3001\u673a\u5668\u5b66\u4e60\u7684\u67d0\u4e2a\u65b9\u9762\u6216\u8005\u53ef\u4ee5\u7efc\u5408\u591a\u4e2a\u4ee5\u4e0a\u5b66\u79d1\u6765\u8bb2\u3002\u4f60\u53ef\u4ee5\u7740\u91cd\u4ecb\u7ecd\u67d0\u4e2a\u4eba\uff0c\u67d0\u4e2a\u60f3\u6cd5\uff0c\u6216\u8005\u4e00\u4e2a\u7ecf\u4e45\u4e0d\u8870\u7684\u601d\u60f3\u3002\u8bf7\u786e\u4fdd\u6dfb\u52a0\u4e86\u591a\u5a92\u4f53\u5143\u7d20\u5728\u4f60\u7684\u65f6\u95f4\u7ebf\u4e2d\u3002</p>"},{"location":"1-Introduction/2-history-of-ML/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u6709\u4e00\u4e2a\u7528 GitHub page \u5c55\u793a\u7684 timeline \u4ee3\u7801\u8fd8\u4e0d\u5b8c\u6574\u5e76\u4e14\u6ca1\u6709\u90e8\u7f72 \u65f6\u95f4\u7ebf\u4e0d\u5b8c\u6574\uff0c\u6ca1\u6709\u7ecf\u8fc7\u5145\u5206\u7684\u7814\u7a76\uff0c\u5e76\u4e14\u6ca1\u6709\u90e8\u7f72"},{"location":"1-Introduction/3-fairness/","title":"Building Machine Learning solutions with responsible AI","text":"<p>Sketchnote by Tomomi Imura</p>"},{"location":"1-Introduction/3-fairness/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"1-Introduction/3-fairness/#introduction","title":"Introduction","text":"<p>In this curriculum, you will start to discover how machine learning can and is impacting our everyday lives. Even now, systems and models are involved in daily decision-making tasks, such as health care diagnoses, loan approvals or detecting fraud. So, it is important that these models work well to provide outcomes that are trustworthy. Just as any software application, AI systems are going to miss expectations or have an undesirable outcome. That is why it is essential to be about to understand and explain the behavior of an AI model. </p> <p>Imagine what can happen when the data you are using to build these models lacks certain demographics, such as race, gender, political view, religion, or disproportionally represents such demographics. What about when the model\u2019s output is interpreted to favor some demographic? What is the consequence for the application? In addition, what happens when the model has an adverse outcome and is harmful to people? Who is accountable for the AI systems behavior? These are some questions we will explore in this curriculum. </p> <p>In this lesson, you will: </p> <ul> <li>Raise your awareness of the importance of fairness in machine learning and fairness-related harms.</li> <li>Become familiar with the practice of exploring outliers and unusual scenarios to ensure reliability and safety</li> <li>Gain understanding on the need to empower everyone by designing inclusive systems</li> <li>Explore how vital it is to protect privacy and security of data and people</li> <li>See the importance of having a glass box approach to explain the behavior of AI models</li> <li>Be mindful of how accountability is essential to build trust in AI systems</li> </ul>"},{"location":"1-Introduction/3-fairness/#prerequisite","title":"Prerequisite","text":"<p>As a prerequisite, please take the \"Responsible AI Principles\" Learn Path and watch the video below on the topic:</p> <p>Learn more about Responsible AI by following this Learning Path</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Microsoft's Approach to Responsible AI</p>"},{"location":"1-Introduction/3-fairness/#fairness","title":"Fairness","text":"<p>AI systems should treat everyone fairly and avoid affecting similar groups of people in different ways. For example, when AI systems provide guidance on medical treatment, loan applications, or employment, they should make the same recommendations to everyone with similar symptoms, financial circumstances, or professional qualifications. Each of us as humans carries around inherited biases that affect our decisions and actions. These biases can be evident in the data that we use to train AI systems. Such manipulation can sometimes happen unintentionally. It is often difficult to consciously know when you are introducing bias in data. </p> <p>\u201cUnfairness\u201d encompasses negative impacts, or \u201charms\u201d, for a group of people, such as those defined in terms of race, gender, age, or disability status. The main fairness-related harms can be classified as: </p> <ul> <li>Allocation, if a gender or ethnicity for example is favored over another.</li> <li>Quality of service. If you train the data for one specific scenario but reality is much more complex, it leads to a poor performing service.  For instance, a hand soap dispenser that could not seem to be able to sense people with dark skin. Reference</li> <li>Denigration. To unfairly criticize and label something or someone. For example, an image labeling technology infamously mislabeled images of dark-skinned people as gorillas.</li> <li>Over- or under- representation. The idea is that a certain group is not seen in a certain profession, and any service or function that keeps promoting that is contributing to harm.</li> <li>Stereotyping. Associating a given group with pre-assigned attributes.  For example, a language translation system betweem English and Turkish may have inaccuraces due to words with stereotypical associations to gender.</li> </ul> <p></p> <p>translation to Turkish</p> <p></p> <p>translation back to English</p> <p>When designing and testing AI systems, we need to ensure that AI is fair and not programmed to make biased or discriminatory decisions, which human beings are also prohibited from making. Guaranteeing fairness in AI and machine learning remains a complex sociotechnical challenge. </p>"},{"location":"1-Introduction/3-fairness/#reliability-and-safety","title":"Reliability and safety","text":"<p>To build trust, AI systems need to be reliable, safe, and consistent under normal and unexpected conditions. It is important to know how AI systems will behavior in a variety of situations, especially when they are outliers. When building AI solutions, there needs to be a substantial amount of focus on how to handle a wide variety of circumstances that the AI solutions would encounter. For example, a self-driving car needs to put people's safety as a top priority. As a result, the AI powering the car need to consider all the possible scenarios that the car could come across such as night, thunderstorms or blizzards, kids running across the street, pets, road constructions etc. How well an AI system can handle a wild range of conditions reliably and safely reflects the level of anticipation the data scientist or AI developer considered during the design or testing of the system.  </p> <p>\ud83c\udfa5 Click the here for a video: </p>"},{"location":"1-Introduction/3-fairness/#inclusiveness","title":"Inclusiveness","text":"<p>AI systems should be designed to engage and empower everyone. When designing and implementing AI systems data scientists and AI developers identify and address potential barriers in the system that could unintentionally exclude people. For example, there are 1 billion people with disabilities around the world. With the advancement of AI, they can access a wide range of information and opportunities more easily in their daily lives. By addressing the barriers, it creates opportunities to innovate and develop AI products with better experiences that benefit everyone. </p> <p>\ud83c\udfa5 Click the here for a video: inclusiveness in AI</p>"},{"location":"1-Introduction/3-fairness/#security-and-privacy","title":"Security and privacy","text":"<p>AI systems should be safe and respect people\u2019s privacy. People have less trust in systems that put their privacy, information, or lives at risk. When training machine learning models, we rely on data to produce the best results. In doing so, the origin of the data and integrity must be considered. For example, was the data user submitted or publicly available? Next, while working with the data, it is crucial to develop AI systems that can protect confidential information and resist attacks. As AI becomes more prevalent, protecting privacy and securing important personal and business information is becoming more critical and complex. Privacy and data security issues require especially close attention for AI because access to data is essential for AI systems to make accurate and informed predictions and decisions about people. </p> <p>\ud83c\udfa5 Click the here for a video: security in AI</p> <ul> <li>As an industry we have made significant advancements in Privacy &amp; security, fueled significantly by regulations like the GDPR (General Data Protection Regulation). </li> <li>Yet with AI systems we must acknowledge the tension between the need for more personal data to make systems more personal and effective \u2013 and privacy. </li> <li>Just like with the birth of connected computers with the internet, we are also seeing a huge uptick in the number of security issues related to AI. </li> <li>At the same time, we have seen AI being used to improve security. As an example, most modern anti-virus scanners are driven by AI heuristics today. </li> <li>We need to ensure that our Data Science processes blend harmoniously with the latest privacy and security practices. </li> </ul>"},{"location":"1-Introduction/3-fairness/#transparency","title":"Transparency","text":"<p>AI systems should be understandable. A crucial part of transparency is explaining the behavior of AI systems and their components. Improving the understanding of AI systems requires that stakeholders comprehend how and why they function so that they can identify potential performance issues, safety and privacy concerns, biases, exclusionary practices, or unintended outcomes. We also believe that those who use AI systems should be honest and forthcoming about when, why, and how they choose to deploy them. As well as the limitations of the systems they use. For example, if a bank uses an AI system to support its consumer lending decisions, it is important to examine the outcomes and understand which data influences the system\u2019s recommendations. Governments are starting to regulate AI across industries, so data scientists and organizations must explain if an AI system meets regulatory requirements, especially when there is an undesirable outcome. </p> <p>\ud83c\udfa5 Click the here for a video: transparency in AI</p> <ul> <li>Because AI systems are so complex, it is hard to understand how they work and interpret the results. </li> <li>This lack of understanding affects the way these systems are managed, operationalized, and documented. </li> <li>This lack of understanding more importantly affects the decisions made using the results these systems produce. </li> </ul>"},{"location":"1-Introduction/3-fairness/#accountability","title":"Accountability","text":"<p>The people who design and deploy AI systems must be accountable for how their systems operate. The need for accountability is particularly crucial with sensitive use technologies like facial recognition. Recently, there has been a growing demand for facial recognition technology, especially from law enforcement organizations who see the potential of the technology in uses like finding missing children. However, these technologies could potentially be used by a government to put their citizens\u2019 fundamental freedoms at risk by, for example, enabling continuous surveillance of specific individuals. Hence, data scientists and organizations need to be responsible for how their AI system impacts individuals or society.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Warnings of Mass Surveillance Through Facial Recognition </p> <p>Ultimately one of the biggest questions for our generation, as the first generation that is bringing AI to society, is how to ensure that computers will remain accountable to people and how to ensure that the people that design computers remain accountable to everyone else.</p>"},{"location":"1-Introduction/3-fairness/#impact-assessment","title":"Impact assessment","text":"<p>Before training a machine learning model, it is important to conduct an impact assessmet to understand the purpose of the AI system; what the intended use is; where it will be deployed; and who will be interacting with the system.  These are helpful for reviewer(s) or testers evaluating the system to know what factors to take into consideration when identifying potential risks and expected consequences.</p> <p>The following are areas of focus when conducting an impact assessment:</p> <ul> <li>Adverse impact on individuals.  Being aware of any restriction or requirements, unsupported use or any known limitations hindering the system's performance is vital to ensure that the system is not used in a way that could cause harm to individuals.</li> <li>Data requirements.  Gaining an understanding of how and where the system will use data enables reviewers to explore any data requirements you would need to be mindful of (e.g., GDPR or HIPPA data regulations).  In addition, examine whether the source or quantity of data is substantial for training.</li> <li>Summary of impact.  Gather a list of potential harms that could  arise from using the system.  Throughout the ML lifecycle, review if the issues identified are mitigated or addressed.</li> <li>Applicable goals for each of the six core principles.  Assess if the goals from each of the principles are met and if there are any gaps.</li> </ul>"},{"location":"1-Introduction/3-fairness/#debugging-with-responsible-ai","title":"Debugging with responsible AI","text":"<p>Similar to debugging a software application, debugging an AI system is a necessary process of identifying and resolving issues in the system.  There are many factors that would affect a model not performing as expected or responsibly.  Most traditional model performance metrics are quantitative aggregates of a model's performance, which are not sufficient to analyze how a model violates the responsible AI principles. Furthermore, a machine learning model is a black box that makes it difficult to understand what drives its outcome or provide explanation when it makes a mistake.  Later in this course, we will learn how to use the Responsible AI dashboard to help debug AI systems.  The dashboard provides a holistic tool for data scientists and AI developers to perform:</p> <ul> <li>Error analysis.  To identify the error distribution of the model that can affect the system's fairness or reliability.</li> <li>Model overview. To discover where there are disparities in the model's performance across data cohorts.</li> <li>Data analysis.  To understand the data distribution and identify any potential bias in the data that could lead to fairness, inclusiveness, and reliability issues.</li> <li>Model interpretability. To understand what affects or influences the model's predictions. This helps in explaining the model's behavior, which is important for transparency and accountability.</li> </ul>"},{"location":"1-Introduction/3-fairness/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>To prevent harms from being introduced in the first place, we should: </p> <ul> <li>have a diversity of backgrounds and perspectives among the people working on systems </li> <li>invest in datasets that reflect the diversity of our society </li> <li>develop better methods throughout the machine learning lifecycle for detecting and correcting responible AI when it occurs </li> </ul> <p>Think about real-life scenarios where a model's untrustworthiness is evident in model-building and usage. What else should we consider? </p>"},{"location":"1-Introduction/3-fairness/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"1-Introduction/3-fairness/#review-self-study","title":"Review &amp; Self Study","text":"<p>In this lesson, you have learned some basics of the concepts of fairness and unfairness in machine learning.  </p> <p>Watch this workshop to dive deeper into the topics: </p> <ul> <li>In pursuit of responsible AI: Bringing principles to practice by Besmira Nushi, Mehrnoosh Sameki and Amit Sharma</li> </ul> <p></p> <p>\ud83c\udfa5 Click the image above for a video: RAI Toolbox: An open-source framework for building responsible AI by Besmira Nushi, Mehrnoosh Sameki, and Amit Sharma</p> <p>Also, read: </p> <ul> <li> <p>Microsoft\u2019s RAI resource center: Responsible AI Resources \u2013 Microsoft AI </p> </li> <li> <p>Microsoft\u2019s FATE research group: FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research </p> </li> </ul> <p>RAI Toolbox: </p> <ul> <li>Responsible AI Toolbox GitHub repository</li> </ul> <p>Read about Azure Machine Learning's tools to ensure fairness:</p> <ul> <li>Azure Machine Learning </li> </ul>"},{"location":"1-Introduction/3-fairness/#assignment","title":"Assignment","text":"<p>Explore RAI Toolbox </p>"},{"location":"1-Introduction/3-fairness/assignment/","title":"Explore the Responsible AI Toolbox","text":""},{"location":"1-Introduction/3-fairness/assignment/#instructions","title":"Instructions","text":"<p>In this lesson you learned about the Responsible AI Toolbox, an \"open-source, community-driven project to help data scientists to analyze and improve AI systems.\" For this assignment, explore one of RAI Toolbox's notebooks and report your findings in a paper or presentation.</p>"},{"location":"1-Introduction/3-fairness/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A paper or powerpoint presentation is presented discussing Fairlearn's systems, the notebook that was run, and the conclusions drawn from running it A paper is presented without conclusions No paper is presented"},{"location":"1-Introduction/4-techniques-of-ML/","title":"Techniques of Machine Learning","text":"<p>The process of building, using, and maintaining machine learning models and the data they use is a very different process from many other development workflows. In this lesson, we will demystify the process, and outline the main techniques you need to know. You will:</p> <ul> <li>Understand the processes underpinning machine learning at a high level.</li> <li>Explore base concepts such as 'models', 'predictions', and 'training data'.</li> </ul>"},{"location":"1-Introduction/4-techniques-of-ML/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>\ud83c\udfa5 Click the image above for a short video working through this lesson.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#introduction","title":"Introduction","text":"<p>On a high level, the craft of creating machine learning (ML) processes is comprised of a number of steps:</p> <ol> <li>Decide on the question. Most ML processes start by asking a question that cannot be answered by a simple conditional program or rules-based engine. These questions often revolve around predictions based on a collection of data.</li> <li>Collect and prepare data. To be able to answer your question, you need data. The quality and, sometimes, quantity of your data will determine how well you can answer your initial question. Visualizing data is an important aspect of this phase. This phase also includes splitting the data into a training and testing group to build a model.</li> <li>Choose a training method. Depending on your question and the nature of your data, you need to choose how you want to train a model to best reflect your data and make accurate predictions against it. This is the part of your ML process that requires specific expertise and, often, a considerable amount of experimentation.</li> <li>Train the model. Using your training data, you'll use various algorithms to train a model to recognize patterns in the data. The model might leverage internal weights that can be adjusted to privilege certain parts of the data over others to build a better model.</li> <li>Evaluate the model. You use never before seen data (your testing data) from your collected set to see how the model is performing.</li> <li>Parameter tuning. Based on the performance of your model, you can redo the process using different parameters, or variables, that control the behavior of the algorithms used to train the model.</li> <li>Predict. Use new inputs to test the accuracy of your model.</li> </ol>"},{"location":"1-Introduction/4-techniques-of-ML/#what-question-to-ask","title":"What question to ask","text":"<p>Computers are particularly skilled at discovering hidden patterns in data. This utility is very helpful for researchers who have questions about a given domain that cannot be easily answered by creating a conditionally-based rules engine. Given an actuarial task, for example, a data scientist might be able to construct handcrafted rules around the mortality of smokers vs non-smokers.</p> <p>When many other variables are brought into the equation, however, a ML model might prove more efficient to predict future mortality rates based on past health history. A more cheerful example might be making weather predictions for the month of April in a given location based on data that includes latitude, longitude, climate change, proximity to the ocean, patterns of the jet stream, and more.</p> <p>\u2705 This slide deck on weather models offers a historical perspective for using ML in weather analysis.  </p>"},{"location":"1-Introduction/4-techniques-of-ML/#pre-building-tasks","title":"Pre-building tasks","text":"<p>Before starting to build your model, there are several tasks you need to complete. To test your question and form a hypothesis based on a model's predictions, you need to identify and configure several elements.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#data","title":"Data","text":"<p>To be able to answer your question with any kind of certainty, you need a good amount of data of the right type. There are two things you need to do at this point:</p> <ul> <li>Collect data. Keeping in mind the previous lesson on fairness in data analysis, collect your data with care. Be aware of the sources of this data, any inherent biases it might have, and document its origin.</li> <li>Prepare data. There are several steps in the data preparation process. You might need to collate data and normalize it if it comes from diverse sources. You can improve the data's quality and quantity through various methods such as converting strings to numbers (as we do in Clustering). You might also generate new data, based on the original (as we do in Classification). You can clean and edit the data (as we will prior to the Web App lesson). Finally, you might also need to randomize it and shuffle it, depending on your training techniques.</li> </ul> <p>\u2705 After collecting and processing your data, take a moment to see if its shape will allow you to address your intended question. It may be that the data will not perform well in your given task, as we discover in our Clustering lessons!</p>"},{"location":"1-Introduction/4-techniques-of-ML/#features-and-target","title":"Features and Target","text":"<p>A feature is a measurable property of your data. In many datasets it is expressed as a column heading like 'date' 'size' or 'color'. Your feature variable, usually represented as <code>X</code> in code, represent the input variable which will be used to train model.</p> <p>A target is a thing you are trying to predict. Target usually represented as <code>y</code> in code, represents the answer to the question you are trying to ask of your data: in December, what color pumpkins will be cheapest? in San Francisco, what neighborhoods will have the best real estate price? Sometimes target is also referred as label attribute.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#selecting-your-feature-variable","title":"Selecting your feature variable","text":"<p>\ud83c\udf93 Feature Selection and Feature Extraction How do you know which variable to choose when building a model? You'll probably go through a process of feature selection or feature extraction to choose the right variables for the most performant model. They're not the same thing, however: \"Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features.\" (source)</p>"},{"location":"1-Introduction/4-techniques-of-ML/#visualize-your-data","title":"Visualize your data","text":"<p>An important aspect of the data scientist's toolkit is the power to visualize data using several excellent libraries such as Seaborn or MatPlotLib. Representing your data visually might allow you to uncover hidden correlations that you can leverage. Your visualizations might also help you to uncover bias or unbalanced data (as we discover in Classification).</p>"},{"location":"1-Introduction/4-techniques-of-ML/#split-your-dataset","title":"Split your dataset","text":"<p>Prior to training, you need to split your dataset into two or more parts of unequal size that still represent the data well.</p> <ul> <li>Training. This part of the dataset is fit to your model to train it. This set constitutes the majority of the original dataset.</li> <li>Testing. A test dataset is an independent group of data, often gathered from the original data, that you use to confirm the performance of the built model.</li> <li>Validating. A validation set is a smaller independent group of examples that you use to tune the model's hyperparameters, or architecture, to improve the model. Depending on your data's size and the question you are asking, you might not need to build this third set (as we note in Time Series Forecasting).</li> </ul>"},{"location":"1-Introduction/4-techniques-of-ML/#building-a-model","title":"Building a model","text":"<p>Using your training data, your goal is to build a model, or a statistical representation of your data, using various algorithms to train it. Training a model exposes it to data and allows it to make assumptions about perceived patterns it discovers, validates, and accepts or rejects.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#decide-on-a-training-method","title":"Decide on a training method","text":"<p>Depending on your question and the nature of your data, you will choose a method to train it. Stepping through Scikit-learn's documentation - which we use in this course - you can explore many ways to train a model. Depending on your experience, you might have to try several different methods to build the best model. You are likely to go through a process whereby data scientists evaluate the performance of a model by feeding it unseen data, checking for accuracy, bias, and other quality-degrading issues, and selecting the most appropriate training method for the task at hand.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#train-a-model","title":"Train a model","text":"<p>Armed with your training data, you are ready to 'fit' it to create a model. You will notice that in many ML libraries you will find the code 'model.fit' - it is at this time that you send in your feature variable as an array of values (usually 'X') and a target variable (usually 'y').</p>"},{"location":"1-Introduction/4-techniques-of-ML/#evaluate-the-model","title":"Evaluate the model","text":"<p>Once the training process is complete (it can take many iterations, or 'epochs', to train a large model), you will be able to evaluate the model's quality by using test data to gauge its performance. This data is a subset of the original data that the model has not previously analyzed. You can print out a table of metrics about your model's quality.</p> <p>\ud83c\udf93 Model fitting</p> <p>In the context of machine learning, model fitting refers to the accuracy of the model's underlying function as it attempts to analyze data with which it is not familiar.</p> <p>\ud83c\udf93 Underfitting and overfitting are common problems that degrade the quality of the model, as the model fits either not well enough or too well. This causes the model to make predictions either too closely aligned or too loosely aligned with its training data. An overfit model predicts training data too well because it has learned the data's details and noise too well. An underfit model is not accurate as it can neither accurately analyze its training data nor data it has not yet 'seen'.</p> <p></p> <p>Infographic by Jen Looper</p>"},{"location":"1-Introduction/4-techniques-of-ML/#parameter-tuning","title":"Parameter tuning","text":"<p>Once your initial training is complete, observe the quality of the model and consider improving it by tweaking its 'hyperparameters'. Read more about the process in the documentation.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#prediction","title":"Prediction","text":"<p>This is the moment where you can use completely new data to test your model's accuracy. In an 'applied' ML setting, where you are building web assets to use the model in production, this process might involve gathering user input (a button press, for example) to set a variable and send it to the model for inference, or evaluation.</p> <p>In these lessons, you will discover how to use these steps to prepare, build, test, evaluate, and predict - all the gestures of a data scientist and more, as you progress in your journey to become a 'full stack' ML engineer.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Draw a flow chart reflecting the steps of a ML practitioner. Where do you see yourself right now in the process? Where do you predict you will find difficulty? What seems easy to you?</p>"},{"location":"1-Introduction/4-techniques-of-ML/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"1-Introduction/4-techniques-of-ML/#review-self-study","title":"Review &amp; Self Study","text":"<p>Search online for interviews with data scientists who discuss their daily work. Here is one.</p>"},{"location":"1-Introduction/4-techniques-of-ML/#assignment","title":"Assignment","text":"<p>Interview a data scientist</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u6280\u672f","text":"<p>\u6784\u5efa\u3001\u4f7f\u7528\u548c\u7ef4\u62a4\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ca\u5176\u4f7f\u7528\u7684\u6570\u636e\u7684\u8fc7\u7a0b\u4e0e\u8bb8\u591a\u5176\u4ed6\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u622a\u7136\u4e0d\u540c\u3002 \u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u63ed\u5f00\u8be5\u8fc7\u7a0b\u7684\u795e\u79d8\u9762\u7eb1\uff0c\u5e76\u6982\u8ff0\u4f60\u9700\u8981\u4e86\u89e3\u7684\u4e3b\u8981\u6280\u672f\u3002 \u4f60\u4f1a\uff1a </p> <ul> <li>\u5728\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3\u652f\u6301\u673a\u5668\u5b66\u4e60\u7684\u8fc7\u7a0b\u3002 </li> <li>\u63a2\u7d22\u57fa\u672c\u6982\u5ff5\uff0c\u4f8b\u5982\u201c\u6a21\u578b\u201d\u3001\u201c\u9884\u6d4b\u201d\u548c\u201c\u8bad\u7ec3\u6570\u636e\u201d\u3002 </li> </ul>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u5728\u8f83\u9ad8\u7684\u5c42\u6b21\u4e0a\uff0c\u521b\u5efa\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u8fc7\u7a0b\u7684\u5de5\u827a\u5305\u62ec\u8bb8\u591a\u6b65\u9aa4\uff1a</p> <ol> <li>\u51b3\u5b9a\u95ee\u9898\u3002 \u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u90fd\u662f\u4ece\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u7684\u6761\u4ef6\u7a0b\u5e8f\u6216\u57fa\u4e8e\u89c4\u5219\u7684\u5f15\u64ce\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u5f00\u59cb\u7684\u3002 \u8fd9\u4e9b\u95ee\u9898\u901a\u5e38\u56f4\u7ed5\u57fa\u4e8e\u6570\u636e\u96c6\u5408\u7684\u9884\u6d4b\u5c55\u5f00\u3002 </li> <li>\u6536\u96c6\u548c\u51c6\u5907\u6570\u636e\u3002\u4e3a\u4e86\u80fd\u591f\u56de\u7b54\u4f60\u7684\u95ee\u9898\uff0c\u4f60\u9700\u8981\u6570\u636e\u3002\u6570\u636e\u7684\u8d28\u91cf\uff08\u6709\u65f6\u662f\u6570\u91cf\uff09\u5c06\u51b3\u5b9a\u4f60\u56de\u7b54\u6700\u521d\u95ee\u9898\u7684\u80fd\u529b\u3002\u53ef\u89c6\u5316\u6570\u636e\u662f\u8fd9\u4e2a\u9636\u6bb5\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u3002\u6b64\u9636\u6bb5\u8fd8\u5305\u62ec\u5c06\u6570\u636e\u62c6\u5206\u4e3a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7ec4\u4ee5\u6784\u5efa\u6a21\u578b\u3002 </li> <li>\u9009\u62e9\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5\u3002\u6839\u636e\u4f60\u7684\u95ee\u9898\u548c\u6570\u636e\u7684\u6027\u8d28\uff0c\u4f60\u9700\u8981\u9009\u62e9\u5982\u4f55\u8bad\u7ec3\u6a21\u578b\u4ee5\u6700\u597d\u5730\u53cd\u6620\u4f60\u7684\u6570\u636e\u5e76\u5bf9\u5176\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u3002\u8fd9\u662f\u4f60\u7684ML\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u9700\u8981\u7279\u5b9a\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u5b9e\u9a8c\u3002 </li> <li>\u8bad\u7ec3\u6a21\u578b\u3002\u4f7f\u7528\u4f60\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f60\u5c06\u4f7f\u7528\u5404\u79cd\u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b\u4ee5\u8bc6\u522b\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u3002\u8be5\u6a21\u578b\u53ef\u80fd\u4f1a\u5229\u7528\u53ef\u4ee5\u8c03\u6574\u7684\u5185\u90e8\u6743\u91cd\u6765\u4f7f\u6570\u636e\u7684\u67d0\u4e9b\u90e8\u5206\u4f18\u4e8e\u5176\u4ed6\u90e8\u5206\uff0c\u4ece\u800c\u6784\u5efa\u66f4\u597d\u7684\u6a21\u578b\u3002 </li> <li>\u8bc4\u4f30\u6a21\u578b\u3002\u4f60\u4f7f\u7528\u6536\u96c6\u5230\u7684\u96c6\u5408\u4e2d\u4ece\u672a\u89c1\u8fc7\u7684\u6570\u636e\uff08\u4f60\u7684\u6d4b\u8bd5\u6570\u636e\uff09\u6765\u67e5\u770b\u6a21\u578b\u7684\u6027\u80fd\u3002 </li> <li>\u53c2\u6570\u8c03\u6574\u3002\u6839\u636e\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u53c2\u6570\u6216\u53d8\u91cf\u91cd\u505a\u8be5\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u53c2\u6570\u6216\u53d8\u91cf\u63a7\u5236\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u7b97\u6cd5\u7684\u884c\u4e3a\u3002 </li> <li>\u9884\u6d4b\u3002\u4f7f\u7528\u65b0\u8f93\u5165\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002 </li> </ol>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_4","title":"\u8981\u95ee\u4ec0\u4e48\u95ee\u9898","text":"<p>\u8ba1\u7b97\u673a\u7279\u522b\u64c5\u957f\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u9690\u85cf\u6a21\u5f0f\u3002\u6b64\u5b9e\u7528\u7a0b\u5e8f\u5bf9\u4e8e\u5bf9\u7ed9\u5b9a\u9886\u57df\u6709\u7591\u95ee\u7684\u7814\u7a76\u4eba\u5458\u975e\u5e38\u6709\u5e2e\u52a9\uff0c\u8fd9\u4e9b\u95ee\u9898\u65e0\u6cd5\u901a\u8fc7\u521b\u5efa\u57fa\u4e8e\u6761\u4ef6\u7684\u89c4\u5219\u5f15\u64ce\u6765\u8f7b\u677e\u56de\u7b54\u3002\u4f8b\u5982\uff0c\u7ed9\u5b9a\u4e00\u9879\u7cbe\u7b97\u4efb\u52a1\uff0c\u6570\u636e\u79d1\u5b66\u5bb6\u53ef\u80fd\u80fd\u591f\u56f4\u7ed5\u5438\u70df\u8005\u4e0e\u975e\u5438\u70df\u8005\u7684\u6b7b\u4ea1\u7387\u6784\u5efa\u624b\u5de5\u89c4\u5219\u3002 </p> <p>\u7136\u800c\uff0c\u5f53\u5c06\u8bb8\u591a\u5176\u4ed6\u53d8\u91cf\u7eb3\u5165\u7b49\u5f0f\u65f6\uff0cML\u6a21\u578b\u53ef\u80fd\u4f1a\u66f4\u6709\u6548\u5730\u6839\u636e\u8fc7\u53bb\u7684\u5065\u5eb7\u53f2\u9884\u6d4b\u672a\u6765\u7684\u6b7b\u4ea1\u7387\u3002\u4e00\u4e2a\u66f4\u4ee4\u4eba\u6109\u5feb\u7684\u4f8b\u5b50\u53ef\u80fd\u662f\u6839\u636e\u5305\u62ec\u7eac\u5ea6\u3001\u7ecf\u5ea6\u3001\u6c14\u5019\u53d8\u5316\u3001\u4e0e\u6d77\u6d0b\u7684\u63a5\u8fd1\u7a0b\u5ea6\u3001\u6025\u6d41\u6a21\u5f0f\u7b49\u5728\u5185\u7684\u6570\u636e\u5bf9\u7ed9\u5b9a\u4f4d\u7f6e\u76844\u6708\u4efd\u8fdb\u884c\u5929\u6c14\u9884\u62a5\u3002 </p> <p>\u2705 \u8fd9\u4e2a\u5173\u4e8e\u5929\u6c14\u6a21\u578b\u7684\u5e7b\u706f\u7247\u4e3a\u5728\u5929\u6c14\u5206\u6790\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5386\u53f2\u89c6\u89d2\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_5","title":"\u9884\u6784\u5efa\u4efb\u52a1","text":"<p>\u5728\u5f00\u59cb\u6784\u5efa\u6a21\u578b\u4e4b\u524d\uff0c\u4f60\u9700\u8981\u5b8c\u6210\u591a\u9879\u4efb\u52a1\u3002\u8981\u6d4b\u8bd5\u4f60\u7684\u95ee\u9898\u5e76\u6839\u636e\u6a21\u578b\u7684\u9884\u6d4b\u5f62\u6210\u5047\u8bbe\uff0c\u4f60\u9700\u8981\u8bc6\u522b\u548c\u914d\u7f6e\u591a\u4e2a\u5143\u7d20\u3002 </p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#data","title":"Data","text":"<p>\u4e3a\u4e86\u80fd\u591f\u786e\u5b9a\u5730\u56de\u7b54\u4f60\u7684\u95ee\u9898\uff0c\u4f60\u9700\u8981\u5927\u91cf\u6b63\u786e\u7c7b\u578b\u7684\u6570\u636e\u3002 \u6b64\u65f6\u4f60\u9700\u8981\u505a\u4e24\u4ef6\u4e8b\uff1a </p> <ul> <li>\u6536\u96c6\u6570\u636e\u3002\u8bb0\u4f4f\u4e4b\u524d\u5173\u4e8e\u6570\u636e\u5206\u6790\u516c\u5e73\u6027\u7684\u8bfe\u7a0b\uff0c\u5c0f\u5fc3\u6536\u96c6\u6570\u636e\u3002\u8bf7\u6ce8\u610f\u6b64\u6570\u636e\u7684\u6765\u6e90\u3001\u5b83\u53ef\u80fd\u5177\u6709\u7684\u4efb\u4f55\u56fa\u6709\u504f\u89c1\uff0c\u5e76\u8bb0\u5f55\u5176\u6765\u6e90\u3002 </li> <li>\u51c6\u5907\u6570\u636e\u3002\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\u6709\u51e0\u4e2a\u6b65\u9aa4\u3002\u5982\u679c\u6570\u636e\u6765\u81ea\u4e0d\u540c\u7684\u6765\u6e90\uff0c\u4f60\u53ef\u80fd\u9700\u8981\u6574\u7406\u6570\u636e\u5e76\u5bf9\u5176\u8fdb\u884c\u6807\u51c6\u5316\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u65b9\u6cd5\u63d0\u9ad8\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\uff0c\u4f8b\u5982\u5c06\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u6570\u5b57\uff08\u5c31\u50cf\u6211\u4eec\u5728\u805a\u7c7b\u4e2d\u6240\u505a\u7684\u90a3\u6837\uff09\u3002\u4f60\u8fd8\u53ef\u4ee5\u6839\u636e\u539f\u59cb\u6570\u636e\u751f\u6210\u65b0\u6570\u636e\uff08\u6b63\u5982\u6211\u4eec\u5728\u5206\u7c7b\u4e2d\u6240\u505a\u7684\u90a3\u6837\uff09\u3002\u4f60\u53ef\u4ee5\u6e05\u7406\u548c\u7f16\u8f91\u6570\u636e\uff08\u5c31\u50cf\u6211\u4eec\u5728 Web App\u8bfe\u7a0b\u4e4b\u524d\u6240\u505a\u7684\u90a3\u6837\uff09\u3002\u6700\u540e\uff0c\u4f60\u53ef\u80fd\u8fd8\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u968f\u673a\u5316\u548c\u6253\u4e71\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u4f60\u7684\u8bad\u7ec3\u6280\u672f\u3002</li> </ul> <p>\u2705 \u5728\u6536\u96c6\u548c\u5904\u7406\u4f60\u7684\u6570\u636e\u540e\uff0c\u82b1\u70b9\u65f6\u95f4\u770b\u770b\u5b83\u7684\u5f62\u72b6\u662f\u5426\u80fd\u8ba9\u4f60\u89e3\u51b3\u4f60\u7684\u9884\u671f\u95ee\u9898\u3002\u6b63\u5982\u6211\u4eec\u5728\u805a\u7c7b\u8bfe\u7a0b\u4e2d\u53d1\u73b0\u7684\u90a3\u6837\uff0c\u6570\u636e\u53ef\u80fd\u5728\u4f60\u7684\u7ed9\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff01</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_6","title":"\u529f\u80fd\u548c\u76ee\u6807","text":"<p>\u529f\u80fd\u662f\u6570\u636e\u7684\u53ef\u6d4b\u91cf\u5c5e\u6027\u3002\u5728\u8bb8\u591a\u6570\u636e\u96c6\u4e2d\uff0c\u5b83\u8868\u793a\u4e3a\u6807\u9898\u4e3a\"\u65e5\u671f\"\"\u5927\u5c0f\"\u6216\"\u989c\u8272\"\u7684\u5217\u3002\u60a8\u7684\u529f\u80fd\u53d8\u91cf\uff08\u901a\u5e38\u5728\u4ee3\u7801\u4e2d\u8868\u793a\u4e3a <code>X</code>\uff09\u8868\u793a\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u5165\u53d8\u91cf\u3002</p> <p>\u76ee\u6807\u5c31\u662f\u4f60\u8bd5\u56fe\u9884\u6d4b\u7684\u4e8b\u60c5\u3002\u76ee\u6807\u901a\u5e38\u8868\u793a\u4e3a\u4ee3\u7801\u4e2d\u7684 <code>y</code>\uff0c\u4ee3\u8868\u60a8\u8bd5\u56fe\u8be2\u95ee\u6570\u636e\u7684\u95ee\u9898\u7684\u7b54\u6848\uff1a\u5728 12 \u6708\uff0c\u4ec0\u4e48\u989c\u8272\u7684\u5357\u74dc\u6700\u4fbf\u5b9c\uff1f\u5728\u65e7\u91d1\u5c71\uff0c\u54ea\u4e9b\u8857\u533a\u7684\u623f\u5730\u4ea7\u4ef7\u683c\u6700\u597d\uff1f\u6709\u65f6\u76ee\u6807\u4e5f\u79f0\u4e3a\u6807\u7b7e\u5c5e\u6027\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_7","title":"\u9009\u62e9\u7279\u5f81\u53d8\u91cf","text":"<p>\ud83c\udf93 \u7279\u5f81\u9009\u62e9\u548c\u7279\u5f81\u63d0\u53d6 \u6784\u5efa\u6a21\u578b\u65f6\u5982\u4f55\u77e5\u9053\u9009\u62e9\u54ea\u4e2a\u53d8\u91cf\uff1f\u4f60\u53ef\u80fd\u4f1a\u7ecf\u5386\u4e00\u4e2a\u7279\u5f81\u9009\u62e9\u6216\u7279\u5f81\u63d0\u53d6\u7684\u8fc7\u7a0b\uff0c\u4ee5\u4fbf\u4e3a\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\u9009\u62e9\u6b63\u786e\u7684\u53d8\u91cf\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u662f\u4e00\u56de\u4e8b\uff1a\u201c\u7279\u5f81\u63d0\u53d6\u662f\u4ece\u57fa\u4e8e\u539f\u59cb\u7279\u5f81\u7684\u51fd\u6570\u4e2d\u521b\u5efa\u65b0\u7279\u5f81\uff0c\u800c\u7279\u5f81\u9009\u62e9\u8fd4\u56de\u7279\u5f81\u7684\u4e00\u4e2a\u5b50\u96c6\u3002\u201d\uff08\u6765\u6e90\uff09</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_8","title":"\u53ef\u89c6\u5316\u6570\u636e","text":"<p>\u6570\u636e\u79d1\u5b66\u5bb6\u5de5\u5177\u5305\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u662f\u80fd\u591f\u4f7f\u7528\u591a\u4e2a\u4f18\u79c0\u7684\u5e93\uff08\u4f8b\u5982 Seaborn \u6216 MatPlotLib\uff09\u5c06\u6570\u636e\u53ef\u89c6\u5316\u3002\u76f4\u89c2\u5730\u8868\u793a\u4f60\u7684\u6570\u636e\u53ef\u80fd\u4f1a\u8ba9\u4f60\u53d1\u73b0\u53ef\u4ee5\u5229\u7528\u7684\u9690\u85cf\u5173\u8054\u3002 \u4f60\u7684\u53ef\u89c6\u5316\u8fd8\u53ef\u4ee5\u5e2e\u52a9\u4f60\u53d1\u73b0\u504f\u89c1\u6216\u4e0d\u5e73\u8861\u7684\u6570\u636e\uff08\u6b63\u5982\u6211\u4eec\u5728 \u5206\u7c7b\u4e2d\u53d1\u73b0\u7684\u90a3\u6837\uff09\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_9","title":"\u62c6\u5206\u6570\u636e\u96c6","text":"<p>\u5728\u8bad\u7ec3\u4e4b\u524d\uff0c\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u62c6\u5206\u4e3a\u4e24\u4e2a\u6216\u591a\u4e2a\u5927\u5c0f\u4e0d\u7b49\u4f46\u4ecd\u80fd\u5f88\u597d\u5730\u4ee3\u8868\u6570\u636e\u7684\u90e8\u5206\u3002</p> <ul> <li>\u8bad\u7ec3\u3002\u8fd9\u90e8\u5206\u6570\u636e\u96c6\u9002\u5408\u4f60\u7684\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u4e2a\u96c6\u5408\u6784\u6210\u4e86\u539f\u59cb\u6570\u636e\u96c6\u7684\u5927\u90e8\u5206\u3002</li> <li>\u6d4b\u8bd5\u3002\u6d4b\u8bd5\u6570\u636e\u96c6\u662f\u4e00\u7ec4\u72ec\u7acb\u7684\u6570\u636e\uff0c\u901a\u5e38\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u6536\u96c6\uff0c\u7528\u4e8e\u786e\u8ba4\u6784\u5efa\u6a21\u578b\u7684\u6027\u80fd\u3002</li> <li>\u9a8c\u8bc1\u3002\u9a8c\u8bc1\u96c6\u662f\u4e00\u4e2a\u8f83\u5c0f\u7684\u72ec\u7acb\u793a\u4f8b\u7ec4\uff0c\u7528\u4e8e\u8c03\u6574\u6a21\u578b\u7684\u8d85\u53c2\u6570\u6216\u67b6\u6784\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u3002\u6839\u636e\u4f60\u7684\u6570\u636e\u5927\u5c0f\u548c\u4f60\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u4f60\u53ef\u80fd\u4e0d\u9700\u8981\u6784\u5efa\u7b2c\u4e09\u7ec4\uff08\u6b63\u5982\u6211\u4eec\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6240\u8ff0\uff09\u3002 </li> </ul>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_10","title":"\u5efa\u7acb\u6a21\u578b","text":"<p>\u4f7f\u7528\u4f60\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f60\u7684\u76ee\u6807\u662f\u6784\u5efa\u6a21\u578b\u6216\u6570\u636e\u7684\u7edf\u8ba1\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u5404\u79cd\u7b97\u6cd5\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3\u3002\u8bad\u7ec3\u6a21\u578b\u5c06\u5176\u66b4\u9732\u7ed9\u6570\u636e\uff0c\u5e76\u5141\u8bb8\u5b83\u5bf9\u5176\u53d1\u73b0\u3001\u9a8c\u8bc1\u548c\u63a5\u53d7\u6216\u62d2\u7edd\u7684\u611f\u77e5\u6a21\u5f0f\u505a\u51fa\u5047\u8bbe\u3002 </p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_11","title":"\u51b3\u5b9a\u4e00\u79cd\u8bad\u7ec3\u65b9\u6cd5","text":"<p>\u6839\u636e\u4f60\u7684\u95ee\u9898\u548c\u6570\u636e\u7684\u6027\u8d28\uff0c\u4f60\u5c06\u9009\u62e9\u4e00\u79cd\u65b9\u6cd5\u6765\u8bad\u7ec3\u5b83\u3002\u9010\u6b65\u5b8c\u6210 Scikit-learn\u7684\u6587\u6863 - \u6211\u4eec\u5728\u672c\u8bfe\u7a0b\u4e2d\u4f7f\u7528 - \u4f60\u53ef\u4ee5\u63a2\u7d22\u591a\u79cd\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\u3002 \u6839\u636e\u4f60\u7684\u7ecf\u9a8c\uff0c\u4f60\u53ef\u80fd\u9700\u8981\u5c1d\u8bd5\u591a\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u6700\u4f73\u6a21\u578b\u3002\u4f60\u53ef\u80fd\u4f1a\u7ecf\u5386\u4e00\u4e2a\u8fc7\u7a0b\uff0c\u5728\u8be5\u8fc7\u7a0b\u4e2d\uff0c\u6570\u636e\u79d1\u5b66\u5bb6\u901a\u8fc7\u63d0\u4f9b\u672a\u89c1\u8fc7\u7684\u6570\u636e\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u68c0\u67e5\u51c6\u786e\u6027\u3001\u504f\u5dee\u548c\u5176\u4ed6\u964d\u4f4e\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u624b\u5934\u7684\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_12","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u6709\u4e86\u60a8\u7684\u57f9\u8bad\u6570\u636e\uff0c\u60a8\u5c31\u53ef\u4ee5\"\u9002\u5e94\"\u5b83\u6765\u521b\u5efa\u6a21\u578b\u3002\u60a8\u4f1a\u6ce8\u610f\u5230\uff0c\u5728\u8bb8\u591a ML \u5e93\u4e2d\uff0c\u60a8\u4f1a\u53d1\u73b0\u4ee3\u7801\"model.fit\"-\u6b64\u65f6\uff0c\u60a8\u5c06\u529f\u80fd\u53d8\u91cf\u4f5c\u4e3a\u4e00\u7cfb\u5217\u503c\uff08\u901a\u5e38\u662f<code>X</code>\uff09\u548c\u76ee\u6807\u53d8\u91cf\uff08\u901a\u5e38\u662f<code>y</code>\uff09\u53d1\u9001\u3002 </p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_13","title":"\u8bc4\u4f30\u6a21\u578b","text":"<p>\u8bad\u7ec3\u8fc7\u7a0b\u5b8c\u6210\u540e\uff08\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u53ef\u80fd\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u6216\u201c\u65f6\u671f\u201d\uff09\uff0c\u4f60\u5c06\u80fd\u591f\u901a\u8fc7\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6765\u8861\u91cf\u6a21\u578b\u7684\u6027\u80fd\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u8d28\u91cf\u3002\u6b64\u6570\u636e\u662f\u6a21\u578b\u5148\u524d\u672a\u5206\u6790\u7684\u539f\u59cb\u6570\u636e\u7684\u5b50\u96c6\u3002 \u4f60\u53ef\u4ee5\u6253\u5370\u51fa\u6709\u5173\u6a21\u578b\u8d28\u91cf\u7684\u6307\u6807\u8868\u3002 </p> <p>\ud83c\udf93 \u6a21\u578b\u62df\u5408</p> <p>\u5728\u673a\u5668\u5b66\u4e60\u7684\u80cc\u666f\u4e0b\uff0c\u6a21\u578b\u62df\u5408\u662f\u6307\u6a21\u578b\u5728\u5c1d\u8bd5\u5206\u6790\u4e0d\u719f\u6089\u7684\u6570\u636e\u65f6\u5176\u5e95\u5c42\u529f\u80fd\u7684\u51c6\u786e\u6027\u3002 </p> <p>\ud83c\udf93 \u6b20\u62df\u5408\u548c\u8fc7\u62df\u5408\u662f\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u56e0\u4e3a\u6a21\u578b\u62df\u5408\u5f97\u4e0d\u591f\u597d\u6216\u592a\u597d\u3002\u8fd9\u4f1a\u5bfc\u81f4\u6a21\u578b\u505a\u51fa\u4e0e\u5176\u8bad\u7ec3\u6570\u636e\u8fc7\u4e8e\u7d27\u5bc6\u5bf9\u9f50\u6216\u8fc7\u4e8e\u677e\u6563\u5bf9\u9f50\u7684\u9884\u6d4b\u3002 \u8fc7\u62df\u5408\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u9884\u6d4b\u592a\u597d\uff0c\u56e0\u4e3a\u5b83\u5df2\u7ecf\u5f88\u597d\u5730\u4e86\u89e3\u4e86\u6570\u636e\u7684\u7ec6\u8282\u548c\u566a\u58f0\u3002\u6b20\u62df\u5408\u6a21\u578b\u5e76\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u5b83\u65e2\u4e0d\u80fd\u51c6\u786e\u5206\u6790\u5176\u8bad\u7ec3\u6570\u636e\uff0c\u4e5f\u4e0d\u80fd\u51c6\u786e\u5206\u6790\u5c1a\u672a\u201c\u770b\u5230\u201d\u7684\u6570\u636e\u3002</p> <p></p> <p>\u4f5c\u8005 Jen Looper</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_14","title":"\u53c2\u6570\u8c03\u4f18","text":"<p>\u521d\u59cb\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u89c2\u5bdf\u6a21\u578b\u7684\u8d28\u91cf\u5e76\u8003\u8651\u901a\u8fc7\u8c03\u6574\u5176\u201c\u8d85\u53c2\u6570\u201d\u6765\u6539\u8fdb\u5b83\u3002\u5728\u6b64\u6587\u6863\u4e2d\u9605\u8bfb\u6709\u5173\u8be5\u8fc7\u7a0b\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_15","title":"\u9884\u6d4b","text":"<p>\u8fd9\u662f\u4f60\u53ef\u4ee5\u4f7f\u7528\u5168\u65b0\u6570\u636e\u6765\u6d4b\u8bd5\u6a21\u578b\u51c6\u786e\u6027\u7684\u65f6\u523b\u3002\u5728\u201c\u5e94\u7528\u201dML\u8bbe\u7f6e\u4e2d\uff0c\u4f60\u6b63\u5728\u6784\u5efaWeb\u8d44\u6e90\u4ee5\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u6a21\u578b\uff0c\u6b64\u8fc7\u7a0b\u53ef\u80fd\u6d89\u53ca\u6536\u96c6\u7528\u6237\u8f93\u5165\uff08\u4f8b\u5982\u6309\u4e0b\u6309\u94ae\uff09\u4ee5\u8bbe\u7f6e\u53d8\u91cf\u5e76\u5c06\u5176\u53d1\u9001\u5230\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u6216\u8005\u8bc4\u4f30\u3002</p> <p>\u5728\u8fd9\u4e9b\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u4e86\u89e3\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u6b65\u9aa4\u6765\u51c6\u5907\u3001\u6784\u5efa\u3001\u6d4b\u8bd5\u3001\u8bc4\u4f30\u548c\u9884\u6d4b\u2014\u6240\u6709\u8fd9\u4e9b\u90fd\u662f\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u59ff\u6001\uff0c\u800c\u4e14\u968f\u7740\u4f60\u5728\u6210\u4e3a\u4e00\u540d\u201c\u5168\u6808\u201dML\u5de5\u7a0b\u5e08\u7684\u65c5\u7a0b\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f60\u5c06\u4e86\u89e3\u66f4\u591a\u3002 </p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_16","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u753b\u4e00\u4e2a\u6d41\u7a0b\u56fe\uff0c\u53cd\u6620ML\u7684\u6b65\u9aa4\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f60\u8ba4\u4e3a\u81ea\u5df1\u73b0\u5728\u5728\u54ea\u91cc\uff1f\u4f60\u9884\u6d4b\u4f60\u5728\u54ea\u91cc\u4f1a\u9047\u5230\u56f0\u96be\uff1f\u4ec0\u4e48\u5bf9\u4f60\u6765\u8bf4\u5f88\u5bb9\u6613\uff1f </p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_17","title":"\u9605\u8bfb\u540e\u6d4b\u9a8c","text":""},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_18","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u5728\u7ebf\u641c\u7d22\u5bf9\u8ba8\u8bba\u65e5\u5e38\u5de5\u4f5c\u7684\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u91c7\u8bbf\u3002 \u8fd9\u662f\u5176\u4e2d\u4e4b\u4e00\u3002</p>"},{"location":"1-Introduction/4-techniques-of-ML/README.zh-cn/#_19","title":"\u4efb\u52a1","text":"<p>\u91c7\u8bbf\u4e00\u540d\u6570\u636e\u79d1\u5b66\u5bb6</p>"},{"location":"1-Introduction/4-techniques-of-ML/assignment/","title":"Interview a data scientist","text":""},{"location":"1-Introduction/4-techniques-of-ML/assignment/#instructions","title":"Instructions","text":"<p>In your company, in a user group, or among your friends or fellow students, talk to someone who works professionally as a data scientist. Write a short paper (500 words) about their daily occupations. Are they specialists, or do they work 'full stack'?</p>"},{"location":"1-Introduction/4-techniques-of-ML/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement An essay of the correct length, with attributed sources, is presented as a .doc file The essay is poorly attributed or shorter than the required length No essay is presented"},{"location":"1-Introduction/4-techniques-of-ML/assignment.zh-cn/","title":"\u91c7\u8bbf\u4e00\u4f4d\u6570\u636e\u79d1\u5b66\u5bb6","text":""},{"location":"1-Introduction/4-techniques-of-ML/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u4f60\u7684\u516c\u53f8\u3001\u4f60\u6240\u5728\u7684\u793e\u7fa4\u3001\u6216\u8005\u5728\u4f60\u7684\u670b\u53cb\u548c\u540c\u5b66\u4e2d\uff0c\u627e\u5230\u4e00\u4f4d\u4ece\u4e8b\u6570\u636e\u79d1\u5b66\u4e13\u4e1a\u5de5\u4f5c\u7684\u4eba\uff0c\u4e0e\u4ed6\u6216\u5979\u4ea4\u6d41\u4e00\u4e0b\u3002\u5199\u4e00\u7bc7\u5173\u4e8e\u4ed6\u4eec\u5de5\u4f5c\u65e5\u5e38\u7684\u5c0f\u77ed\u6587\uff08500\u5b57\u5de6\u53f3\uff09\u3002\u4ed6\u4eec\u662f\u4e13\u5bb6\uff0c\u8fd8\u662f\u8bf4\u4ed6\u4eec\u662f\u201c\u5168\u6808\u201d\u5f00\u53d1\u8005\uff1f</p>"},{"location":"1-Introduction/4-techniques-of-ML/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e00\u7bc7\u6e05\u6670\u63cf\u8ff0\u4e86\u804c\u4e1a\u5c5e\u6027\u4e14\u5b57\u6570\u7b26\u5408\u89c4\u8303\u7684word\u6587\u6863 \u63d0\u4ea4\u7684\u6587\u6863\u804c\u4e1a\u5c5e\u6027\u63cf\u8ff0\u5f97\u4e0d\u6e05\u6670\u6216\u8005\u5b57\u6570\u4e0d\u5408\u89c4\u8303 \u5565\u90fd\u6ca1\u6709\u4ea4"},{"location":"1-Introduction/translations/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u5165\u95e8","text":"<p>\u8bfe\u7a0b\u7684\u672c\u7ae0\u8282\u5c06\u4e3a\u60a8\u4ecb\u7ecd\u673a\u5668\u5b66\u4e60\u9886\u57df\u80cc\u540e\u7684\u57fa\u672c\u6982\u5ff5\u3001\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff0c\u5e76\u5b66\u4e60\u5b83\u7684\u5386\u53f2\u4ee5\u53ca\u66fe\u4e3a\u6b64\u505a\u51fa\u8d21\u732e\u7684\u6280\u672f\u7814\u7a76\u8005\u4eec\u3002\u8ba9\u6211\u4eec\u4e00\u8d77\u5f00\u59cb\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u7684\u5168\u65b0\u4e16\u754c\u5427\uff01</p> <p></p> <p>\u56fe\u7247\u7531 Bill Oxford \u63d0\u4f9b\uff0c\u6765\u81ea Unsplash</p>"},{"location":"1-Introduction/translations/README.zh-cn/#_2","title":"\u8bfe\u7a0b\u5b89\u6392","text":"<ol> <li>\u673a\u5668\u5b66\u4e60\u7b80\u4ecb</li> <li>\u673a\u5668\u5b66\u4e60\u7684\u5386\u53f2</li> <li>\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027</li> <li>\u673a\u5668\u5b66\u4e60\u6280\u672f</li> </ol>"},{"location":"1-Introduction/translations/README.zh-cn/#_3","title":"\u81f4\u8c22","text":"<p>\"\u673a\u5668\u5b66\u4e60\u7b80\u4ecb\"\u7531 Muhammad Sakib Khan Inan, Ornella Altunyan \u53ca Jen Looper\uff0c\u5171\u540c\u503e \u2665\ufe0f \u800c\u4f5c</p> <p>\"\u673a\u5668\u5b66\u4e60\u53ca\u4eba\u5de5\u667a\u80fd\u5386\u53f2\" \u7531 Jen Looper \u53ca Amy Boyd\u503e \u2665\ufe0f \u800c\u4f5c</p> <p>\"\u516c\u5e73\u6027\u4e0e\u673a\u5668\u5b66\u4e60\" \u7531 Tomomi Imura \u503e \u2665\ufe0f \u800c\u4f5c </p> <p>\"\u673a\u5668\u5b66\u4e60\u7684\u6280\u672f\" \u7531 Jen Looper \u53ca Chris Noring \u503e \u2665\ufe0f \u800c\u4f5c</p>"},{"location":"2-Regression/","title":"Regression models for machine learning","text":""},{"location":"2-Regression/#regional-topic-regression-models-for-pumpkin-prices-in-north-america","title":"Regional topic: Regression models for pumpkin prices in North America \ud83c\udf83","text":"<p>In North America, pumpkins are often carved into scary faces for Halloween. Let's discover more about these fascinating vegetables!</p> <p></p> <p>Photo by Beth Teutschmann on Unsplash</p>"},{"location":"2-Regression/#what-you-will-learn","title":"What you will learn","text":"<p>\ud83c\udfa5 Click the image above for a quick introduction video to this lesson</p> <p>The lessons in this section cover types of regression in the context of machine learning. Regression models can help determine the relationship between variables. This type of model can predict values such as length, temperature, or age, thus uncovering relationships between variables as it analyzes data points.</p> <p>In this series of lessons, you'll discover the differences between linear and logistic regression, and when you should prefer one over the other.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video introducing regression models.</p> <p>In this group of lessons, you will get set up to begin machine learning tasks, including configuring Visual Studio Code to manage notebooks, the common environment for data scientists. You will discover Scikit-learn, a library for machine learning, and you will build your first models, focusing on Regression models in this chapter.</p> <p>There are useful low-code tools that can help you learn about working with regression models. Try Azure ML for this task</p>"},{"location":"2-Regression/#lessons","title":"Lessons","text":"<ol> <li>Tools of the trade</li> <li>Managing data</li> <li>Linear and polynomial regression</li> <li>Logistic regression</li> </ol>"},{"location":"2-Regression/#credits","title":"Credits","text":"<p>\"ML with regression\" was written with \u2665\ufe0f by Jen Looper</p> <p>\u2665\ufe0f Quiz contributors include: Muhammad Sakib Khan Inan and Ornella Altunyan</p> <p>The pumpkin dataset is suggested by this project on Kaggle and its data is sourced from the Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture. We have added some points around color based on variety to normalize the distribution. This data is in the public domain.</p>"},{"location":"2-Regression/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u56de\u5f52\u6a21\u578b","text":""},{"location":"2-Regression/README.zh-cn/#_2","title":"\u672c\u8282\u4e3b\u9898\uff1a \u5317\u7f8e\u5357\u74dc\u4ef7\u683c\u7684\u56de\u5f52\u6a21\u578b \ud83c\udf83","text":"<p>\u5728\u5317\u7f8e\uff0c\u5357\u74dc\u7ecf\u5e38\u5728\u4e07\u5723\u8282\u88ab\u523b\u4e0a\u5413\u4eba\u7684\u9b3c\u8138\u3002\u8ba9\u6211\u4eec\u6765\u6df1\u5165\u7814\u7a76\u4e00\u4e0b\u8fd9\u79cd\u5947\u5999\u7684\u852c\u83dc</p> <p></p> <p>Foto oleh Beth Teutschmann di Unsplash</p>"},{"location":"2-Regression/README.zh-cn/#_3","title":"\u4f60\u4f1a\u5b66\u5230\u4ec0\u4e48","text":"<p>\u8fd9\u8282\u7684\u8bfe\u7a0b\u5305\u62ec\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u7684\u591a\u79cd\u56de\u5f52\u6a21\u578b\u3002\u56de\u5f52\u6a21\u578b\u53ef\u4ee5\u660e\u786e\u591a\u79cd\u53d8\u91cf\u95f4\u7684_\u5173\u7cfb_\u3002\u8fd9\u79cd\u6a21\u578b\u53ef\u4ee5\u7528\u6765\u9884\u6d4b\u7c7b\u4f3c\u957f\u5ea6\u3001\u6e29\u5ea6\u548c\u5e74\u9f84\u4e4b\u7c7b\u7684\u503c\uff0c \u901a\u8fc7\u5206\u6790\u6570\u636e\u70b9\u6765\u63ed\u793a\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</p> <p>\u5728\u672c\u8282\u7684\u4e00\u7cfb\u5217\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u4f1a\u5b66\u5230\u7ebf\u6027\u56de\u5f52\u548c\u903b\u8f91\u56de\u5f52\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u5e76\u4e14\u4f60\u5c06\u77e5\u9053\u5bf9\u4e8e\u7279\u5b9a\u95ee\u9898\u5982\u4f55\u5728\u8fd9\u4e24\u79cd\u6a21\u578b\u4e2d\u8fdb\u884c\u9009\u62e9</p> <p>\u5728\u8fd9\u7ec4\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u4f1a\u51c6\u5907\u597d\u5305\u62ec\u4e3a\u7ba1\u7406\u7b14\u8bb0\u800c\u8bbe\u7f6eVS Code\u3001\u914d\u7f6e\u6570\u636e\u79d1\u5b66\u5bb6\u5e38\u7528\u7684\u73af\u5883\u7b49\u673a\u5668\u5b66\u4e60\u7684\u521d\u59cb\u4efb\u52a1\u3002\u4f60\u4f1a\u5f00\u59cb\u4e0a\u624bScikit-learn\u5b66\u4e60\u9879\u76ee\uff08\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u767e\u79d1\uff09\uff0c\u5e76\u4e14\u4f60\u4f1a\u4ee5\u56de\u5f52\u6a21\u578b\u4e3a\u4e3b\u6784\u5efa\u8d77\u4f60\u7684\u7b2c\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b</p> <p>\u8fd9\u91cc\u6709\u4e00\u4e9b\u4ee3\u7801\u96be\u5ea6\u8f83\u4f4e\u4f46\u5f88\u6709\u7528\u7684\u5de5\u5177\u53ef\u4ee5\u5e2e\u52a9\u4f60\u5b66\u4e60\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u3002 \u8bd5\u4e00\u4e0b Azure ML for this task</p>"},{"location":"2-Regression/README.zh-cn/#lessons","title":"Lessons","text":"<ol> <li>\u4ea4\u6613\u7684\u5de5\u5177</li> <li>\u7ba1\u7406\u6570\u636e</li> <li>\u7ebf\u6027\u548c\u591a\u9879\u5f0f\u56de\u5f52</li> <li>\u903b\u8f91\u56de\u5f52</li> </ol>"},{"location":"2-Regression/README.zh-cn/#credits","title":"Credits","text":"<p>\"\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u56de\u5f52\" \u7531Jen Looper\u2665\ufe0f \u64b0\u5199</p> <p>\u2665\ufe0f \u6d4b\u8bd5\u7684\u8d21\u732e\u8005: Muhammad Sakib Khan Inan \u548c Ornella Altunyan</p> <p>\u5357\u74dc\u6570\u636e\u96c6\u53d7\u6b64\u542f\u53d1 this project on Kaggle \u5e76\u4e14\u5176\u6570\u636e\u6e90\u81ea Specialty Crops Terminal Markets Standard Reports \u7531\u7f8e\u56fd\u519c\u4e1a\u90e8\u4e0a\u4f20\u5206\u4eab\u3002\u6211\u4eec\u6839\u636e\u79cd\u7c7b\u6dfb\u52a0\u4e86\u56f4\u7ed5\u989c\u8272\u7684\u4e00\u4e9b\u6570\u636e\u70b9\u3002\u8fd9\u4e9b\u6570\u636e\u5904\u5728\u516c\u5171\u7684\u57df\u540d\u4e0a\u3002</p>"},{"location":"2-Regression/1-Tools/","title":"Get started with Python and Scikit-learn for regression models","text":"<p>Sketchnote by Tomomi Imura</p>"},{"location":"2-Regression/1-Tools/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"2-Regression/1-Tools/#this-lesson-is-available-in-r","title":"This lesson is available in R!","text":""},{"location":"2-Regression/1-Tools/#introduction","title":"Introduction","text":"<p>In these four lessons, you will discover how to build regression models. We will discuss what these are for shortly. But before you do anything, make sure you have the right tools in place to start the process!</p> <p>In this lesson, you will learn how to:</p> <ul> <li>Configure your computer for local machine learning tasks.</li> <li>Work with Jupyter notebooks.</li> <li>Use Scikit-learn, including installation.</li> <li>Explore linear regression with a hands-on exercise.</li> </ul>"},{"location":"2-Regression/1-Tools/#installations-and-configurations","title":"Installations and configurations","text":"<p>\ud83c\udfa5 Click the image above for a short video working through configuring your computer for ML.</p> <ol> <li>Install Python. Ensure that Python is installed on your computer. You will use Python for many data science and machine learning tasks. Most computer systems already include a Python installation. There are useful Python Coding Packs available as well, to ease the setup for some users.</li> </ol> <p>Some usages of Python, however, require one version of the software, whereas others require a different version. For this reason, it's useful to work within a virtual environment.</p> <ol> <li>Install Visual Studio Code. Make sure you have Visual Studio Code installed on your computer. Follow these instructions to install Visual Studio Code for the basic installation. You are going to use Python in Visual Studio Code in this course, so you might want to brush up on how to configure Visual Studio Code for Python development.</li> </ol> <p>Get comfortable with Python by working through this collection of Learn modules</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: using Python within VS Code.</p> <ol> <li> <p>Install Scikit-learn, by following these instructions. Since you need to ensure that you use Python 3, it's recommended that you use a virtual environment. Note, if you are installing this library on a M1 Mac, there are special instructions on the page linked above.</p> </li> <li> <p>Install Jupyter Notebook. You will need to install the Jupyter package.</p> </li> </ol>"},{"location":"2-Regression/1-Tools/#your-ml-authoring-environment","title":"Your ML authoring environment","text":"<p>You are going to use notebooks to develop your Python code and create machine learning models. This type of file is a common tool for data scientists, and they can be identified by their suffix or extension <code>.ipynb</code>.</p> <p>Notebooks are an interactive environment that allow the developer to both code and add notes and write documentation around the code which is quite helpful for experimental or research-oriented projects.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video working through this exercise.</p>"},{"location":"2-Regression/1-Tools/#exercise-work-with-a-notebook","title":"Exercise - work with a notebook","text":"<p>In this folder, you will find the file notebook.ipynb.</p> <ol> <li>Open notebook.ipynb in Visual Studio Code.</li> </ol> <p>A Jupyter server will start with Python 3+ started. You will find areas of the notebook that can be <code>run</code>, pieces of code. You can run a code block, by selecting the icon that looks like a play button.</p> <ol> <li>Select the <code>md</code> icon and add a bit of markdown, and the following text # Welcome to your notebook.</li> </ol> <p>Next, add some Python code.</p> <ol> <li>Type print('hello notebook') in the code block.</li> <li>Select the arrow to run the code.</li> </ol> <p>You should see the printed statement:</p> <pre><code>```output\nhello notebook\n```\n</code></pre> <p></p> <p>You can interleaf your code with comments to self-document the notebook.</p> <p>\u2705 Think for a minute how different a web developer's working environment is versus that of a data scientist.</p>"},{"location":"2-Regression/1-Tools/#up-and-running-with-scikit-learn","title":"Up and running with Scikit-learn","text":"<p>Now that Python is set up in your local environment, and you are comfortable with Jupyter notebooks, let's get equally comfortable with Scikit-learn (pronounce it <code>sci</code> as in <code>science</code>). Scikit-learn provides an extensive API to help you perform ML tasks.</p> <p>According to their website, \"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\"</p> <p>In this course, you will use Scikit-learn and other tools to build machine learning models to perform what we call 'traditional machine learning' tasks. We have deliberately avoided neural networks and deep learning, as they are better covered in our forthcoming 'AI for Beginners' curriculum.</p> <p>Scikit-learn makes it straightforward to build models and evaluate them for use. It is primarily focused on using numeric data and contains several ready-made datasets for use as learning tools. It also includes pre-built models for students to try. Let's explore the process of loading prepackaged data and using a built in estimator  first ML model with Scikit-learn with some basic data.</p>"},{"location":"2-Regression/1-Tools/#exercise-your-first-scikit-learn-notebook","title":"Exercise - your first Scikit-learn notebook","text":"<p>This tutorial was inspired by the linear regression example on Scikit-learn's web site.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video working through this exercise.</p> <p>In the notebook.ipynb file associated to this lesson, clear out all the cells by pressing the 'trash can' icon.</p> <p>In this section, you will work with a small dataset about diabetes that is built into Scikit-learn for learning purposes. Imagine that you wanted to test a treatment for diabetic patients. Machine Learning models might help you determine which patients would respond better to the treatment, based on combinations of variables. Even a very basic regression model, when visualized, might show information about variables that would help you organize your theoretical clinical trials.</p> <p>\u2705 There are many types of regression methods, and which one you pick depends on the answer you're looking for. If you want to predict the probable height for a person of a given age, you'd use linear regression, as you're seeking a numeric value. If you're interested in discovering whether a type of cuisine should be considered vegan or not, you're looking for a category assignment so you would use logistic regression. You'll learn more about logistic regression later. Think a bit about some questions you can ask of data, and which of these methods would be more appropriate.</p> <p>Let's get started on this task.</p>"},{"location":"2-Regression/1-Tools/#import-libraries","title":"Import libraries","text":"<p>For this task we will import some libraries:</p> <ul> <li>matplotlib. It's a useful graphing tool and we will use it to create a line plot.</li> <li>numpy. numpy is a useful library for handling numeric data in Python.</li> <li>sklearn. This is the Scikit-learn library.</li> </ul> <p>Import some libraries to help with your tasks.</p> <ol> <li>Add imports by typing the following code:</li> </ol> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model, model_selection\n</code></pre> <p>Above you are importing <code>matplotlib</code>, <code>numpy</code> and you are importing <code>datasets</code>, <code>linear_model</code> and <code>model_selection</code> from <code>sklearn</code>. <code>model_selection</code> is used for splitting data into training and test sets.</p>"},{"location":"2-Regression/1-Tools/#the-diabetes-dataset","title":"The diabetes dataset","text":"<p>The built-in diabetes dataset includes 442 samples of data around diabetes, with 10 feature variables, some of which include:</p> <ul> <li>age: age in years</li> <li>bmi: body mass index</li> <li>bp: average blood pressure</li> <li>s1 tc: T-Cells (a type of white blood cells)</li> </ul> <p>\u2705 This dataset includes the concept of 'sex' as a feature variable important to research around diabetes. Many medical datasets include this type of binary classification. Think a bit about how categorizations such as this might exclude certain parts of a population from treatments.</p> <p>Now, load up the X and y data.</p> <p>\ud83c\udf93 Remember, this is supervised learning, and we need a named 'y' target.</p> <p>In a new code cell, load the diabetes dataset by calling <code>load_diabetes()</code>. The input <code>return_X_y=True</code> signals that <code>X</code> will be a data matrix, and <code>y</code> will be the regression target.</p> <ol> <li> <p>Add some print commands to show the shape of the data matrix and its first element:</p> <pre><code>X, y = datasets.load_diabetes(return_X_y=True)\nprint(X.shape)\nprint(X[0])\n</code></pre> <p>What you are getting back as a response, is a tuple. What you are doing is to assign the two first values of the tuple to <code>X</code> and <code>y</code> respectively. Learn more about tuples.</p> <p>You can see that this data has 442 items shaped in arrays of 10 elements:</p> <pre><code>(442, 10)\n[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n-0.04340085 -0.00259226  0.01990842 -0.01764613]\n</code></pre> <p>\u2705 Think a bit about the relationship between the data and the regression target. Linear regression predicts relationships between feature X and target variable y. Can you find the target for the diabetes dataset in the documentation? What is this dataset demonstrating, given that target?</p> </li> <li> <p>Next, select a portion of this dataset to plot by selecting the 3rd column of the dataset. You can do this by using the <code>:</code> operator to select all rows, and then selecting the 3rd column using the index (2). You can also reshape the data to be a 2D array - as required for plotting - by using <code>reshape(n_rows, n_columns)</code>. If one of the parameter is -1, the corresponding dimension is calculated automatically.</p> </li> </ol> <pre><code>X = X[:, 2]\nX = X.reshape((-1,1))\n</code></pre> <p>\u2705 At any time, print out the data to check its shape.</p> <ol> <li>Now that you have data ready to be plotted, you can see if a machine can help determine a logical split between the numbers in this dataset. To do this, you need to split both the data (X) and the target (y) into test and training sets. Scikit-learn has a straightforward way to do this; you can split your test data at a given point.</li> </ol> <pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)\n</code></pre> <ol> <li> <p>Now you are ready to train your model! Load up the linear regression model and train it with your X and y training sets using <code>model.fit()</code>:</p> <pre><code>model = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)\n</code></pre> <p>\u2705 <code>model.fit()</code> is a function you'll see in many ML libraries such as TensorFlow</p> </li> <li> <p>Then, create a prediction using test data, using the function <code>predict()</code>. This will be used to draw the line between data groups</p> <pre><code>y_pred = model.predict(X_test)\n</code></pre> </li> <li> <p>Now it's time to show the data in a plot. Matplotlib is a very useful tool for this task. Create a scatterplot of all the X and y test data, and use the prediction to draw a line in the most appropriate place, between the model's data groupings.</p> <pre><code>plt.scatter(X_test, y_test,  color='black')\nplt.plot(X_test, y_pred, color='blue', linewidth=3)\nplt.xlabel('Scaled BMIs')\nplt.ylabel('Disease Progression')\nplt.title('A Graph Plot Showing Diabetes Progression Against BMI')\nplt.show()\n</code></pre> </li> </ol> <p></p> <p>\u2705 Think a bit about what's going on here. A straight line is running through many small dots of data, but what is it doing exactly? Can you see how you should be able to use this line to predict where a new, unseen data point should fit in relationship to the plot's y axis? Try to put into words the practical use of this model.</p> <p>Congratulations, you built your first linear regression model, created a prediction with it, and displayed it in a plot!</p>"},{"location":"2-Regression/1-Tools/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Plot a different variable from this dataset. Hint: edit this line: <code>X = X[:,2]</code>. Given this dataset's target, what are you able to discover about the progression of diabetes as a disease?</p>"},{"location":"2-Regression/1-Tools/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"2-Regression/1-Tools/#review-self-study","title":"Review &amp; Self Study","text":"<p>In this tutorial, you worked with simple linear regression, rather than univariate or multiple linear regression. Read a little about the differences between these methods, or take a look at this video</p> <p>Read more about the concept of regression and think about what kinds of questions can be answered by this technique. Take this tutorial to deepen your understanding.</p>"},{"location":"2-Regression/1-Tools/#assignment","title":"Assignment","text":"<p>A different dataset</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/","title":"\u5f00\u59cb\u4f7f\u7528 Python \u548c Scikit \u5b66\u4e60\u56de\u5f52\u6a21\u578b","text":"<p>\u4f5c\u8005 Tomomi Imura</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b","text":""},{"location":"2-Regression/1-Tools/README.zh-cn/#_2","title":"\u4ecb\u7ecd","text":"<p>\u5728\u8fd9\u56db\u8282\u8bfe\u4e2d\uff0c\u4f60\u5c06\u4e86\u89e3\u5982\u4f55\u6784\u5efa\u56de\u5f52\u6a21\u578b\u3002\u6211\u4eec\u5c06\u5f88\u5feb\u8ba8\u8bba\u8fd9\u4e9b\u662f\u4ec0\u4e48\u3002\u4f46\u5728\u4f60\u505a\u4efb\u4f55\u4e8b\u60c5\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u4f60\u6709\u5408\u9002\u7684\u5de5\u5177\u6765\u5f00\u59cb\u8fd9\u4e2a\u8fc7\u7a0b\uff01</p> <p>\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5982\u4f55\uff1a</p> <ul> <li>\u4e3a\u672c\u5730\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u914d\u7f6e\u4f60\u7684\u8ba1\u7b97\u673a\u3002</li> <li>\u4f7f\u7528 Jupyter notebooks\u3002</li> <li>\u4f7f\u7528 Scikit-learn\uff0c\u5305\u62ec\u5b89\u88c5\u3002</li> <li>\u901a\u8fc7\u52a8\u624b\u7ec3\u4e60\u63a2\u7d22\u7ebf\u6027\u56de\u5f52\u3002 </li> </ul>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_3","title":"\u5b89\u88c5\u548c\u914d\u7f6e","text":"<p>\ud83c\udfa5 \u5355\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1a\u5728 VS Code \u4e2d\u4f7f\u7528 Python\u3002 </p> <ol> <li>\u5b89\u88c5 Python\u3002\u786e\u4fdd\u4f60\u7684\u8ba1\u7b97\u673a\u4e0a\u5b89\u88c5\u4e86 Python\u3002\u4f60\u5c06\u5728\u8bb8\u591a\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f7f\u7528 Python\u3002\u5927\u591a\u6570\u8ba1\u7b97\u673a\u7cfb\u7edf\u5df2\u7ecf\u5b89\u88c5\u4e86 Python\u3002\u4e5f\u6709\u4e00\u4e9b\u6709\u7528\u7684 Python \u7f16\u7801\u5305 \u53ef\u7528\u4e8e\u7b80\u5316\u67d0\u4e9b\u7528\u6237\u7684\u8bbe\u7f6e\u3002 </li> </ol> <p>\u7136\u800c\uff0cPython \u7684\u67d0\u4e9b\u7528\u6cd5\u9700\u8981\u4e00\u4e2a\u7248\u672c\u7684\u8f6f\u4ef6\uff0c\u800c\u5176\u4ed6\u7528\u6cd5\u5219\u9700\u8981\u53e6\u4e00\u4e2a\u4e0d\u540c\u7684\u7248\u672c\u3002 \u56e0\u6b64\uff0c\u5728 \u865a\u62df\u73af\u5883 \u4e2d\u5de5\u4f5c\u5f88\u6709\u7528\u3002</p> <ol> <li>\u5b89\u88c5 Visual Studio Code\u3002\u786e\u4fdd\u4f60\u7684\u8ba1\u7b97\u673a\u4e0a\u5b89\u88c5\u4e86 Visual Studio Code\u3002\u6309\u7167\u8fd9\u4e9b\u8bf4\u660e \u5b89\u88c5 Visual Studio Code \u8fdb\u884c\u57fa\u672c\u5b89\u88c5\u3002\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5728 Visual Studio Code \u4e2d\u4f7f\u7528 Python\uff0c\u56e0\u6b64\u4f60\u53ef\u80fd\u60f3\u590d\u4e60\u5982\u4f55 \u914d\u7f6e Visual Studio Code \u7528\u4e8e Python \u5f00\u53d1\u3002 </li> </ol> <p>\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e00\u7cfb\u5217\u7684 \u5b66\u4e60\u6a21\u5757 \u719f\u6089 Python</p> <ol> <li> <p>\u6309\u7167 \u8fd9\u4e9b\u8bf4\u660e \u5b89\u88c5 Scikit learn\u3002\u7531\u4e8e\u4f60\u9700\u8981\u786e\u4fdd\u4f7f\u7528 Python3\uff0c\u56e0\u6b64\u5efa\u8bae\u4f60\u4f7f\u7528\u865a\u62df\u73af\u5883\u3002\u6ce8\u610f\uff0c\u5982\u679c\u4f60\u662f\u5728 M1 Mac \u4e0a\u5b89\u88c5\u8fd9\u4e2a\u5e93\uff0c\u5728\u4e0a\u9762\u94fe\u63a5\u7684\u9875\u9762\u4e0a\u6709\u7279\u522b\u7684\u8bf4\u660e\u3002</p> </li> <li> <p>\u5b89\u88c5 Jupyter Notebook\u3002\u4f60\u9700\u8981 \u5b89\u88c5 Jupyter \u5305\u3002</p> </li> </ol>"},{"location":"2-Regression/1-Tools/README.zh-cn/#ml","title":"\u4f60\u7684 ML \u5de5\u4f5c\u73af\u5883","text":"<p>\u4f60\u5c06\u4f7f\u7528 notebooks \u5f00\u53d1 Python \u4ee3\u7801\u5e76\u521b\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u6587\u4ef6\u662f\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5e38\u7528\u5de5\u5177\uff0c\u53ef\u4ee5\u901a\u8fc7\u540e\u7f00\u6216\u6269\u5c55\u540d <code>.ipynb</code> \u6765\u8bc6\u522b\u5b83\u4eec\u3002</p> <p>Notebooks \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u73af\u5883\uff0c\u5141\u8bb8\u5f00\u53d1\u4eba\u5458\u7f16\u5199\u4ee3\u7801\u5e76\u6dfb\u52a0\u6ce8\u91ca\u5e76\u56f4\u7ed5\u4ee3\u7801\u7f16\u5199\u6587\u6863\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9a8c\u6216\u9762\u5411\u7814\u7a76\u7684\u9879\u76ee\u975e\u5e38\u6709\u5e2e\u52a9\u3002 </p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#-notebook","title":"\u7ec3\u4e60 - \u4f7f\u7528 notebook","text":"<ol> <li> <p>\u5728 Visual Studio Code \u4e2d\u6253\u5f00 notebook.ipynb\u3002</p> <p>Jupyter \u670d\u52a1\u5668\u5c06\u4ee5 python3+\u542f\u52a8\u3002\u4f60\u4f1a\u53d1\u73b0 notebook \u53ef\u4ee5\u201c\u8fd0\u884c\u201d\u7684\u533a\u57df\u3001\u4ee3\u7801\u5757\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u770b\u8d77\u6765\u50cf\u64ad\u653e\u6309\u94ae\u7684\u56fe\u6807\u6765\u8fd0\u884c\u4ee3\u7801\u5757\u3002</p> </li> <li> <p>\u9009\u62e9 <code>md</code> \u56fe\u6807\u5e76\u6dfb\u52a0\u4e00\u70b9 markdown\uff0c\u8f93\u5165\u6587\u5b57 # Welcome to your notebook\u3002</p> </li> </ol> <p>\u63a5\u4e0b\u6765\uff0c\u6dfb\u52a0\u4e00\u4e9b Python \u4ee3\u7801\u3002</p> <ol> <li> <p>\u5728\u4ee3\u7801\u5757\u4e2d\u8f93\u5165 print(\"hello notebook\")\u3002</p> </li> <li> <p>\u9009\u62e9\u7bad\u5934\u8fd0\u884c\u4ee3\u7801\u3002</p> <p>\u4f60\u5e94\u8be5\u770b\u5230\u6253\u5370\u7684\u8bed\u53e5\uff1a </p> <pre><code>hello notebook\n</code></pre> </li> </ol> <p></p> <p>\u4f60\u53ef\u4ee5\u4e3a\u4f60\u7684\u4ee3\u7801\u6dfb\u52a0\u6ce8\u91ca\uff0c\u4ee5\u4fbf notebook \u53ef\u4ee5\u81ea\u63cf\u8ff0\u3002</p> <p>\u2705 \u60f3\u4e00\u60f3 web \u5f00\u53d1\u4eba\u5458\u7684\u5de5\u4f5c\u73af\u5883\u4e0e\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5de5\u4f5c\u73af\u5883\u6709\u591a\u5927\u7684\u4e0d\u540c\u3002</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#scikit-learn","title":"\u542f\u52a8\u5e76\u8fd0\u884c Scikit-learn","text":"<p>\u73b0\u5728 Python \u5df2\u5728\u4f60\u7684\u672c\u5730\u73af\u5883\u4e2d\u8bbe\u7f6e\u597d\uff0c\u5e76\u4e14\u4f60\u5bf9 Jupyter notebook \u611f\u5230\u6ee1\u610f\uff0c\u8ba9\u6211\u4eec\u540c\u6837\u719f\u6089 Scikit-learn\uff08\u5728\u201cscience\u201d\u4e2d\u53d1\u97f3\u4e3a\u201csci\u201d\uff09\u3002 Scikit-learn \u63d0\u4f9b\u4e86 \u5927\u91cf\u7684 API \u6765\u5e2e\u52a9\u4f60\u6267\u884c ML \u4efb\u52a1\u3002</p> <p>\u6839\u636e\u4ed6\u4eec\u7684 \u7f51\u7ad9\uff0c\u201cScikit-learn \u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u652f\u6301\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u3002\u5b83\u8fd8\u63d0\u4f9b\u4e86\u5404\u79cd\u6a21\u578b\u62df\u5408\u5de5\u5177\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\u4ee5\u53ca\u8bb8\u591a\u5176\u4ed6\u5b9e\u7528\u7a0b\u5e8f\u3002\u201d</p> <p>\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u4f7f\u7528 Scikit-learn \u548c\u5176\u4ed6\u5de5\u5177\u6765\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u6267\u884c\u6211\u4eec\u6240\u8c13\u7684\u201c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u201d\u4efb\u52a1\u3002\u6211\u4eec\u7279\u610f\u907f\u514d\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u6211\u4eec\u5373\u5c06\u63a8\u51fa\u7684\u201c\u9762\u5411\u521d\u5b66\u8005\u7684\u4eba\u5de5\u667a\u80fd\u201d\u8bfe\u7a0b\u4e2d\u5f97\u5230\u4e86\u66f4\u597d\u7684\u4ecb\u7ecd\u3002 </p> <p>Scikit-learn \u4f7f\u6784\u5efa\u6a21\u578b\u548c\u8bc4\u4f30\u5b83\u4eec\u7684\u4f7f\u7528\u53d8\u5f97\u7b80\u5355\u3002\u5b83\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4f7f\u7528\u6570\u5b57\u6570\u636e\uff0c\u5e76\u5305\u542b\u51e0\u4e2a\u73b0\u6210\u7684\u6570\u636e\u96c6\u7528\u4f5c\u5b66\u4e60\u5de5\u5177\u3002\u5b83\u8fd8\u5305\u62ec\u4f9b\u5b66\u751f\u5c1d\u8bd5\u7684\u9884\u5efa\u6a21\u578b\u3002\u8ba9\u6211\u4eec\u63a2\u7d22\u52a0\u8f7d\u9884\u5148\u6253\u5305\u7684\u6570\u636e\u548c\u4f7f\u7528\u5185\u7f6e\u7684 estimator first ML \u6a21\u578b\u548c Scikit-learn \u4ee5\u53ca\u4e00\u4e9b\u57fa\u672c\u6570\u636e\u7684\u8fc7\u7a0b\u3002</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#-scikit-learn-notebook","title":"\u7ec3\u4e60 - \u4f60\u7684\u7b2c\u4e00\u4e2a Scikit-learn notebook","text":"<p>\u672c\u6559\u7a0b\u7684\u7075\u611f\u6765\u81ea Scikit-learn \u7f51\u7ad9\u4e0a\u7684 \u7ebf\u6027\u56de\u5f52\u793a\u4f8b\u3002</p> <p>\u5728\u4e0e\u672c\u8bfe\u7a0b\u76f8\u5173\u7684 notebook.ipynb \u6587\u4ef6\u4e2d\uff0c\u901a\u8fc7\u70b9\u51fb\u201c\u5783\u573e\u6876\u201d\u56fe\u6807\u6e05\u9664\u6240\u6709\u5355\u5143\u683c\u3002 </p> <p>\u5728\u672c\u8282\u4e2d\uff0c\u4f60\u5c06\u4f7f\u7528\u4e00\u4e2a\u5173\u4e8e\u7cd6\u5c3f\u75c5\u7684\u5c0f\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5185\u7f6e\u4e8e Scikit-learn \u4e2d\u4ee5\u7528\u4e8e\u5b66\u4e60\u76ee\u7684\u3002\u60f3\u8c61\u4e00\u4e0b\uff0c\u4f60\u60f3\u4e3a\u7cd6\u5c3f\u75c5\u60a3\u8005\u6d4b\u8bd5\u4e00\u79cd\u6cbb\u7597\u65b9\u6cd5\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u4f1a\u5e2e\u52a9\u4f60\u6839\u636e\u53d8\u91cf\u7ec4\u5408\u786e\u5b9a\u54ea\u4e9b\u60a3\u8005\u5bf9\u6cbb\u7597\u53cd\u5e94\u66f4\u597d\u3002\u5373\u4f7f\u662f\u975e\u5e38\u57fa\u672c\u7684\u56de\u5f52\u6a21\u578b\uff0c\u5728\u53ef\u89c6\u5316\u65f6\uff0c\u4e5f\u53ef\u80fd\u4f1a\u663e\u793a\u6709\u52a9\u4e8e\u7ec4\u7ec7\u7406\u8bba\u4e34\u5e8a\u8bd5\u9a8c\u7684\u53d8\u91cf\u4fe1\u606f\u3002</p> <p>\u2705 \u56de\u5f52\u65b9\u6cd5\u6709\u5f88\u591a\u79cd\uff0c\u4f60\u9009\u62e9\u54ea\u4e00\u79cd\u53d6\u51b3\u4e8e\u4f60\u6b63\u5728\u5bfb\u627e\u7684\u7b54\u6848\u3002\u5982\u679c\u4f60\u60f3\u9884\u6d4b\u7ed9\u5b9a\u5e74\u9f84\u7684\u4eba\u7684\u53ef\u80fd\u8eab\u9ad8\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\uff0c\u56e0\u4e3a\u4f60\u6b63\u5728\u5bfb\u627e\u6570\u503c\u3002\u5982\u679c\u4f60\u6709\u5174\u8da3\u4e86\u89e3\u67d0\u79cd\u83dc\u80b4\u662f\u5426\u5e94\u88ab\u89c6\u4e3a\u7d20\u98df\u4e3b\u4e49\u8005\uff0c\u90a3\u4e48\u4f60\u6b63\u5728\u5bfb\u627e\u7c7b\u522b\u5206\u914d\uff0c\u4ee5\u4fbf\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u3002\u7a0d\u540e\u4f60\u5c06\u4e86\u89e3\u6709\u5173\u903b\u8f91\u56de\u5f52\u7684\u66f4\u591a\u4fe1\u606f\u3002\u60f3\u4e00\u60f3\u4f60\u53ef\u4ee5\u5bf9\u6570\u636e\u63d0\u51fa\u7684\u4e00\u4e9b\u95ee\u9898\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\u7684\u54ea\u4e00\u4e2a\u66f4\u5408\u9002\u3002</p> <p>\u8ba9\u6211\u4eec\u5f00\u59cb\u8fd9\u9879\u4efb\u52a1\u3002</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_4","title":"\u5bfc\u5165\u5e93","text":"<p>\u5bf9\u4e8e\u6b64\u4efb\u52a1\uff0c\u6211\u4eec\u5c06\u5bfc\u5165\u4e00\u4e9b\u5e93\uff1a</p> <ul> <li>matplotlib\u3002\u8fd9\u662f\u4e00\u4e2a\u6709\u7528\u7684 \u7ed8\u56fe\u5de5\u5177\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u6765\u521b\u5efa\u7ebf\u56fe\u3002</li> <li>numpy\u3002 numpy \u662f\u4e00\u4e2a\u6709\u7528\u7684\u5e93\uff0c\u7528\u4e8e\u5728 Python \u4e2d\u5904\u7406\u6570\u5b57\u6570\u636e\u3002</li> <li>sklearn\u3002\u8fd9\u662f Scikit-learn \u5e93\u3002</li> </ul> <p>\u5bfc\u5165\u4e00\u4e9b\u5e93\u6765\u5e2e\u52a9\u4f60\u5b8c\u6210\u4efb\u52a1\u3002</p> <ol> <li> <p>\u901a\u8fc7\u8f93\u5165\u4ee5\u4e0b\u4ee3\u7801\u6dfb\u52a0\u5bfc\u5165\uff1a</p> <p><code>python    import matplotlib.pyplot as plt    import numpy as np    from sklearn import datasets, linear_model, model_selection</code></p> <p>\u5728\u4e0a\u9762\u7684\u4ee3\u7801\u4e2d\uff0c\u4f60\u6b63\u5728\u5bfc\u5165 <code>matplottlib</code>\u3001<code>numpy</code>\uff0c\u4f60\u6b63\u5728\u4ece <code>sklearn</code> \u5bfc\u5165 <code>datasets</code>\u3001<code>linear_model</code> \u548c <code>model_selection</code>\u3002 <code>model_selection</code> \u7528\u4e8e\u5c06\u6570\u636e\u62c6\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u3002 </p> </li> </ol>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_5","title":"\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6","text":"<p>\u5185\u7f6e\u7684 \u7cd6\u5c3f\u75c5\u6570\u636e\u96c6 \u5305\u542b 442 \u4e2a\u56f4\u7ed5\u7cd6\u5c3f\u75c5\u7684\u6570\u636e\u6837\u672c\uff0c\u5177\u6709 10 \u4e2a\u7279\u5f81\u53d8\u91cf\uff0c\u5176\u4e2d\u5305\u62ec\uff1a</p> <ul> <li>age\uff1a\u5c81\u6570</li> <li>bmi\uff1a\u4f53\u91cd\u6307\u6570</li> <li>bp\uff1a\u5e73\u5747\u8840\u538b</li> <li>s1 tc\uff1aT \u7ec6\u80de\uff08\u4e00\u79cd\u767d\u7ec6\u80de\uff09</li> </ul> <p>\u2705 \u8be5\u6570\u636e\u96c6\u5305\u62ec\u201c\u6027\u522b\u201d\u7684\u6982\u5ff5\uff0c\u4f5c\u4e3a\u5bf9\u7cd6\u5c3f\u75c5\u7814\u7a76\u5f88\u91cd\u8981\u7684\u7279\u5f81\u53d8\u91cf\u3002\u8bb8\u591a\u533b\u5b66\u6570\u636e\u96c6\u5305\u62ec\u8fd9\u79cd\u7c7b\u578b\u7684\u4e8c\u5143\u5206\u7c7b\u3002\u60f3\u4e00\u60f3\u8bf8\u5982\u6b64\u7c7b\u7684\u5206\u7c7b\u5982\u4f55\u5c06\u4eba\u7fa4\u7684\u67d0\u4e9b\u90e8\u5206\u6392\u9664\u5728\u6cbb\u7597\u4e4b\u5916\u3002</p> <p>\u73b0\u5728\uff0c\u52a0\u8f7d X \u548c y \u6570\u636e\u3002</p> <p>\ud83c\udf93 \u8bf7\u8bb0\u4f4f\uff0c\u8fd9\u662f\u76d1\u7763\u5b66\u4e60\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u547d\u540d\u4e3a\u201cy\u201d\u7684\u76ee\u6807\u3002</p> <p>\u5728\u65b0\u7684\u4ee3\u7801\u5355\u5143\u4e2d\uff0c\u901a\u8fc7\u8c03\u7528 <code>load_diabetes()</code> \u52a0\u8f7d\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u3002\u8f93\u5165 <code>return_X_y=True</code> \u8868\u793a <code>X</code> \u5c06\u662f\u4e00\u4e2a\u6570\u636e\u77e9\u9635\uff0c\u800c<code>y</code>\u5c06\u662f\u56de\u5f52\u76ee\u6807\u3002</p> <ol> <li> <p>\u6dfb\u52a0\u4e00\u4e9b\u6253\u5370\u547d\u4ee4\u6765\u663e\u793a\u6570\u636e\u77e9\u9635\u7684\u5f62\u72b6\u53ca\u5176\u7b2c\u4e00\u4e2a\u5143\u7d20\uff1a </p> <pre><code>X, y = datasets.load_diabetes(return_X_y=True)\nprint(X.shape)\nprint(X[0])\n</code></pre> <p>\u4f5c\u4e3a\u54cd\u5e94\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u5143\u7ec4\u3002\u4f60\u6b63\u5728\u505a\u7684\u662f\u5c06\u5143\u7ec4\u7684\u524d\u4e24\u4e2a\u503c\u5206\u522b\u5206\u914d\u7ed9 <code>X</code> \u548c <code>y</code>\u3002\u4e86\u89e3\u66f4\u591a \u5173\u4e8e\u5143\u7ec4\u3002</p> <p>\u4f60\u53ef\u4ee5\u770b\u5230\u8fd9\u4e2a\u6570\u636e\u6709 442 \u4e2a\u9879\u76ee\uff0c\u7ec4\u6210\u4e86 10 \u4e2a\u5143\u7d20\u7684\u6570\u7ec4\uff1a</p> <pre><code>(442, 10)\n[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n-0.04340085 -0.00259226  0.01990842 -0.01764613]\n</code></pre> <p>\u2705 \u7a0d\u5fae\u601d\u8003\u4e00\u4e0b\u6570\u636e\u548c\u56de\u5f52\u76ee\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7ebf\u6027\u56de\u5f52\u9884\u6d4b\u7279\u5f81 X \u548c\u76ee\u6807\u53d8\u91cf y \u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4f60\u80fd\u5728\u6587\u6863\u4e2d\u627e\u5230\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u7684 \u76ee\u6807 \u5417\uff1f\u9274\u4e8e\u8be5\u76ee\u6807\uff0c\u8be5\u6570\u636e\u96c6\u5c55\u793a\u4e86\u4ec0\u4e48\uff1f</p> </li> <li> <p>\u63a5\u4e0b\u6765\uff0c\u901a\u8fc7\u4f7f\u7528 numpy \u7684 <code>newaxis</code> \u51fd\u6570\u5c06\u6570\u636e\u96c6\u7684\u4e00\u90e8\u5206\u6392\u5217\u5230\u4e00\u4e2a\u65b0\u6570\u7ec4\u4e2d\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6839\u636e\u5b83\u786e\u5b9a\u7684\u6a21\u5f0f\u5728\u6b64\u6570\u636e\u4e2d\u7684\u503c\u4e4b\u95f4\u751f\u6210\u4e00\u6761\u7ebf\u3002</p> </li> </ol> <pre><code>X = X[:, np.newaxis, 2]\n</code></pre> <p>\u2705 \u968f\u65f6\u6253\u5370\u6570\u636e\u4ee5\u68c0\u67e5\u5176\u5f62\u72b6\u3002</p> <ol> <li>\u73b0\u5728\u4f60\u5df2\u51c6\u5907\u597d\u7ed8\u5236\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u8ba1\u7b97\u673a\u662f\u5426\u53ef\u4ee5\u5e2e\u52a9\u786e\u5b9a\u6b64\u6570\u636e\u96c6\u4e2d\u6570\u5b57\u4e4b\u95f4\u7684\u903b\u8f91\u5206\u5272\u3002\u4e3a\u6b64\u4f60\u9700\u8981\u5c06\u6570\u636e (X) \u548c\u76ee\u6807 (y) \u62c6\u5206\u4e3a\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\u3002Scikit-learn \u6709\u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u505a\u5230\u8fd9\u4e00\u70b9\uff1b\u4f60\u53ef\u4ee5\u5728\u7ed9\u5b9a\u70b9\u62c6\u5206\u6d4b\u8bd5\u6570\u636e\u3002</li> </ol> <pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)\n</code></pre> <ol> <li> <p>\u73b0\u5728\u4f60\u5df2\u51c6\u5907\u597d\u8bad\u7ec3\u4f60\u7684\u6a21\u578b\uff01\u52a0\u8f7d\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5e76\u4f7f\u7528 <code>model.fit()</code> \u4f7f\u7528 X \u548c y \u8bad\u7ec3\u96c6\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3\uff1a</p> <pre><code>model = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)\n</code></pre> <p>\u2705 <code>model.fit()</code> \u662f\u4e00\u4e2a\u4f60\u4f1a\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u5e93\uff08\u4f8b\u5982 TensorFlow\uff09\u4e2d\u770b\u5230\u7684\u51fd\u6570</p> </li> <li> <p>\u7136\u540e\uff0c\u4f7f\u7528\u51fd\u6570 <code>predict()</code>\uff0c\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u521b\u5efa\u9884\u6d4b\u3002\u8fd9\u5c06\u7528\u4e8e\u7ed8\u5236\u6570\u636e\u7ec4\u4e4b\u95f4\u7684\u7ebf</p> <pre><code>y_pred = model.predict(X_test)\n</code></pre> </li> <li> <p>\u73b0\u5728\u662f\u65f6\u5019\u5728\u56fe\u4e2d\u663e\u793a\u6570\u636e\u4e86\u3002Matplotlib \u662f\u5b8c\u6210\u6b64\u4efb\u52a1\u7684\u975e\u5e38\u6709\u7528\u7684\u5de5\u5177\u3002\u521b\u5efa\u6240\u6709 X \u548c y \u6d4b\u8bd5\u6570\u636e\u7684\u6563\u70b9\u56fe\uff0c\u5e76\u4f7f\u7528\u9884\u6d4b\u5728\u6a21\u578b\u7684\u6570\u636e\u5206\u7ec4\u4e4b\u95f4\u6700\u5408\u9002\u7684\u4f4d\u7f6e\u753b\u4e00\u6761\u7ebf\u3002</p> <pre><code>plt.scatter(X_test, y_test,  color='black')\nplt.plot(X_test, y_pred, color='blue', linewidth=3)\nplt.show()\n</code></pre> </li> </ol> <p></p> <p>\u2705 \u60f3\u4e00\u60f3\u8fd9\u91cc\u53d1\u751f\u4e86\u4ec0\u4e48\u3002\u4e00\u6761\u76f4\u7ebf\u7a7f\u8fc7\u8bb8\u591a\u5c0f\u6570\u636e\u70b9\uff0c\u4f46\u5b83\u5230\u5e95\u5728\u505a\u4ec0\u4e48\uff1f\u4f60\u80fd\u770b\u5230\u4f60\u5e94\u8be5\u5982\u4f55\u4f7f\u7528\u8fd9\u6761\u7ebf\u6765\u9884\u6d4b\u4e00\u4e2a\u65b0\u7684\u3001\u672a\u89c1\u8fc7\u7684\u6570\u636e\u70b9\u5bf9\u5e94\u7684 y \u8f74\u503c\u5417\uff1f\u5c1d\u8bd5\u7528\u8bed\u8a00\u63cf\u8ff0\u8be5\u6a21\u578b\u7684\u5b9e\u9645\u7528\u9014\u3002</p> <p>\u606d\u559c\uff0c\u4f60\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u4f7f\u7528\u5b83\u521b\u5efa\u4e86\u9884\u6d4b\uff0c\u5e76\u5c06\u5176\u663e\u793a\u5728\u7ed8\u56fe\u4e2d\uff01</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_6","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u4ece\u8fd9\u4e2a\u6570\u636e\u96c6\u4e2d\u7ed8\u5236\u4e00\u4e2a\u4e0d\u540c\u7684\u53d8\u91cf\u3002\u63d0\u793a\uff1a\u7f16\u8f91\u8fd9\u4e00\u884c\uff1a<code>X = X[:, np.newaxis, 2]</code>\u3002\u9274\u4e8e\u6b64\u6570\u636e\u96c6\u7684\u76ee\u6807\uff0c\u4f60\u80fd\u591f\u53d1\u73b0\u7cd6\u5c3f\u75c5\u4f5c\u4e3a\u4e00\u79cd\u75be\u75c5\u7684\u8fdb\u5c55\u60c5\u51b5\u5417\uff1f</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_7","title":"\u8bfe\u540e\u6d4b","text":""},{"location":"2-Regression/1-Tools/README.zh-cn/#_8","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u4f7f\u7528\u4e86\u7b80\u5355\u7ebf\u6027\u56de\u5f52\uff0c\u800c\u4e0d\u662f\u5355\u53d8\u91cf\u6216\u591a\u5143\u7ebf\u6027\u56de\u5f52\u3002\u9605\u8bfb\u4e00\u4e9b\u5173\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u5dee\u5f02\u7684\u4fe1\u606f\uff0c\u6216\u67e5\u770b \u6b64\u89c6\u9891</p> <p>\u9605\u8bfb\u6709\u5173\u56de\u5f52\u6982\u5ff5\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u601d\u8003\u8fd9\u79cd\u6280\u672f\u53ef\u4ee5\u56de\u7b54\u54ea\u4e9b\u7c7b\u578b\u7684\u95ee\u9898\u3002\u7528\u8fd9\u4e2a \u6559\u7a0b \u52a0\u6df1\u4f60\u7684\u7406\u89e3\u3002</p>"},{"location":"2-Regression/1-Tools/README.zh-cn/#_9","title":"\u4efb\u52a1","text":"<p>\u4e0d\u540c\u7684\u6570\u636e\u96c6</p>"},{"location":"2-Regression/1-Tools/assignment/","title":"Regression with Scikit-learn","text":""},{"location":"2-Regression/1-Tools/assignment/#instructions","title":"Instructions","text":"<p>Take a look at the Linnerud dataset in Scikit-learn. This dataset has multiple targets: 'It consists of three exercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club'.</p> <p>In your own words, describe how to create a Regression model that would plot the relationship between the waistline and how many situps are accomplished. Do the same for the other datapoints in this dataset.</p>"},{"location":"2-Regression/1-Tools/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement Submit a descriptive paragraph Well-written paragraph is submitted A few sentences are submitted No description is supplied"},{"location":"2-Regression/1-Tools/assignment.zh-cn/","title":"\u7528 Scikit-learn \u5b9e\u73b0\u4e00\u6b21\u56de\u5f52\u7b97\u6cd5","text":""},{"location":"2-Regression/1-Tools/assignment.zh-cn/#_1","title":"\u8bf4\u660e","text":"<p>\u5148\u770b\u770b Scikit-learn \u4e2d\u7684 Linnerud \u6570\u636e\u96c6 \u8fd9\u4e2a\u6570\u636e\u96c6\u4e2d\u6709\u591a\u4e2a\u76ee\u6807\u53d8\u91cf\uff08target\uff09\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e09\u79cd\u8fd0\u52a8\uff08\u8bad\u7ec3\u6570\u636e\uff09\u548c\u4e09\u4e2a\u751f\u7406\u6307\u6807\uff08\u76ee\u6807\u53d8\u91cf\uff09\u7ec4\u6210\uff0c\u8fd9\u4e9b\u6570\u636e\u90fd\u662f\u4ece\u4e00\u4e2a\u5065\u8eab\u4ff1\u4e50\u90e8\u4e2d\u7684 20 \u540d\u4e2d\u5e74\u7537\u5b50\u6536\u96c6\u5230\u7684\u3002</p> <p>\u4e4b\u540e\u7528\u81ea\u5df1\u7684\u65b9\u5f0f\uff0c\u521b\u5efa\u4e00\u4e2a\u53ef\u4ee5\u63cf\u8ff0\u8170\u56f4\u548c\u5b8c\u6210\u4ef0\u5367\u8d77\u5750\u4e2a\u6570\u5173\u7cfb\u7684\u56de\u5f52\u6a21\u578b\u3002\u7528\u540c\u6837\u7684\u65b9\u5f0f\u5bf9\u8fd9\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u5176\u5b83\u6570\u636e\u4e5f\u5efa\u7acb\u4e00\u4e0b\u6a21\u578b\u63a2\u7a76\u4e00\u4e0b\u5176\u4e2d\u7684\u5173\u7cfb\u3002</p>"},{"location":"2-Regression/1-Tools/assignment.zh-cn/#_2","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u9700\u8981\u63d0\u4ea4\u4e00\u6bb5\u80fd\u63cf\u8ff0\u6570\u636e\u96c6\u4e2d\u5173\u7cfb\u7684\u6587\u5b57 \u5f88\u597d\u7684\u63cf\u8ff0\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u5173\u7cfb \u53ea\u80fd\u63cf\u8ff0\u5c11\u90e8\u5206\u7684\u5173\u7cfb \u5565\u90fd\u6ca1\u6709\u63d0\u4ea4"},{"location":"2-Regression/1-Tools/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"2-Regression/2-Data/","title":"Build a regression model using Scikit-learn: prepare and visualize data","text":"<p>Infographic by Dasani Madipalli</p>"},{"location":"2-Regression/2-Data/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"2-Regression/2-Data/#this-lesson-is-available-in-r","title":"This lesson is available in R!","text":""},{"location":"2-Regression/2-Data/#introduction","title":"Introduction","text":"<p>Now that you are set up with the tools you need to start tackling machine learning model building with Scikit-learn, you are ready to start asking questions of your data. As you work with data and apply ML solutions, it's very important to understand how to ask the right question to properly unlock the potentials of your dataset.</p> <p>In this lesson, you will learn:</p> <ul> <li>How to prepare your data for model-building.</li> <li>How to use Matplotlib for data visualization.</li> </ul>"},{"location":"2-Regression/2-Data/#asking-the-right-question-of-your-data","title":"Asking the right question of your data","text":"<p>The question you need answered will determine what type of ML algorithms you will leverage. And the quality of the answer you get back will be heavily dependent on the nature of your data.</p> <p>Take a look at the data provided for this lesson. You can open this .csv file in VS Code. A quick skim immediately shows that there are blanks and a mix of strings and numeric data. There's also a strange column called 'Package' where the data is a mix between 'sacks', 'bins' and other values. The data, in fact, is a bit of a mess.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video working through preparing the data for this lesson.</p> <p>In fact, it is not very common to be gifted a dataset that is completely ready to use to create a ML model out of the box. In this lesson, you will learn how to prepare a raw dataset using standard Python libraries. You will also learn various techniques to visualize the data.</p>"},{"location":"2-Regression/2-Data/#case-study-the-pumpkin-market","title":"Case study: 'the pumpkin market'","text":"<p>In this folder you will find a .csv file in the root <code>data</code> folder called US-pumpkins.csv which includes 1757 lines of data about the market for pumpkins, sorted into groupings by city. This is raw data extracted from the Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture.</p>"},{"location":"2-Regression/2-Data/#preparing-data","title":"Preparing data","text":"<p>This data is in the public domain. It can be downloaded in many separate files, per city, from the USDA web site. To avoid too many separate files, we have concatenated all the city data into one spreadsheet, thus we have already prepared the data a bit. Next, let's take a closer look at the data.</p>"},{"location":"2-Regression/2-Data/#the-pumpkin-data-early-conclusions","title":"The pumpkin data - early conclusions","text":"<p>What do you notice about this data? You already saw that there is a mix of strings, numbers, blanks and strange values that you need to make sense of.</p> <p>What question can you ask of this data, using a Regression technique? What about \"Predict the price of a pumpkin for sale during a given month\". Looking again at the data, there are some changes you need to make to create the data structure necessary for the task.</p>"},{"location":"2-Regression/2-Data/#exercise-analyze-the-pumpkin-data","title":"Exercise - analyze the pumpkin data","text":"<p>Let's use Pandas, (the name stands for <code>Python Data Analysis</code>) a tool very useful for shaping data, to analyze and prepare this pumpkin data.</p>"},{"location":"2-Regression/2-Data/#first-check-for-missing-dates","title":"First, check for missing dates","text":"<p>You will first need to take steps to check for missing dates:</p> <ol> <li>Convert the dates to a month format (these are US dates, so the format is <code>MM/DD/YYYY</code>).</li> <li>Extract the month to a new column.</li> </ol> <p>Open the notebook.ipynb file in Visual Studio Code and import the spreadsheet in to a new Pandas dataframe.</p> <ol> <li> <p>Use the <code>head()</code> function to view the first five rows.</p> <pre><code>import pandas as pd\npumpkins = pd.read_csv('../data/US-pumpkins.csv')\npumpkins.head()\n</code></pre> <p>\u2705 What function would you use to view the last five rows?</p> </li> <li> <p>Check if there is missing data in the current dataframe:</p> <pre><code>pumpkins.isnull().sum()\n</code></pre> <p>There is missing data, but maybe it won't matter for the task at hand.</p> </li> <li> <p>To make your dataframe easier to work with, select only the columns you need, using the <code>loc</code> function which extracts from the original dataframe a group of rows (passed as first parameter) and columns (passed as second parameter). The expression <code>:</code> in the case below means \"all rows\".</p> <pre><code>columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']\npumpkins = pumpkins.loc[:, columns_to_select]\n</code></pre> </li> </ol>"},{"location":"2-Regression/2-Data/#second-determine-average-price-of-pumpkin","title":"Second, determine average price of pumpkin","text":"<p>Think about how to determine the average price of a pumpkin in a given month. What columns would you pick for this task? Hint: you'll need 3 columns.</p> <p>Solution: take the average of the <code>Low Price</code> and <code>High Price</code> columns to populate the new Price column, and convert the Date column to only show the month. Fortunately, according to the check above, there is no missing data for dates or prices.</p> <ol> <li> <p>To calculate the average, add the following code:</p> <pre><code>price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2\n\nmonth = pd.DatetimeIndex(pumpkins['Date']).month\n</code></pre> </li> </ol> <p>\u2705 Feel free to print any data you'd like to check using <code>print(month)</code>.</p> <ol> <li> <p>Now, copy your converted data into a fresh Pandas dataframe:</p> <pre><code>new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})\n</code></pre> <p>Printing out your dataframe will show you a clean, tidy dataset on which you can build your new regression model.</p> </li> </ol>"},{"location":"2-Regression/2-Data/#but-wait-theres-something-odd-here","title":"But wait! There's something odd here","text":"<p>If you look at the <code>Package</code> column, pumpkins are sold in many different configurations. Some are sold in '1 1/9 bushel' measures, and some in '1/2 bushel' measures, some per pumpkin, some per pound, and some in big boxes with varying widths.</p> <p>Pumpkins seem very hard to weigh consistently</p> <p>Digging into the original data, it's interesting that anything with <code>Unit of Sale</code> equalling 'EACH' or 'PER BIN' also have the <code>Package</code> type per inch, per bin, or 'each'. Pumpkins seem to be very hard to weigh consistently, so let's filter them by selecting only pumpkins with the string 'bushel' in their <code>Package</code> column.</p> <ol> <li> <p>Add a filter at the top of the file, under the initial .csv import:</p> <pre><code>pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]\n</code></pre> <p>If you print the data now, you can see that you are only getting the 415 or so rows of data containing pumpkins by the bushel.</p> </li> </ol>"},{"location":"2-Regression/2-Data/#but-wait-theres-one-more-thing-to-do","title":"But wait! There's one more thing to do","text":"<p>Did you notice that the bushel amount varies per row? You need to normalize the pricing so that you show the pricing per bushel, so do some math to standardize it.</p> <ol> <li> <p>Add these lines after the block creating the new_pumpkins dataframe:</p> <pre><code>new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)\n\nnew_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)\n</code></pre> </li> </ol> <p>\u2705 According to The Spruce Eats, a bushel's weight depends on the type of produce, as it's a volume measurement. \"A bushel of tomatoes, for example, is supposed to weigh 56 pounds... Leaves and greens take up more space with less weight, so a bushel of spinach is only 20 pounds.\" It's all pretty complicated! Let's not bother with making a bushel-to-pound conversion, and instead price by the bushel. All this study of bushels of pumpkins, however, goes to show how very important it is to understand the nature of your data!</p> <p>Now, you can analyze the pricing per unit based on their bushel measurement. If you print out the data one more time, you can see how it's standardized.</p> <p>\u2705 Did you notice that pumpkins sold by the half-bushel are very expensive? Can you figure out why? Hint: little pumpkins are way pricier than big ones, probably because there are so many more of them per bushel, given the unused space taken by one big hollow pie pumpkin.</p>"},{"location":"2-Regression/2-Data/#visualization-strategies","title":"Visualization Strategies","text":"<p>Part of the data scientist's role is to demonstrate the quality and nature of the data they are working with. To do this, they often create interesting visualizations, or plots, graphs, and charts, showing different aspects of data. In this way, they are able to visually show relationships and gaps that are otherwise hard to uncover.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video working through visualizing the data for this lesson.</p> <p>Visualizations can also help determine the machine learning technique most appropriate for the data. A scatterplot that seems to follow a line, for example, indicates that the data is a good candidate for a linear regression exercise.</p> <p>One data visualization library that works well in Jupyter notebooks is Matplotlib (which you also saw in the previous lesson).</p> <p>Get more experience with data visualization in these tutorials.</p>"},{"location":"2-Regression/2-Data/#exercise-experiment-with-matplotlib","title":"Exercise - experiment with Matplotlib","text":"<p>Try to create some basic plots to display the new dataframe you just created. What would a basic line plot show?</p> <ol> <li> <p>Import Matplotlib at the top of the file, under the Pandas import:</p> <pre><code>import matplotlib.pyplot as plt\n</code></pre> </li> <li> <p>Rerun the entire notebook to refresh.</p> </li> <li> <p>At the bottom of the notebook, add a cell to plot the data as a box:</p> <pre><code>price = new_pumpkins.Price\nmonth = new_pumpkins.Month\nplt.scatter(price, month)\nplt.show()\n</code></pre> <p></p> <p>Is this a useful plot? Does anything about it surprise you?</p> <p>It's not particularly useful as all it does is display in your data as a spread of points in a given month.</p> </li> </ol>"},{"location":"2-Regression/2-Data/#make-it-useful","title":"Make it useful","text":"<p>To get charts to display useful data, you usually need to group the data somehow. Let's try creating a plot where the y axis shows the months and the data demonstrates the distribution of data.</p> <ol> <li> <p>Add a cell to create a grouped bar chart:</p> <pre><code>new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')\nplt.ylabel(\"Pumpkin Price\")\n</code></pre> <p></p> <p>This is a more useful data visualization! It seems to indicate that the highest price for pumpkins occurs in September and October. Does that meet your expectation? Why or why not?</p> </li> </ol>"},{"location":"2-Regression/2-Data/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Explore the different types of visualization that Matplotlib offers. Which types are most appropriate for regression problems?</p>"},{"location":"2-Regression/2-Data/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"2-Regression/2-Data/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take a look at the many ways to visualize data. Make a list of the various libraries available and note which are best for given types of tasks, for example 2D visualizations vs. 3D visualizations. What do you discover?</p>"},{"location":"2-Regression/2-Data/#assignment","title":"Assignment","text":"<p>Exploring visualization</p>"},{"location":"2-Regression/2-Data/README.zh-cn/","title":"\u4f7f\u7528 Scikit-learn \u6784\u5efa\u56de\u5f52\u6a21\u578b\uff1a\u51c6\u5907\u548c\u53ef\u89c6\u5316\u6570\u636e","text":"<p>\u4f5c\u8005 Dasani Madipalli</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b","text":""},{"location":"2-Regression/2-Data/README.zh-cn/#_2","title":"\u4ecb\u7ecd","text":"<p>\u65e2\u7136\u4f60\u5df2\u7ecf\u8bbe\u7f6e\u4e86\u5f00\u59cb\u4f7f\u7528 Scikit-learn \u5904\u7406\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6784\u5efa\u6240\u9700\u7684\u5de5\u5177\uff0c\u4f60\u5c31\u53ef\u4ee5\u5f00\u59cb\u5bf9\u6570\u636e\u63d0\u51fa\u95ee\u9898\u4e86\u3002\u5f53\u4f60\u5904\u7406\u6570\u636e\u5e76\u5e94\u7528ML\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u4e86\u89e3\u5982\u4f55\u63d0\u51fa\u6b63\u786e\u7684\u95ee\u9898\u4ee5\u6b63\u786e\u91ca\u653e\u6570\u636e\u96c6\u7684\u6f5c\u529b\u975e\u5e38\u91cd\u8981\u3002</p> <p>\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\uff1a</p> <ul> <li>\u5982\u4f55\u4e3a\u6a21\u578b\u6784\u5efa\u51c6\u5907\u6570\u636e\u3002</li> <li>\u5982\u4f55\u4f7f\u7528 Matplotlib \u8fdb\u884c\u6570\u636e\u53ef\u89c6\u5316\u3002</li> </ul>"},{"location":"2-Regression/2-Data/README.zh-cn/#_3","title":"\u5bf9\u4f60\u7684\u6570\u636e\u63d0\u51fa\u6b63\u786e\u7684\u95ee\u9898","text":"<p>\u4f60\u63d0\u51fa\u7684\u95ee\u9898\u5c06\u51b3\u5b9a\u4f60\u5c06\u4f7f\u7528\u54ea\u79cd\u7c7b\u578b\u7684 ML \u7b97\u6cd5\u3002\u4f60\u5f97\u5230\u7684\u7b54\u6848\u7684\u8d28\u91cf\u5c06\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4f60\u7684\u6570\u636e\u7684\u6027\u8d28\u3002</p> <p>\u67e5\u770b\u4e3a\u672c\u8bfe\u7a0b\u63d0\u4f9b\u7684\u6570\u636e\u3002\u4f60\u53ef\u4ee5\u5728 VS Code \u4e2d\u6253\u5f00\u8fd9\u4e2a .csv \u6587\u4ef6\u3002\u5feb\u901f\u6d4f\u89c8\u4e00\u4e0b\u5c31\u4f1a\u53d1\u73b0\u6709\u7a7a\u683c\uff0c\u8fd8\u6709\u5b57\u7b26\u4e32\u548c\u6570\u5b57\u6570\u636e\u7684\u6df7\u5408\u3002\u8fd8\u6709\u4e00\u4e2a\u5947\u602a\u7684\u5217\u53eb\u505a\u201cPackage\u201d\uff0c\u5176\u4e2d\u7684\u6570\u636e\u662f\u201csacks\u201d\u3001\u201cbins\u201d\u548c\u5176\u4ed6\u503c\u7684\u6df7\u5408\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6570\u636e\u6709\u70b9\u4e71\u3002</p> <p>\u4e8b\u5b9e\u4e0a\uff0c\u5f97\u5230\u4e00\u4e2a\u5b8c\u5168\u51c6\u5907\u597d\u7528\u4e8e\u521b\u5efa ML \u6a21\u578b\u7684\u5f00\u7bb1\u5373\u7528\u6570\u636e\u96c6\u5e76\u4e0d\u662f\u5f88\u5e38\u89c1\u3002\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u6807\u51c6 Python \u5e93\u51c6\u5907\u539f\u59cb\u6570\u636e\u96c6\u3002\u4f60\u8fd8\u5c06\u5b66\u4e60\u5404\u79cd\u6280\u672f\u6765\u53ef\u89c6\u5316\u6570\u636e\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_4","title":"\u6848\u4f8b\u7814\u7a76\uff1a\u201c\u5357\u74dc\u5e02\u573a\u201d","text":"<p>\u4f60\u5c06\u5728 <code>data</code> \u6587\u4ef6\u5939\u4e2d\u627e\u5230\u4e00\u4e2a\u540d\u4e3a US-pumpkins.csv \u7684 .csv \u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u6709\u5173\u5357\u74dc\u5e02\u573a\u7684 1757 \u884c\u6570\u636e\uff0c\u5df2\u6309\u57ce\u5e02\u6392\u5e8f\u5206\u7ec4\u3002\u8fd9\u662f\u4ece\u7f8e\u56fd\u519c\u4e1a\u90e8\u5206\u53d1\u7684\u7279\u79cd\u4f5c\u7269\u7ec8\u7aef\u5e02\u573a\u6807\u51c6\u62a5\u544a\u4e2d\u63d0\u53d6\u7684\u539f\u59cb\u6570\u636e\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_5","title":"\u51c6\u5907\u6570\u636e","text":"<p>\u8fd9\u4e9b\u6570\u636e\u5c5e\u4e8e\u516c\u5171\u9886\u57df\u3002\u5b83\u53ef\u4ee5\u4ece\u7f8e\u56fd\u519c\u4e1a\u90e8\u7f51\u7ad9\u4e0b\u8f7d\uff0c\u6bcf\u4e2a\u57ce\u5e02\u6709\u8bb8\u591a\u4e0d\u540c\u7684\u6587\u4ef6\u3002\u4e3a\u4e86\u907f\u514d\u592a\u591a\u5355\u72ec\u7684\u6587\u4ef6\uff0c\u6211\u4eec\u5c06\u6240\u6709\u57ce\u5e02\u6570\u636e\u5408\u5e76\u5230\u4e00\u4e2a\u7535\u5b50\u8868\u683c\u4e2d\uff0c\u56e0\u6b64\u6211\u4eec\u5df2\u7ecf\u51c6\u5907\u4e86\u4e00\u4e9b\u6570\u636e\u3002\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u4ed4\u7ec6\u770b\u770b\u6570\u636e\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#-","title":"\u5357\u74dc\u6570\u636e - \u65e9\u671f\u7ed3\u8bba","text":"<p>\u4f60\u5bf9\u8fd9\u4e9b\u6570\u636e\u6709\u4ec0\u4e48\u770b\u6cd5\uff1f\u4f60\u5df2\u7ecf\u770b\u5230\u4e86\u65e0\u6cd5\u7406\u89e3\u7684\u5b57\u7b26\u4e32\u3001\u6570\u5b57\u3001\u7a7a\u683c\u548c\u5947\u602a\u503c\u7684\u6df7\u5408\u4f53\u3002</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u56de\u5f52\u6280\u672f\u5bf9\u8fd9\u4e9b\u6570\u636e\u63d0\u51fa\u4ec0\u4e48\u95ee\u9898\uff1f\u201c\u9884\u6d4b\u7ed9\u5b9a\u6708\u4efd\u5185\u5f85\u552e\u5357\u74dc\u7684\u4ef7\u683c\u201d\u600e\u4e48\u6837\uff1f\u518d\u6b21\u67e5\u770b\u6570\u636e\uff0c\u4f60\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u66f4\u6539\u624d\u80fd\u521b\u5efa\u4efb\u52a1\u6240\u9700\u7684\u6570\u636e\u7ed3\u6784\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u5206\u6790\u5357\u74dc\u6570\u636e","text":"<p>\u8ba9\u6211\u4eec\u4f7f\u7528 Pandas\uff0c\uff08\u201cPython \u6570\u636e\u5206\u6790\u201d Python Data Analysis \u7684\u610f\u601d\uff09\u4e00\u4e2a\u975e\u5e38\u6709\u7528\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u51c6\u5907\u5357\u74dc\u6570\u636e\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_6","title":"\u9996\u5148\uff0c\u68c0\u67e5\u9057\u6f0f\u7684\u65e5\u671f","text":"<p>\u4f60\u9996\u5148\u9700\u8981\u91c7\u53d6\u4ee5\u4e0b\u6b65\u9aa4\u6765\u68c0\u67e5\u7f3a\u5c11\u7684\u65e5\u671f\uff1a</p> <ol> <li> <p>\u5c06\u65e5\u671f\u8f6c\u6362\u4e3a\u6708\u4efd\u683c\u5f0f\uff08\u8fd9\u4e9b\u662f\u7f8e\u56fd\u65e5\u671f\uff0c\u56e0\u6b64\u683c\u5f0f\u4e3a <code>MM/DD/YYYY</code>\uff09\u3002</p> </li> <li> <p>\u5c06\u6708\u4efd\u63d0\u53d6\u5230\u65b0\u5217\u3002</p> </li> </ol> <p>\u5728 Visual Studio Code \u4e2d\u6253\u5f00 notebook.ipynb \u6587\u4ef6\uff0c\u5e76\u5c06\u7535\u5b50\u8868\u683c\u5bfc\u5165\u5230\u65b0\u7684 Pandas dataframe \u4e2d\u3002</p> <ol> <li> <p>\u4f7f\u7528 <code>head()</code> \u51fd\u6570\u67e5\u770b\u524d\u4e94\u884c\u3002</p> <pre><code>import pandas as pd\npumpkins = pd.read_csv('../../data/US-pumpkins.csv')\npumpkins.head()\n</code></pre> <p>\u2705 \u4f7f\u7528\u4ec0\u4e48\u51fd\u6570\u6765\u67e5\u770b\u6700\u540e\u4e94\u884c\uff1f</p> </li> <li> <p>\u68c0\u67e5\u5f53\u524d dataframe \u4e2d\u662f\u5426\u7f3a\u5c11\u6570\u636e\uff1a</p> <pre><code>pumpkins.isnull().sum()\n</code></pre> <p>\u6709\u6570\u636e\u4e22\u5931\uff0c\u4f46\u53ef\u80fd\u5bf9\u624b\u5934\u7684\u4efb\u52a1\u6765\u8bf4\u65e0\u5173\u7d27\u8981\u3002</p> </li> <li> <p>\u4e3a\u4e86\u8ba9\u4f60\u7684 dataframe \u66f4\u5bb9\u6613\u4f7f\u7528\uff0c\u4f7f\u7528 <code>drop()</code> \u5220\u9664\u5b83\u7684\u51e0\u4e2a\u5217\uff0c\u53ea\u4fdd\u7559\u4f60\u9700\u8981\u7684\u5217\uff1a</p> <pre><code>new_columns = ['Package', 'Month', 'Low Price', 'High Price', 'Date']\npumpkins = pumpkins.drop([c for c in pumpkins.columns if c not in new_columns], axis=1)\n</code></pre> </li> </ol>"},{"location":"2-Regression/2-Data/README.zh-cn/#_7","title":"\u7136\u540e\uff0c\u786e\u5b9a\u5357\u74dc\u7684\u5e73\u5747\u4ef7\u683c","text":"<p>\u8003\u8651\u5982\u4f55\u786e\u5b9a\u7ed9\u5b9a\u6708\u4efd\u5357\u74dc\u7684\u5e73\u5747\u4ef7\u683c\u3002\u4f60\u4f1a\u4e3a\u6b64\u4efb\u52a1\u9009\u62e9\u54ea\u4e9b\u5217\uff1f\u63d0\u793a\uff1a\u4f60\u9700\u8981 3 \u5217\u3002</p> <p>\u89e3\u51b3\u65b9\u6848\uff1a\u53d6 <code>Low Price</code> \u548c <code>High Price</code> \u5217\u7684\u5e73\u5747\u503c\u6765\u586b\u5145\u65b0\u7684 Price \u5217\uff0c\u5c06 Date \u5217\u8f6c\u6362\u6210\u53ea\u663e\u793a\u6708\u4efd\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u6839\u636e\u4e0a\u9762\u7684\u68c0\u67e5\uff0c\u6ca1\u6709\u4e22\u5931\u65e5\u671f\u6216\u4ef7\u683c\u7684\u6570\u636e\u3002</p> <ol> <li> <p>\u8981\u8ba1\u7b97\u5e73\u5747\u503c\uff0c\u8bf7\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code>price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2\n\nmonth = pd.DatetimeIndex(pumpkins['Date']).month\n</code></pre> </li> </ol> <p>\u2705 \u8bf7\u968f\u610f\u4f7f\u7528 <code>print(month)</code> \u6253\u5370\u4f60\u60f3\u68c0\u67e5\u7684\u4efb\u4f55\u6570\u636e\u3002</p> <ol> <li> <p>\u73b0\u5728\uff0c\u5c06\u8f6c\u6362\u540e\u7684\u6570\u636e\u590d\u5236\u5230\u65b0\u7684 Pandas dataframe \u4e2d\uff1a</p> <pre><code>new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})\n</code></pre> <p>\u6253\u5370\u51fa\u7684 dataframe \u5c06\u5411\u4f60\u5c55\u793a\u4e00\u4e2a\u5e72\u51c0\u6574\u6d01\u7684\u6570\u636e\u96c6\uff0c\u4f60\u53ef\u4ee5\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u65b0\u7684\u56de\u5f52\u6a21\u578b\u3002</p> </li> </ol>"},{"location":"2-Regression/2-Data/README.zh-cn/#_8","title":"\u4f46\u662f\u7b49\u7b49\uff01\u8fd9\u91cc\u6709\u70b9\u5947\u602a","text":"<p>\u5982\u679c\u4f60\u770b\u770b <code>Package</code>(\u5305\u88c5)\u4e00\u680f\uff0c\u5357\u74dc\u6709\u5f88\u591a\u4e0d\u540c\u7684\u914d\u7f6e\u3002\u6709\u7684\u4ee5 1 1/9 \u84b2\u5f0f\u8033\u7684\u5c3a\u5bf8\u51fa\u552e\uff0c\u6709\u7684\u4ee5 1/2 \u84b2\u5f0f\u8033\u7684\u5c3a\u5bf8\u51fa\u552e\uff0c\u6709\u7684\u4ee5\u6bcf\u53ea\u5357\u74dc\u51fa\u552e\uff0c\u6709\u7684\u4ee5\u6bcf\u78c5\u51fa\u552e\uff0c\u6709\u7684\u4ee5\u4e0d\u540c\u5bbd\u5ea6\u7684\u5927\u76d2\u5b50\u51fa\u552e\u3002</p> <p>\u5357\u74dc\u4f3c\u4e4e\u5f88\u96be\u7edf\u4e00\u79f0\u91cd\u65b9\u5f0f</p> <p>\u6df1\u5165\u7814\u7a76\u539f\u59cb\u6570\u636e\uff0c\u6709\u8da3\u7684\u662f\uff0c\u4efb\u4f55 <code>Unit of Sale</code> \u7b49\u4e8e\u201cEACH\u201d\u6216\u201cPER BIN\u201d\u7684\u4e1c\u897f\u4e5f\u5177\u6709\u6bcf\u82f1\u5bf8\u3001\u6bcf\u7bb1\u6216\u201c\u6bcf\u4e2a\u201d\u7684 <code>Package</code> \u7c7b\u578b\u3002\u5357\u74dc\u4f3c\u4e4e\u5f88\u96be\u91c7\u7528\u7edf\u4e00\u79f0\u91cd\u65b9\u5f0f\uff0c\u56e0\u6b64\u8ba9\u6211\u4eec\u901a\u8fc7\u4ec5\u9009\u62e9 <code>Package</code> \u5217\u4e2d\u5e26\u6709\u5b57\u7b26\u4e32\u201c\u84b2\u5f0f\u8033\u201d\u7684\u5357\u74dc\u6765\u8fc7\u6ee4\u5b83\u4eec\u3002</p> <ol> <li> <p>\u5728\u521d\u59cb .csv \u5bfc\u5165\u4e0b\u6dfb\u52a0\u8fc7\u6ee4\u5668\uff1a</p> <pre><code>pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]\n</code></pre> <p>\u5982\u679c\u4f60\u73b0\u5728\u6253\u5370\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u4f60\u53ea\u83b7\u5f97\u4e86 415 \u884c\u5de6\u53f3\u5305\u542b\u6309\u84b2\u5f0f\u8033\u8ba1\u7b97\u7684\u5357\u74dc\u7684\u6570\u636e\u3002</p> </li> </ol>"},{"location":"2-Regression/2-Data/README.zh-cn/#_9","title":"\u53ef\u662f\u7b49\u7b49\uff01 \u8fd8\u6709\u4e00\u4ef6\u4e8b\u8981\u505a","text":"<p>\u4f60\u662f\u5426\u6ce8\u610f\u5230\u6bcf\u884c\u7684\u84b2\u5f0f\u8033\u6570\u91cf\u4e0d\u540c\uff1f\u4f60\u9700\u8981\u5bf9\u5b9a\u4ef7\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u4ee5\u4fbf\u663e\u793a\u6bcf\u84b2\u5f0f\u8033\u7684\u5b9a\u4ef7\uff0c\u56e0\u6b64\u8bf7\u8fdb\u884c\u4e00\u4e9b\u6570\u5b66\u8ba1\u7b97\u4ee5\u5bf9\u5176\u8fdb\u884c\u6807\u51c6\u5316\u3002</p> <ol> <li> <p>\u5728\u521b\u5efa new_pumpkins dataframe \u7684\u4ee3\u7801\u5757\u4e4b\u540e\u6dfb\u52a0\u8fd9\u4e9b\u884c\uff1a</p> <pre><code>new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)\n\nnew_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)\n</code></pre> </li> </ol> <p>\u2705 \u6839\u636e The Spruce Eats\uff0c\u84b2\u5f0f\u8033\u7684\u91cd\u91cf\u53d6\u51b3\u4e8e\u4ea7\u54c1\u7684\u7c7b\u578b\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u79cd\u4f53\u79ef\u6d4b\u91cf\u3002\u201c\u4f8b\u5982\uff0c\u4e00\u84b2\u5f0f\u8033\u897f\u7ea2\u67ff\u5e94\u8be5\u91cd56 \u78c5\u2026\u2026\u53f6\u5b50\u548c\u852c\u83dc\u5360\u636e\u66f4\u591a\u7a7a\u95f4\uff0c\u91cd\u91cf\u66f4\u8f7b\uff0c\u6240\u4ee5\u4e00\u84b2\u5f0f\u8033\u83e0\u83dc\u53ea\u670920\u78c5\u3002\u201d \u8fd9\u4e00\u5207\u90fd\u76f8\u5f53\u590d\u6742\uff01\u8ba9\u6211\u4eec\u4e0d\u8981\u8d39\u5fc3\u8fdb\u884c\u84b2\u5f0f\u8033\u5230\u78c5\u7684\u8f6c\u6362\uff0c\u800c\u662f\u6309\u84b2\u5f0f\u8033\u5b9a\u4ef7\u3002\u7136\u800c\uff0c\u6240\u6709\u8fd9\u4e9b\u5bf9\u84b2\u5f0f\u8033\u5357\u74dc\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e86\u89e3\u6570\u636e\u7684\u6027\u8d28\u662f\u591a\u4e48\u91cd\u8981\uff01</p> <p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u6839\u636e\u84b2\u5f0f\u8033\u6d4b\u91cf\u6765\u5206\u6790\u6bcf\u5355\u4f4d\u7684\u5b9a\u4ef7\u3002\u5982\u679c\u4f60\u518d\u6253\u5370\u4e00\u6b21\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u5b83\u662f\u5982\u4f55\u6807\u51c6\u5316\u7684\u3002</p> <p>\u2705 \u4f60\u6709\u6ca1\u6709\u6ce8\u610f\u5230\u534a\u84b2\u5f0f\u8033\u5356\u7684\u5357\u74dc\u5f88\u8d35\uff1f\u4f60\u80fd\u5f04\u6e05\u695a\u4e3a\u4ec0\u4e48\u5417\uff1f\u63d0\u793a\uff1a\u5c0f\u5357\u74dc\u6bd4\u5927\u5357\u74dc\u8d35\u5f97\u591a\uff0c\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u8003\u8651\u5230\u4e00\u4e2a\u5927\u7684\u7a7a\u5fc3\u9985\u997c\u5357\u74dc\u5360\u7528\u7684\u672a\u4f7f\u7528\u7a7a\u95f4\uff0c\u6bcf\u84b2\u5f0f\u8033\u7684\u5357\u74dc\u8981\u591a\u5f97\u591a\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_10","title":"\u53ef\u89c6\u5316\u7b56\u7565","text":"<p>\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u90e8\u5206\u804c\u8d23\u662f\u5c55\u793a\u4ed6\u4eec\u4f7f\u7528\u7684\u6570\u636e\u7684\u8d28\u91cf\u548c\u6027\u8d28\u3002\u4e3a\u6b64\uff0c\u4ed6\u4eec\u901a\u5e38\u4f1a\u521b\u5efa\u6709\u8da3\u7684\u53ef\u89c6\u5316\u6216\u7ed8\u56fe\u3001\u56fe\u5f62\u548c\u56fe\u8868\uff0c\u4ee5\u663e\u793a\u6570\u636e\u7684\u4e0d\u540c\u65b9\u9762\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u4ed6\u4eec\u80fd\u591f\u76f4\u89c2\u5730\u5c55\u793a\u96be\u4ee5\u53d1\u73b0\u7684\u5173\u7cfb\u548c\u5dee\u8ddd\u3002</p> <p>\u53ef\u89c6\u5316\u8fd8\u53ef\u4ee5\u5e2e\u52a9\u786e\u5b9a\u6700\u9002\u5408\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002\u4f8b\u5982\uff0c\u4f3c\u4e4e\u6cbf\u7740\u4e00\u6761\u7ebf\u7684\u6563\u70b9\u56fe\u8868\u660e\u8be5\u6570\u636e\u662f\u7ebf\u6027\u56de\u5f52\u7ec3\u4e60\u7684\u826f\u597d\u5019\u9009\u8005\u3002</p> <p>\u4e00\u4e2a\u5728 Jupyter notebooks \u4e2d\u8fd0\u884c\u826f\u597d\u7684\u6570\u636e\u53ef\u89c6\u5316\u5e93\u662f Matplotlib\uff08\u4f60\u5728\u4e0a\u4e00\u8bfe\u4e2d\u4e5f\u770b\u5230\u8fc7\uff09\u3002</p> <p>\u5728\u8fd9\u4e9b\u6559\u7a0b\u4e2d\u83b7\u5f97\u66f4\u591a\u6570\u636e\u53ef\u89c6\u5316\u7ecf\u9a8c\u3002</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#-matplotlib","title":"\u7ec3\u4e60 - \u4f7f\u7528 Matplotlib \u8fdb\u884c\u5b9e\u9a8c","text":"<p>\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e9b\u57fa\u672c\u56fe\u5f62\u6765\u663e\u793a\u4f60\u521a\u521a\u521b\u5efa\u7684\u65b0 dataframe\u3002\u57fa\u672c\u7ebf\u56fe\u4f1a\u663e\u793a\u4ec0\u4e48\uff1f</p> <ol> <li> <p>\u5728\u6587\u4ef6\u9876\u90e8\u5bfc\u5165 Matplotlib\uff1a</p> <pre><code>import matplotlib.pyplot as plt\n</code></pre> </li> <li> <p>\u91cd\u65b0\u5237\u65b0\u4ee5\u8fd0\u884c\u6574\u4e2a notebook\u3002</p> </li> <li> <p>\u5728 notebook \u5e95\u90e8\uff0c\u6dfb\u52a0\u4e00\u4e2a\u5355\u5143\u683c\u4ee5\u7ed8\u5236\u6570\u636e\uff1a</p> <pre><code>price = new_pumpkins.Price\nmonth = new_pumpkins.Month\nplt.scatter(price, month)\nplt.show()\n</code></pre> <p></p> <p>\u8fd9\u662f\u4e00\u4e2a\u6709\u7528\u7684\u56fe\u5417\uff1f\u6709\u4ec0\u4e48\u8ba9\u4f60\u5403\u60ca\u7684\u5417\uff1f</p> <p>\u5b83\u5e76\u4e0d\u662f\u7279\u522b\u6709\u7528\uff0c\u56e0\u4e3a\u5b83\u6240\u505a\u7684\u53ea\u662f\u5728\u4f60\u7684\u6570\u636e\u4e2d\u663e\u793a\u4e3a\u7ed9\u5b9a\u6708\u4efd\u7684\u70b9\u6570\u5206\u5e03\u3002</p> </li> </ol>"},{"location":"2-Regression/2-Data/README.zh-cn/#_11","title":"\u8ba9\u5b83\u6709\u7528","text":"<p>\u4e3a\u4e86\u8ba9\u56fe\u8868\u663e\u793a\u6709\u7528\u7684\u6570\u636e\uff0c\u4f60\u901a\u5e38\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7ec4\u3002\u8ba9\u6211\u4eec\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u56fe\uff0c\u5176\u4e2d y \u8f74\u663e\u793a\u6708\u4efd\uff0c\u6570\u636e\u663e\u793a\u6570\u636e\u7684\u5206\u5e03\u3002</p> <ol> <li> <p>\u6dfb\u52a0\u5355\u5143\u683c\u4ee5\u521b\u5efa\u5206\u7ec4\u67f1\u72b6\u56fe\uff1a</p> <pre><code>new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')\nplt.ylabel(\"Pumpkin Price\")\n</code></pre> <p></p> <p>\u8fd9\u662f\u4e00\u4e2a\u66f4\u6709\u7528\u7684\u6570\u636e\u53ef\u89c6\u5316\uff01\u4f3c\u4e4e\u8868\u660e\u5357\u74dc\u7684\u6700\u9ad8\u4ef7\u683c\u51fa\u73b0\u5728 9 \u6708\u548c 10 \u6708\u3002\u8fd9\u7b26\u5408\u4f60\u7684\u671f\u671b\u5417\uff1f\u4e3a\u4ec0\u4e48\uff1f\u4e3a\u4ec0\u4e48\u4e0d\uff1f</p> </li> </ol>"},{"location":"2-Regression/2-Data/README.zh-cn/#_12","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u63a2\u7d22 Matplotlib \u63d0\u4f9b\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u53ef\u89c6\u5316\u3002\u54ea\u79cd\u7c7b\u578b\u6700\u9002\u5408\u56de\u5f52\u95ee\u9898\uff1f</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_13","title":"\u8bfe\u540e\u6d4b","text":""},{"location":"2-Regression/2-Data/README.zh-cn/#_14","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u8bf7\u770b\u4e00\u4e0b\u53ef\u89c6\u5316\u6570\u636e\u7684\u591a\u79cd\u65b9\u6cd5\u3002\u5217\u51fa\u5404\u79cd\u53ef\u7528\u7684\u5e93\uff0c\u5e76\u6ce8\u610f\u54ea\u4e9b\u5e93\u6700\u9002\u5408\u7ed9\u5b9a\u7c7b\u578b\u7684\u4efb\u52a1\uff0c\u4f8b\u5982 2D \u53ef\u89c6\u5316\u4e0e 3D \u53ef\u89c6\u5316\u3002\u4f60\u53d1\u73b0\u4e86\u4ec0\u4e48\uff1f</p>"},{"location":"2-Regression/2-Data/README.zh-cn/#_15","title":"\u4efb\u52a1","text":"<p>\u63a2\u7d22\u53ef\u89c6\u5316</p>"},{"location":"2-Regression/2-Data/assignment/","title":"Exploring Visualizations","text":"<p>There are several different libraries that are available for data visualization. Create some visualizations using the Pumpkin data in this lesson with matplotlib and seaborn in a sample notebook. Which libraries are easier to work with?</p>"},{"location":"2-Regression/2-Data/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is submitted with two explorations/visualizations A notebook is submitted with one explorations/visualizations A notebook is not submitted"},{"location":"2-Regression/2-Data/assignment.zh-cn/","title":"\u63a2\u7d22\u6570\u636e\u53ef\u89c6\u5316","text":"<p>\u6709\u597d\u51e0\u4e2a\u5e93\u90fd\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u53ef\u89c6\u5316\u3002\u7528 matplotlib \u548c seaborn \u5bf9\u672c\u8bfe\u4e2d\u6d89\u53ca\u7684 Pumpkin \u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e9b\u6570\u636e\u53ef\u89c6\u5316\u7684\u56fe\u6807\u3002\u5e76\u601d\u8003\u54ea\u4e2a\u5e93\u66f4\u5bb9\u6613\u4f7f\u7528\uff1f</p>"},{"location":"2-Regression/2-Data/assignment.zh-cn/#_2","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u542b\u6709\u4e24\u79cd\u63a2\u7d22\u53ef\u89c6\u5316\u65b9\u6cd5\u7684 notebook \u5de5\u7a0b\u6587\u4ef6 \u63d0\u4ea4\u4e86\u53ea\u5305\u542b\u6709\u4e00\u79cd\u63a2\u7d22\u53ef\u89c6\u5316\u65b9\u6cd5\u7684 notebook \u5de5\u7a0b\u6587\u4ef6 \u6ca1\u63d0\u4ea4  notebook \u5de5\u7a0b\u6587\u4ef6"},{"location":"2-Regression/2-Data/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"2-Regression/3-Linear/","title":"Build a regression model using Scikit-learn: regression four ways","text":"<p>Infographic by Dasani Madipalli</p>"},{"location":"2-Regression/3-Linear/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"2-Regression/3-Linear/#this-lesson-is-available-in-r","title":"This lesson is available in R!","text":""},{"location":"2-Regression/3-Linear/#introduction","title":"Introduction","text":"<p>So far you have explored what regression is with sample data gathered from the pumpkin pricing dataset that we will use throughout this lesson. You have also visualized it using Matplotlib.</p> <p>Now you are ready to dive deeper into regression for ML. While visualization allows you to make sense of data, the real power of Machine Learning comes from training models. Models are trained on historic data to automatically capture data dependencies, and they allow you to predict outcomes for new data, which the model has not seem before.</p> <p>In this lesson, you will learn more about two types of regression: basic linear regression and polynomial regression, along with some of the math underlying these techniques. Those models will allow us to predict pumpkin prices depending on different input data. </p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video overview of linear regression.</p> <p>Throughout this curriculum, we assume minimal knowledge of math, and seek to make it accessible for students coming from other fields, so watch for notes, \ud83e\uddee callouts, diagrams, and other learning tools to aid in comprehension.</p>"},{"location":"2-Regression/3-Linear/#prerequisite","title":"Prerequisite","text":"<p>You should be familiar by now with the structure of the pumpkin data that we are examining. You can find it preloaded and pre-cleaned in this lesson's notebook.ipynb file. In the file, the pumpkin price is displayed per bushel in a new data frame.  Make sure you can run these notebooks in kernels in Visual Studio Code.</p>"},{"location":"2-Regression/3-Linear/#preparation","title":"Preparation","text":"<p>As a reminder, you are loading this data so as to ask questions of it. </p> <ul> <li>When is the best time to buy pumpkins? </li> <li>What price can I expect of a case of miniature pumpkins?</li> <li>Should I buy them in half-bushel baskets or by the 1 1/9 bushel box? Let's keep digging into this data.</li> </ul> <p>In the previous lesson, you created a Pandas data frame and populated it with part of the original dataset, standardizing the pricing by the bushel. By doing that, however, you were only able to gather about 400 datapoints and only for the fall months. </p> <p>Take a look at the data that we preloaded in this lesson's accompanying notebook. The data is preloaded and an initial scatterplot is charted to show month data. Maybe we can get a little more detail about the nature of the data by cleaning it more.</p>"},{"location":"2-Regression/3-Linear/#a-linear-regression-line","title":"A linear regression line","text":"<p>As you learned in Lesson 1, the goal of a linear regression exercise is to be able to plot a line to:</p> <ul> <li>Show variable relationships. Show the relationship between variables</li> <li>Make predictions. Make accurate predictions on where a new datapoint would fall in relationship to that line. </li> </ul> <p>It is typical of Least-Squares Regression to draw this type of line. The term 'least-squares' means that all the datapoints surrounding the regression line are squared and then added up. Ideally, that final sum is as small as possible, because we want a low number of errors, or <code>least-squares</code>. </p> <p>We do so since we want to model a line that has the least cumulative distance from all of our data points. We also square the terms before adding them since we are concerned with its magnitude rather than its direction.</p> <p>\ud83e\uddee Show me the math </p> <p>This line, called the line of best fit can be expressed by an equation: </p> <pre><code>Y = a + bX\n</code></pre> <p><code>X</code> is the 'explanatory variable'. <code>Y</code> is the 'dependent variable'. The slope of the line is <code>b</code> and <code>a</code> is the y-intercept, which refers to the value of <code>Y</code> when <code>X = 0</code>. </p> <p></p> <p>First, calculate the slope <code>b</code>. Infographic by Jen Looper</p> <p>In other words, and referring to our pumpkin data's original question: \"predict the price of a pumpkin per bushel by month\", <code>X</code> would refer to the price and <code>Y</code> would refer to the month of sale. </p> <p></p> <p>Calculate the value of Y. If you're paying around $4, it must be April! Infographic by Jen Looper</p> <p>The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where <code>Y</code> is situated when <code>X = 0</code>.</p> <p>You can observe the method of calculation for these values on the Math is Fun web site. Also visit this Least-squares calculator to watch how the numbers' values impact the line.</p>"},{"location":"2-Regression/3-Linear/#correlation","title":"Correlation","text":"<p>One more term to understand is the Correlation Coefficient between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.</p> <p>A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.</p> <p>\u2705 Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of <code>Month</code>, eg. day of the year (i.e. number of days since the beginning of the year)?</p> <p>In the code below, we will assume that we have cleaned up the data, and obtained a data frame called <code>new_pumpkins</code>, similar to the following:</p> ID Month DayOfYear Variety City Package Low Price High Price Price 70 9 267 PIE TYPE BALTIMORE 1 1/9 bushel cartons 15.0 15.0 13.636364 71 9 267 PIE TYPE BALTIMORE 1 1/9 bushel cartons 18.0 18.0 16.363636 72 10 274 PIE TYPE BALTIMORE 1 1/9 bushel cartons 18.0 18.0 16.363636 73 10 274 PIE TYPE BALTIMORE 1 1/9 bushel cartons 17.0 17.0 15.454545 74 10 281 PIE TYPE BALTIMORE 1 1/9 bushel cartons 15.0 15.0 13.636364 <p>The code to clean the data is available in <code>notebook.ipynb</code>. We have performed the same cleaning steps as in the previous lesson, and have calculated <code>DayOfYear</code> column using the following expression: </p> <pre><code>day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)\n</code></pre> <p>Now that you have an understanding of the math behind linear regression, let's create a Regression model to see if we can predict which package of pumpkins will have the best pumpkin prices. Someone buying pumpkins for a holiday pumpkin patch might want this information to be able to optimize their purchases of pumpkin packages for the patch.</p>"},{"location":"2-Regression/3-Linear/#looking-for-correlation","title":"Looking for Correlation","text":"<p>\ud83c\udfa5 Click the image above for a short video overview of correlation.</p> <p>From the previous lesson you have probably seen that the average price for different months looks like this:</p> <p></p> <p>This suggests that there should be some correlation, and we can try training linear regression model to predict the relationship between <code>Month</code> and <code>Price</code>, or between <code>DayOfYear</code> and <code>Price</code>. Here is the scatter plot that shows the latter relationship:</p> <p> </p> <p>Let's see if there is a correlation using the <code>corr</code> function:</p> <pre><code>print(new_pumpkins['Month'].corr(new_pumpkins['Price']))\nprint(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))\n</code></pre> <p>It looks like the correlation is pretty small, -0.15 by <code>Month</code> and -0.17 by the <code>DayOfMonth</code>, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an <code>ax</code> parameter to the <code>scatter</code> plotting function we can plot all points on the same graph:</p> <pre><code>ax=None\ncolors = ['red','blue','green','yellow']\nfor i,var in enumerate(new_pumpkins['Variety'].unique()):\n    df = new_pumpkins[new_pumpkins['Variety']==var]\n    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)\n</code></pre> <p> </p> <p>Our investigation suggests that variety has more effect on the overall price than the actual selling date. We can see this with a bar graph:</p> <pre><code>new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')\n</code></pre> <p> </p> <p>Let us focus for the moment only on one pumpkin variety, the 'pie type', and see what effect the date has on the price:</p> <p><pre><code>pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']\npie_pumpkins.plot.scatter('DayOfYear','Price') \n</code></pre> </p> <p>If we now calculate the correlation between <code>Price</code> and <code>DayOfYear</code> using <code>corr</code> function, we will get something like <code>-0.27</code> - which means that training a predictive model makes sense.</p> <p>Before training a linear regression model, it is important to make sure that our data is clean. Linear regression does not work well with missing values, thus it makes sense to get rid of all empty cells:</p> <pre><code>pie_pumpkins.dropna(inplace=True)\npie_pumpkins.info()\n</code></pre> <p>Another approach would be to fill those empty values with mean values from the corresponding column.</p>"},{"location":"2-Regression/3-Linear/#simple-linear-regression","title":"Simple Linear Regression","text":"<p>\ud83c\udfa5 Click the image above for a short video overview of linear and polynomial regression.</p> <p>To train our Linear Regression model, we will use the Scikit-learn library.</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n</code></pre> <p>We start by separating input values (features) and the expected output (label) into separate numpy arrays:</p> <pre><code>X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)\ny = pie_pumpkins['Price']\n</code></pre> <p>Note that we had to perform <code>reshape</code> on the input data in order for the Linear Regression package to understand it correctly. Linear Regression expects a 2D-array as an input, where each row of the array corresponds to a vector of input features. In our case, since we have only one input - we need an array with shape N\u00d71, where N is the dataset size.</p> <p>Then, we need to split the data into train and test datasets, so that we can validate our model after training:</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n</code></pre> <p>Finally, training the actual Linear Regression model takes only two lines of code. We define the <code>LinearRegression</code> object, and fit it to our data using the <code>fit</code> method:</p> <pre><code>lin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n</code></pre> <p>The <code>LinearRegression</code> object after <code>fit</code>-ting contains all the coefficients of the regression, which can be accessed using <code>.coef_</code> property. In our case, there is just one coefficient, which should be around <code>-0.017</code>. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using <code>lin_reg.intercept_</code> - it will be around <code>21</code> in our case, indicating the price at the beginning of the year.</p> <p>To see how accurate our model is, we can predict prices on a test dataset, and then measure how close our predictions are to the expected values. This can be done using mean square error (MSE) metrics, which is the mean of all squared differences between expected and predicted value.</p> <pre><code>pred = lin_reg.predict(X_test)\n\nmse = np.sqrt(mean_squared_error(y_test,pred))\nprint(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')\n</code></pre> <p>Our error seems to be around 2 points, which is ~17%. Not too good. Another indicator of model quality is the coefficient of determination, which can be obtained like this:</p> <p><pre><code>score = lin_reg.score(X_train,y_train)\nprint('Model determination: ', score)\n</code></pre> If the value is 0, it means that the model does not take input data into account, and acts as the worst linear predictor, which is simply a mean value of the result. The value of 1 means that we can perfectly predict all expected outputs. In our case, the coefficient is around 0.06, which is quite low.</p> <p>We can also plot the test data together with the regression line to better see how regression works in our case:</p> <pre><code>plt.scatter(X_test,y_test)\nplt.plot(X_test,pred)\n</code></pre> <p></p>"},{"location":"2-Regression/3-Linear/#polynomial-regression","title":"Polynomial Regression","text":"<p>Another type of Linear Regression is Polynomial Regression. While sometimes there's a linear relationship between variables - the bigger the pumpkin in volume, the higher the price - sometimes these relationships can't be plotted as a plane or straight line. </p> <p>\u2705 Here are some more examples of data that could use Polynomial Regression</p> <p>Take another look at the relationship between Date and Price. Does this scatterplot seem like it should necessarily be analyzed by a straight line? Can't prices fluctuate? In this case, you can try polynomial regression.</p> <p>\u2705 Polynomials are mathematical expressions that might consist of one or more variables and coefficients</p> <p>Polynomial regression creates a curved line to better fit nonlinear data. In our case, if we include a squared <code>DayOfYear</code> variable into input data, we should be able to fit our data with a parabolic curve, which will have a minimum at a certain point within the year.</p> <p>Scikit-learn includes a helpful pipeline API to combine different steps of data processing together. A pipeline is a chain of estimators. In our case, we will create a pipeline that first adds polynomial features to our model, and then trains the regression:</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())\n\npipeline.fit(X_train,y_train)\n</code></pre> <p>Using <code>PolynomialFeatures(2)</code> means that we will include all second-degree polynomials from the input data. In our case it will just mean <code>DayOfYear</code><sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.</p> <p>Pipelines can be used in the same manner as the original <code>LinearRegression</code> object, i.e. we can <code>fit</code> the pipeline, and then use <code>predict</code> to get the prediction results. Here is the graph showing test data, and the approximation curve:</p> <p></p> <p>Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!</p> <p>You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? </p> <p>\ud83c\udf83 Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!</p>"},{"location":"2-Regression/3-Linear/#categorical-features","title":"Categorical Features","text":"<p>In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the <code>Variety</code> column is somewhat different from columns like <code>Month</code>, because it contains non-numeric values. Such columns are called categorical.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video overview of using categorical features.</p> <p>Here you can see how average price depends on variety:</p> <p></p> <p>To take variety into account, we first need to convert it to numeric form, or encode it. There are several way we can do it:</p> <ul> <li>Simple numeric encoding will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.</li> <li>One-hot encoding will replace the <code>Variety</code> column by 4 different columns, one for each variety. Each column will contain <code>1</code> if the corresponding row is of a given variety, and <code>0</code> otherwise. This means that there will be four coefficients in linear regression, one for each pumpkin variety, responsible for \"starting price\" (or rather \"additional price\") for that particular variety.</li> </ul> <p>The code below shows how we can one-hot encode a variety:</p> <pre><code>pd.get_dummies(new_pumpkins['Variety'])\n</code></pre> ID FAIRYTALE MINIATURE MIXED HEIRLOOM VARIETIES PIE TYPE 70 0 0 0 1 71 0 0 0 1 ... ... ... ... ... 1738 0 1 0 0 1739 0 1 0 0 1740 0 1 0 0 1741 0 1 0 0 1742 0 1 0 0 <p>To train linear regression using one-hot encoded variety as input, we just need to initialize <code>X</code> and <code>y</code> data correctly:</p> <pre><code>X = pd.get_dummies(new_pumpkins['Variety'])\ny = new_pumpkins['Price']\n</code></pre> <p>The rest of the code is the same as what we used above to train Linear Regression. If you try it, you will see that the mean squared error is about the same, but we get much higher coefficient of determination (~77%). To get even more accurate predictions, we can take more categorical features into account, as well as numeric features, such as <code>Month</code> or <code>DayOfYear</code>. To get one large array of features, we can use <code>join</code>:</p> <pre><code>X = pd.get_dummies(new_pumpkins['Variety']) \\\n        .join(new_pumpkins['Month']) \\\n        .join(pd.get_dummies(new_pumpkins['City'])) \\\n        .join(pd.get_dummies(new_pumpkins['Package']))\ny = new_pumpkins['Price']\n</code></pre> <p>Here we also take into account <code>City</code> and <code>Package</code> type, which gives us MSE 2.84 (10%), and determination 0.94!</p>"},{"location":"2-Regression/3-Linear/#putting-it-all-together","title":"Putting it all together","text":"<p>To make the best model, we can use combined (one-hot encoded categorical + numeric) data from the above example together with Polynomial Regression. Here is the complete code for your convenience:</p> <pre><code># set up training data\nX = pd.get_dummies(new_pumpkins['Variety']) \\\n        .join(new_pumpkins['Month']) \\\n        .join(pd.get_dummies(new_pumpkins['City'])) \\\n        .join(pd.get_dummies(new_pumpkins['Package']))\ny = new_pumpkins['Price']\n\n# make train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# setup and train the pipeline\npipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())\npipeline.fit(X_train,y_train)\n\n# predict results for test data\npred = pipeline.predict(X_test)\n\n# calculate MSE and determination\nmse = np.sqrt(mean_squared_error(y_test,pred))\nprint(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')\n\nscore = pipeline.score(X_train,y_train)\nprint('Model determination: ', score)\n</code></pre> <p>This should give us the best determination coefficient of almost 97%, and MSE=2.23 (~8% prediction error).</p> Model MSE Determination <code>DayOfYear</code> Linear 2.77 (17.2%) 0.07 <code>DayOfYear</code> Polynomial 2.73 (17.0%) 0.08 <code>Variety</code> Linear 5.24 (19.7%) 0.77 All features Linear 2.84 (10.5%) 0.94 All features Polynomial 2.23 (8.25%) 0.97 <p>\ud83c\udfc6 Well done! You created four Regression models in one lesson, and improved the model quality to 97%. In the final section on Regression, you will learn about Logistic Regression to determine categories. </p>"},{"location":"2-Regression/3-Linear/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Test several different variables in this notebook to see how correlation corresponds to model accuracy.</p>"},{"location":"2-Regression/3-Linear/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"2-Regression/3-Linear/#review-self-study","title":"Review &amp; Self Study","text":"<p>In this lesson we learned about Linear Regression. There are other important types of Regression. Read about Stepwise, Ridge, Lasso and Elasticnet techniques. A good course to study to learn more is the Stanford Statistical Learning course</p>"},{"location":"2-Regression/3-Linear/#assignment","title":"Assignment","text":"<p>Build a Model</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/","title":"\u4f7f\u7528 Scikit-learn \u6784\u5efa\u56de\u5f52\u6a21\u578b\uff1a\u4e24\u79cd\u65b9\u5f0f\u7684\u56de\u5f52","text":"<p>\u4f5c\u8005 Dasani Madipalli</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b","text":""},{"location":"2-Regression/3-Linear/README.zh-cn/#_2","title":"\u4ecb\u7ecd","text":"<p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u4f60\u5df2\u7ecf\u901a\u8fc7\u4ece\u6211\u4eec\u5c06\u5728\u672c\u8bfe\u7a0b\u4e2d\u4f7f\u7528\u7684\u5357\u74dc\u5b9a\u4ef7\u6570\u636e\u96c6\u6536\u96c6\u7684\u6837\u672c\u6570\u636e\u63a2\u7d22\u4e86\u4ec0\u4e48\u662f\u56de\u5f52\u3002\u4f60\u8fd8\u4f7f\u7528 Matplotlib \u5bf9\u5176\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u3002</p> <p>\u73b0\u5728\u4f60\u5df2\u51c6\u5907\u597d\u6df1\u5165\u7814\u7a76 ML \u7684\u56de\u5f52\u3002 \u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u8be6\u7ec6\u4e86\u89e3\u4e24\u79cd\u7c7b\u578b\u7684\u56de\u5f52\uff1a\u57fa\u672c\u7ebf\u6027\u56de\u5f52 \u548c \u591a\u9879\u5f0f\u56de\u5f52\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6280\u672f\u80cc\u540e\u7684\u4e00\u4e9b\u6570\u5b66\u77e5\u8bc6\u3002</p> <p>\u5728\u6574\u4e2a\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u6570\u5b66\u77e5\u8bc6\u6700\u5c11\uff0c\u5e76\u8bd5\u56fe\u8ba9\u6765\u81ea\u5176\u4ed6\u9886\u57df\u7684\u5b66\u751f\u4e5f\u80fd\u63a5\u89e6\u5230\u5b83\uff0c\u56e0\u6b64\u8bf7\u4f7f\u7528\u7b14\u8bb0\u3001\ud83e\uddee\u6807\u6ce8\u3001\u56fe\u8868\u548c\u5176\u4ed6\u5b66\u4e60\u5de5\u5177\u4ee5\u5e2e\u52a9\u7406\u89e3\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_3","title":"\u524d\u63d0","text":"<p>\u4f60\u73b0\u5728\u5e94\u8be5\u719f\u6089\u6211\u4eec\u6b63\u5728\u68c0\u67e5\u7684\u5357\u74dc\u6570\u636e\u7684\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u5728\u672c\u8bfe\u7684 notebook.ipynb \u6587\u4ef6\u4e2d\u627e\u5230\u5b83\u3002 \u5728\u8fd9\u4e2a\u6587\u4ef6\u4e2d\uff0c\u5357\u74dc\u7684\u4ef7\u683c\u663e\u793a\u5728\u4e00\u4e2a\u65b0\u7684 dataframe \u4e2d\u3002\u786e\u4fdd\u53ef\u4ee5\u5728 Visual Studio Code \u4ee3\u7801\u7684\u5185\u6838\u4e2d\u8fd0\u884c\u8fd9\u4e9b notebooks\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_4","title":"\u51c6\u5907","text":"<p>\u63d0\u9192\u4e00\u4e0b\uff0c\u4f60\u6b63\u5728\u52a0\u8f7d\u6b64\u6570\u636e\u4ee5\u63d0\u51fa\u95ee\u9898\u3002</p> <ul> <li>\u4ec0\u4e48\u65f6\u5019\u4e70\u5357\u74dc\u6700\u597d\uff1f</li> <li>\u4e00\u7bb1\u5fae\u578b\u5357\u74dc\u7684\u4ef7\u683c\u662f\u591a\u5c11\uff1f</li> <li>\u6211\u5e94\u8be5\u4e70\u534a\u84b2\u5f0f\u8033\u8fd8\u662f 1 1/9 \u84b2\u5f0f\u8033\uff1f</li> </ul> <p>\u8ba9\u6211\u4eec\u7ee7\u7eed\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u6570\u636e\u3002</p> <p>\u5728\u4e0a\u4e00\u8bfe\u4e2d\uff0c\u4f60\u521b\u5efa\u4e86\u4e00\u4e2a Pandas dataframe \u5e76\u7528\u539f\u59cb\u6570\u636e\u96c6\u7684\u4e00\u90e8\u5206\u586b\u5145\u5b83\uff0c\u6309\u84b2\u5f0f\u8033\u6807\u51c6\u5316\u5b9a\u4ef7\u3002\u4f46\u662f\uff0c\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u4f60\u53ea\u80fd\u6536\u96c6\u5927\u7ea6 400 \u4e2a\u6570\u636e\u70b9\uff0c\u800c\u4e14\u53ea\u80fd\u6536\u96c6\u79cb\u5b63\u6708\u4efd\u7684\u6570\u636e\u3002</p> <p>\u770b\u770b\u6211\u4eec\u5728\u672c\u8bfe\u968f\u9644\u7684 notebook \u4e2d\u9884\u52a0\u8f7d\u7684\u6570\u636e\u3002\u6570\u636e\u5df2\u9884\u52a0\u8f7d\uff0c\u5e76\u7ed8\u5236\u4e86\u521d\u59cb\u6563\u70b9\u56fe\u4ee5\u663e\u793a\u6708\u4efd\u6570\u636e\u3002\u4e5f\u8bb8\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u66f4\u591a\u5730\u6e05\u7406\u6570\u636e\u6765\u83b7\u5f97\u66f4\u591a\u5173\u4e8e\u6570\u636e\u6027\u8d28\u7684\u7ec6\u8282\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_5","title":"\u7ebf\u6027\u56de\u5f52\u7ebf","text":"<p>\u6b63\u5982\u4f60\u5728\u7b2c 1 \u8bfe\u4e2d\u5b66\u5230\u7684\uff0c\u7ebf\u6027\u56de\u5f52\u7ec3\u4e60\u7684\u76ee\u6807\u662f\u80fd\u591f\u7ed8\u5236\u4e00\u6761\u7ebf\u4ee5\u4fbf\uff1a</p> <ul> <li>\u663e\u793a\u53d8\u91cf\u5173\u7cfb\u3002 \u663e\u793a\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb</li> <li>\u4f5c\u51fa\u9884\u6d4b\u3002 \u51c6\u786e\u9884\u6d4b\u65b0\u6570\u636e\u70b9\u4e0e\u8be5\u7ebf\u7684\u5173\u7cfb\u3002</li> </ul> <p>\u7ed8\u5236\u8fd9\u79cd\u7c7b\u578b\u7684\u7ebf\u662f\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u7684\u5178\u578b\u505a\u6cd5\u3002\u672f\u8bed\u201c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u201d\u610f\u5473\u7740\u5c06\u56de\u5f52\u7ebf\u5468\u56f4\u7684\u6240\u6709\u6570\u636e\u70b9\u5e73\u65b9\uff0c\u7136\u540e\u76f8\u52a0\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u6700\u7ec8\u548c\u5c3d\u53ef\u80fd\u5c0f\uff0c\u56e0\u4e3a\u6211\u4eec\u5e0c\u671b\u9519\u8bef\u6570\u91cf\u8f83\u5c11\uff0c\u6216\u201c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u201d\u3002</p> <p>\u6211\u4eec\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u6211\u4eec\u60f3\u8981\u5bf9\u4e00\u6761\u4e0e\u6240\u6709\u6570\u636e\u70b9\u7684\u7d2f\u79ef\u8ddd\u79bb\u6700\u5c0f\u7684\u7ebf\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u8fd8\u5728\u6dfb\u52a0\u5b83\u4eec\u4e4b\u524d\u5bf9\u8fd9\u4e9b\u9879\u8fdb\u884c\u5e73\u65b9\uff0c\u56e0\u4e3a\u6211\u4eec\u5173\u5fc3\u7684\u662f\u5b83\u7684\u5927\u5c0f\u800c\u4e0d\u662f\u5b83\u7684\u65b9\u5411\u3002</p> <p>\ud83e\uddee \u6570\u5b66\u77e5\u8bc6</p> <p>\u8fd9\u6761\u7ebf\u79f0\u4e3a \u6700\u4f73\u62df\u5408\u7ebf\uff0c\u53ef\u4ee5\u7528\u4e00\u4e2a\u7b49\u5f0f\u8868\u793a\uff1a</p> <pre><code>Y = a + bX\n</code></pre> <p><code>X</code> \u662f\u201c\u89e3\u91ca\u53d8\u91cf\u201d\u3002<code>Y</code> \u662f\u201c\u56e0\u53d8\u91cf\u201d\u3002\u76f4\u7ebf\u7684\u659c\u7387\u662f <code>b</code>\uff0c<code>a</code> \u662f y \u8f74\u622a\u8ddd\uff0c\u6307\u7684\u662f <code>X = 0</code> \u65f6 <code>Y</code> \u7684\u503c\u3002</p> <p></p> <p>\u9996\u5148\uff0c\u8ba1\u7b97\u659c\u7387 <code>b</code>\u3002\u4f5c\u8005 Jen Looper</p> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c\u53c2\u8003\u6211\u4eec\u7684\u5357\u74dc\u6570\u636e\u7684\u539f\u59cb\u95ee\u9898\uff1a\u201c\u6309\u6708\u9884\u6d4b\u6bcf\u84b2\u5f0f\u8033\u5357\u74dc\u7684\u4ef7\u683c\u201d\uff0c<code>X</code> \u6307\u7684\u662f\u4ef7\u683c\uff0c<code>Y</code> \u6307\u7684\u662f\u9500\u552e\u6708\u4efd\u3002</p> <p></p> <p>\u8ba1\u7b97 Y \u7684\u503c\u3002\u5982\u679c\u4f60\u652f\u4ed8\u5927\u7ea6 4 \u7f8e\u5143\uff0c\u90a3\u4e00\u5b9a\u662f\u56db\u6708\uff01\u4f5c\u8005 Jen Looper</p> <p>\u8ba1\u7b97\u76f4\u7ebf\u7684\u6570\u5b66\u5fc5\u987b\u8bc1\u660e\u76f4\u7ebf\u7684\u659c\u7387\uff0c\u8fd9\u4e5f\u53d6\u51b3\u4e8e\u622a\u8ddd\uff0c\u6216\u8005\u5f53 <code>X = 0</code> \u65f6 <code>Y</code> \u6240\u5728\u7684\u4f4d\u7f6e\u3002</p> <p>\u4f60\u53ef\u4ee5\u5728 Math is Fun \u7f51\u7ad9\u4e0a\u89c2\u5bdf\u8fd9\u4e9b\u503c\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002\u53e6\u8bf7\u8bbf\u95ee\u8fd9\u4e2a\u6700\u5c0f\u4e8c\u4e58\u8ba1\u7b97\u5668\u4ee5\u89c2\u5bdf\u6570\u5b57\u7684\u503c\u5982\u4f55\u5f71\u54cd\u76f4\u7ebf\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_6","title":"\u76f8\u5173\u6027","text":"<p>\u53e6\u4e00\u4e2a\u9700\u8981\u7406\u89e3\u7684\u672f\u8bed\u662f\u7ed9\u5b9a X \u548c Y \u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\u3002\u4f7f\u7528\u6563\u70b9\u56fe\uff0c\u4f60\u53ef\u4ee5\u5feb\u901f\u53ef\u89c6\u5316\u8be5\u7cfb\u6570\u3002\u6570\u636e\u70b9\u6563\u5e03\u5728\u4e00\u6761\u76f4\u7ebf\u4e0a\u7684\u56fe\u5177\u6709\u9ad8\u76f8\u5173\u6027\uff0c\u4f46\u6570\u636e\u70b9\u6563\u5e03\u5728 X \u548c Y \u4e4b\u95f4\u7684\u56fe\u5177\u6709\u4f4e\u76f8\u5173\u6027\u3002</p> <p>\u4e00\u4e2a\u597d\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5c06\u662f\u4e00\u4e2a\u7528\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u6cd5\u4e0e\u76f4\u7ebf\u56de\u5f52\u5f97\u5230\u7684\u9ad8\uff08\u66f4\u63a5\u8fd1\u4e8e 1\uff09\u76f8\u5173\u7cfb\u6570\u7684\u6a21\u578b\u3002</p> <p>\u2705 \u8fd0\u884c\u672c\u8bfe\u968f\u9644\u7684 notebook \u5e76\u67e5\u770b City to Price \u6563\u70b9\u56fe\u3002\u6839\u636e\u4f60\u5bf9\u6563\u70b9\u56fe\u7684\u89c6\u89c9\u89e3\u91ca\uff0c\u5c06\u5357\u74dc\u9500\u552e\u7684\u57ce\u5e02\u4e0e\u4ef7\u683c\u76f8\u5173\u8054\u7684\u6570\u636e\u4f3c\u4e4e\u5177\u6709\u9ad8\u76f8\u5173\u6027\u6216\u4f4e\u76f8\u5173\u6027\uff1f</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_7","title":"\u4e3a\u56de\u5f52\u51c6\u5907\u6570\u636e","text":"<p>\u73b0\u5728\u4f60\u5df2\u7ecf\u4e86\u89e3\u4e86\u672c\u7ec3\u4e60\u80cc\u540e\u7684\u6570\u5b66\u539f\u7406\uff0c\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u56de\u5f52\u6a21\u578b\uff0c\u770b\u770b\u4f60\u662f\u5426\u53ef\u4ee5\u9884\u6d4b\u54ea\u4e2a\u5357\u74dc\u5305\u88c5\u7684\u5357\u74dc\u4ef7\u683c\u6700\u4f18\u60e0\u3002\u4e3a\u8282\u65e5\u8d2d\u4e70\u5357\u74dc\u7684\u4eba\u53ef\u80fd\u5e0c\u671b\u6b64\u4fe1\u606f\u80fd\u591f\u4f18\u5316\u4ed6\u4eec\u5982\u4f55\u8d2d\u4e70\u5357\u74dc\u5305\u88c5\u3002</p> <p>\u7531\u4e8e\u4f60\u5c06\u4f7f\u7528 Scikit-learn\uff0c\u56e0\u6b64\u6ca1\u6709\u7406\u7531\u624b\u52a8\u6267\u884c\u6b64\u64cd\u4f5c\uff08\u5c3d\u7ba1\u4f60\u53ef\u4ee5\uff01\uff09\u3002\u5728\u8bfe\u7a0b notebook \u7684\u4e3b\u8981\u6570\u636e\u5904\u7406\u5757\u4e2d\uff0c\u4ece Scikit-learn \u6dfb\u52a0\u4e00\u4e2a\u5e93\u4ee5\u81ea\u52a8\u5c06\u6240\u6709\u5b57\u7b26\u4e32\u6570\u636e\u8f6c\u6362\u4e3a\u6570\u5b57\uff1a</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nnew_pumpkins.iloc[:, 0:-1] = new_pumpkins.iloc[:, 0:-1].apply(LabelEncoder().fit_transform)\n</code></pre> <p>\u5982\u679c\u4f60\u73b0\u5728\u67e5\u770b new_pumpkins dataframe\uff0c\u4f60\u4f1a\u770b\u5230\u6240\u6709\u5b57\u7b26\u4e32\u73b0\u5728\u90fd\u662f\u6570\u5b57\u3002\u8fd9\u8ba9\u4f60\u66f4\u96be\u9605\u8bfb\uff0c\u4f46\u5bf9 Scikit-learn \u6765\u8bf4\u66f4\u5bb9\u6613\u7406\u89e3\uff01</p> <p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u5bf9\u6700\u9002\u5408\u56de\u5f52\u7684\u6570\u636e\u505a\u51fa\u66f4\u6709\u6839\u636e\u7684\u51b3\u7b56\uff08\u4e0d\u4ec5\u4ec5\u662f\u57fa\u4e8e\u89c2\u5bdf\u6563\u70b9\u56fe\uff09\u3002</p> <p>\u5c1d\u8bd5\u5728\u6570\u636e\u7684\u4e24\u70b9\u4e4b\u95f4\u627e\u5230\u826f\u597d\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u6784\u5efa\u826f\u597d\u7684\u9884\u6d4b\u6a21\u578b\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u57ce\u5e02\u548c\u4ef7\u683c\u4e4b\u95f4\u53ea\u6709\u5fae\u5f31\u7684\u76f8\u5173\u6027\uff1a</p> <pre><code>print(new_pumpkins['City'].corr(new_pumpkins['Price']))\n0.32363971816089226\n</code></pre> <p>\u7136\u800c\uff0c\u5305\u88c5\u548c\u5b83\u7684\u4ef7\u683c\u4e4b\u95f4\u6709\u66f4\u597d\u7684\u76f8\u5173\u6027\u3002\u8fd9\u662f\u6709\u9053\u7406\u7684\uff0c\u5bf9\u5427\uff1f\u901a\u5e38\uff0c\u519c\u4ea7\u54c1\u7bb1\u8d8a\u5927\uff0c\u4ef7\u683c\u8d8a\u9ad8\u3002</p> <pre><code>print(new_pumpkins['Package'].corr(new_pumpkins['Price']))\n0.6061712937226021\n</code></pre> <p>\u5bf9\u8fd9\u4e9b\u6570\u636e\u63d0\u51fa\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u95ee\u9898\u662f\uff1a\u201c\u6211\u53ef\u4ee5\u671f\u671b\u7ed9\u5b9a\u7684\u5357\u74dc\u5305\u88c5\u7684\u4ef7\u683c\u662f\u591a\u5c11\uff1f\u201d</p> <p>\u8ba9\u6211\u4eec\u5efa\u7acb\u8fd9\u4e2a\u56de\u5f52\u6a21\u578b</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_8","title":"\u5efa\u7acb\u7ebf\u6027\u6a21\u578b","text":"<p>\u5728\u6784\u5efa\u6a21\u578b\u4e4b\u524d\uff0c\u518d\u5bf9\u6570\u636e\u8fdb\u884c\u4e00\u6b21\u6574\u7406\u3002\u5220\u9664\u4efb\u4f55\u7a7a\u6570\u636e\u5e76\u518d\u6b21\u68c0\u67e5\u6570\u636e\u7684\u6837\u5b50\u3002</p> <pre><code>new_pumpkins.dropna(inplace=True)\nnew_pumpkins.info()\n</code></pre> <p>\u7136\u540e\uff0c\u4ece\u8fd9\u4e2a\u6700\u5c0f\u96c6\u5408\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 dataframe \u5e76\u5c06\u5176\u6253\u5370\u51fa\u6765\uff1a</p> <pre><code>new_columns = ['Package', 'Price']\nlin_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')\n\nlin_pumpkins\n</code></pre> <pre><code>    Package Price\n70  0   13.636364\n71  0   16.363636\n72  0   16.363636\n73  0   15.454545\n74  0   13.636364\n... ... ...\n1738    2   30.000000\n1739    2   28.750000\n1740    2   25.750000\n1741    2   24.000000\n1742    2   24.000000\n415 rows \u00d7 2 columns\n</code></pre> <ol> <li>\u73b0\u5728\u4f60\u53ef\u4ee5\u5206\u914d X \u548c y \u5750\u6807\u6570\u636e\uff1a</li> </ol> <pre><code>X = lin_pumpkins.values[:, :1]\ny = lin_pumpkins.values[:, 1:2]\n</code></pre> <p>\u2705 \u8fd9\u91cc\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f\u4f60\u6b63\u5728\u4f7f\u7528 Python slice notation \u6765\u521b\u5efa\u6570\u7ec4\u6765\u586b\u5145 <code>X</code> \u548c <code>y</code>\u3002</p> <ol> <li>\u63a5\u4e0b\u6765\uff0c\u5f00\u59cb\u56de\u5f52\u6a21\u578b\u6784\u5efa\u4f8b\u7a0b\uff1a</li> </ol> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n\npred = lin_reg.predict(X_test)\n\naccuracy_score = lin_reg.score(X_train,y_train)\nprint('Model Accuracy: ', accuracy_score)\n</code></pre> <p>\u56e0\u4e3a\u76f8\u5173\u6027\u4e0d\u662f\u7279\u522b\u597d\uff0c\u6240\u4ee5\u751f\u6210\u7684\u6a21\u578b\u4e0d\u662f\u975e\u5e38\u51c6\u786e\u3002</p> <pre><code>Model Accuracy:  0.3315342327998987\n</code></pre> <ol> <li>\u4f60\u53ef\u4ee5\u5c06\u8fc7\u7a0b\u4e2d\u7ed8\u5236\u7684\u7ebf\u6761\u53ef\u89c6\u5316\uff1a</li> </ol> <pre><code>plt.scatter(X_test, y_test,  color='black')\nplt.plot(X_test, pred, color='blue', linewidth=3)\n\nplt.xlabel('Package')\nplt.ylabel('Price')\n\nplt.show()\n</code></pre> <p></p> <ol> <li>\u9488\u5bf9\u5047\u8bbe\u7684\u54c1\u79cd\u6d4b\u8bd5\u6a21\u578b\uff1a</li> </ol> <pre><code>lin_reg.predict( np.array([ [2.75] ]) )\n</code></pre> <p>\u8fd9\u4e2a\u795e\u8bdd\u822c\u7684\u54c1\u79cd\u7684\u4ef7\u683c\u662f\uff1a</p> <pre><code>array([[33.15655975]])\n</code></pre> <p>\u5982\u679c\u56de\u5f52\u7ebf\u7684\u903b\u8f91\u6210\u7acb\uff0c\u8fd9\u4e2a\u6570\u5b57\u662f\u6709\u610f\u4e49\u7684\u3002</p> <p>\ud83c\udf83 \u606d\u559c\u4f60\uff0c\u4f60\u521a\u521a\u521b\u5efa\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u53ef\u4ee5\u5e2e\u52a9\u9884\u6d4b\u51e0\u4e2a\u5357\u74dc\u54c1\u79cd\u7684\u4ef7\u683c\u3002\u4f60\u7684\u8282\u65e5\u5357\u74dc\u5730\u4f1a\u5f88\u6f02\u4eae\u7684\u3002\u4f46\u662f\u4f60\u53ef\u4ee5\u521b\u9020\u4e00\u4e2a\u66f4\u597d\u7684\u6a21\u578b\uff01</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_9","title":"\u591a\u9879\u5f0f\u56de\u5f52","text":"<p>\u53e6\u4e00\u79cd\u7ebf\u6027\u56de\u5f52\u662f\u591a\u9879\u5f0f\u56de\u5f52\u3002\u867d\u7136\u6709\u65f6\u53d8\u91cf\u4e4b\u95f4\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\u2014\u2014\u5357\u74dc\u7684\u4f53\u79ef\u8d8a\u5927\uff0c\u4ef7\u683c\u5c31\u8d8a\u9ad8\u2014\u2014\u4f46\u6709\u65f6\u8fd9\u4e9b\u5173\u7cfb\u4e0d\u80fd\u7ed8\u5236\u6210\u5e73\u9762\u6216\u76f4\u7ebf\u3002</p> <p>\u2705 \u8fd9\u91cc\u6709\u53ef\u4ee5\u4f7f\u7528\u591a\u9879\u5f0f\u56de\u5f52\u6570\u636e\u7684\u66f4\u591a\u793a\u4f8b</p> <p>\u518d\u770b\u4e00\u4e0b\u4e0a\u56fe\u4e2d\u54c1\u79cd\u4e0e\u4ef7\u683c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u8fd9\u4e2a\u6563\u70b9\u56fe\u770b\u8d77\u6765\u662f\u5426\u5e94\u8be5\u7528\u4e00\u6761\u76f4\u7ebf\u6765\u5206\u6790\uff1f\u4e5f\u8bb8\u4e0d\u662f\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u591a\u9879\u5f0f\u56de\u5f52\u3002</p> <p>\u2705 \u591a\u9879\u5f0f\u662f\u53ef\u80fd\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u53d8\u91cf\u548c\u7cfb\u6570\u7ec4\u6210\u7684\u6570\u5b66\u8868\u8fbe\u5f0f</p> <p>\u591a\u9879\u5f0f\u56de\u5f52\u521b\u5efa\u4e00\u6761\u66f2\u7ebf\u4ee5\u66f4\u597d\u5730\u62df\u5408\u975e\u7ebf\u6027\u6570\u636e\u3002</p> <ol> <li>\u8ba9\u6211\u4eec\u91cd\u65b0\u521b\u5efa\u4e00\u4e2a\u586b\u5145\u4e86\u539f\u59cb\u5357\u74dc\u6570\u636e\u7247\u6bb5\u7684 dataframe\uff1a</li> </ol> <pre><code>new_columns = ['Variety', 'Package', 'City', 'Month', 'Price']\npoly_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')\n\npoly_pumpkins\n</code></pre> <p>\u53ef\u89c6\u5316 dataframe \u4e2d\u6570\u636e\u4e4b\u95f4\u76f8\u5173\u6027\u7684\u4e00\u79cd\u597d\u65b9\u6cd5\u662f\u5c06\u5176\u663e\u793a\u5728\u201ccoolwarm\u201d\u56fe\u8868\u4e2d\uff1a</p> <ol> <li>\u4f7f\u7528 <code>Background_gradient()</code> \u65b9\u6cd5\u548c <code>coolwarm</code> \u4f5c\u4e3a\u5176\u53c2\u6570\u503c\uff1a</li> </ol> <pre><code>corr = poly_pumpkins.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n</code></pre> <p>\u8fd9\u6bb5\u4ee3\u7801\u521b\u5efa\u4e86\u4e00\u4e2a\u70ed\u56fe:    </p> <p>\u67e5\u770b\u6b64\u56fe\u8868\uff0c\u4f60\u53ef\u4ee5\u76f4\u89c2\u5730\u770b\u5230 Package \u548c Price \u4e4b\u95f4\u7684\u826f\u597d\u76f8\u5173\u6027\u3002\u6240\u4ee5\u4f60\u5e94\u8be5\u80fd\u591f\u521b\u5efa\u4e00\u4e2a\u6bd4\u4e0a\u4e00\u4e2a\u66f4\u597d\u7684\u6a21\u578b\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_10","title":"\u521b\u5efa\u7ba1\u9053","text":"<p>Scikit-learn \u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u591a\u9879\u5f0f\u56de\u5f52\u6a21\u578b\u7684\u6709\u7528 API - <code>make_pipeline</code> API\u3002 \u521b\u5efa\u4e86\u4e00\u4e2a\u201c\u7ba1\u9053\u201d\uff0c\u5b83\u662f\u4e00\u4e2a\u4f30\u8ba1\u5668\u94fe\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7ba1\u9053\u5305\u62ec\u591a\u9879\u5f0f\u7279\u5f81\u6216\u5f62\u6210\u975e\u7ebf\u6027\u8def\u5f84\u7684\u9884\u6d4b\u3002</p> <ol> <li>\u6784\u5efa X \u548c y \u5217\uff1a</li> </ol> <pre><code>X=poly_pumpkins.iloc[:,3:4].values\ny=poly_pumpkins.iloc[:,4:5].values\n</code></pre> <ol> <li>\u901a\u8fc7\u8c03\u7528 <code>make_pipeline()</code> \u65b9\u6cd5\u521b\u5efa\u7ba1\u9053\uff1a</li> </ol> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(PolynomialFeatures(4), LinearRegression())\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\npipeline.fit(np.array(X_train), y_train)\n\ny_pred=pipeline.predict(X_test)\n</code></pre>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_11","title":"\u521b\u5efa\u5e8f\u5217","text":"<p>\u6b64\u65f6\uff0c\u4f60\u9700\u8981\u4f7f\u7528_\u6392\u5e8f\u597d\u7684_\u6570\u636e\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 dataframe \uff0c\u4ee5\u4fbf\u7ba1\u9053\u53ef\u4ee5\u521b\u5efa\u5e8f\u5217\u3002</p> <p>\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code>df = pd.DataFrame({'x': X_test[:,0], 'y': y_pred[:,0]})\ndf.sort_values(by='x',inplace = True)\npoints = pd.DataFrame(df).to_numpy()\n\nplt.plot(points[:, 0], points[:, 1],color=\"blue\", linewidth=3)\nplt.xlabel('Package')\nplt.ylabel('Price')\nplt.scatter(X,y, color=\"black\")\nplt.show()\n</code></pre> <p>\u4f60\u901a\u8fc7\u8c03\u7528 <code>pd.DataFrame</code> \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684 dataframe\u3002\u7136\u540e\u901a\u8fc7\u8c03\u7528 <code>sort_values()</code> \u5bf9\u503c\u8fdb\u884c\u6392\u5e8f\u3002\u6700\u540e\u4f60\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u56fe\uff1a</p> <p></p> <p>\u4f60\u53ef\u4ee5\u770b\u5230\u66f4\u9002\u5408\u4f60\u7684\u6570\u636e\u7684\u66f2\u7ebf\u3002</p> <p>\u8ba9\u6211\u4eec\u68c0\u67e5\u6a21\u578b\u7684\u51c6\u786e\u6027\uff1a</p> <pre><code>accuracy_score = pipeline.score(X_train,y_train)\nprint('Model Accuracy: ', accuracy_score)\n</code></pre> <p>\u77a7\uff01</p> <pre><code>Model Accuracy:  0.8537946517073784\n</code></pre> <p>\u8fd9\u6837\u597d\u591a\u4e86\uff01\u8bd5\u7740\u9884\u6d4b\u4e00\u4e2a\u4ef7\u683c\uff1a</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_12","title":"\u505a\u4e2a\u9884\u6d4b","text":"<p>\u6211\u4eec\u53ef\u4ee5\u8f93\u5165\u4e00\u4e2a\u65b0\u503c\u5e76\u5f97\u5230\u4e00\u4e2a\u9884\u6d4b\u5417\uff1f</p> <p>\u8c03\u7528 <code>predict()</code> \u8fdb\u884c\u9884\u6d4b\uff1a</p> <pre><code>pipeline.predict( np.array([ [2.75] ]) )\n</code></pre> <p>\u4f60\u4f1a\u5f97\u5230\u8fd9\u6837\u7684\u9884\u6d4b\uff1a</p> <pre><code>array([[46.34509342]])\n</code></pre> <p>\u53c2\u7167\u56fe\u50cf\uff0c\u8fd9\u786e\u5b9e\u6709\u9053\u7406\uff01\u800c\u4e14\uff0c\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u6bd4\u524d\u4e00\u4e2a\u66f4\u597d\u7684\u6a21\u578b\uff0c\u770b\u540c\u6837\u7684\u6570\u636e\uff0c\u4f60\u9700\u8981\u4e3a\u8fd9\u4e9b\u66f4\u6602\u8d35\u7684\u5357\u74dc\u505a\u597d\u9884\u7b97\uff01</p> <p>\ud83c\udfc6 \u5e72\u5f97\u4e0d\u9519\uff01\u4f60\u5728\u4e00\u8282\u8bfe\u4e2d\u521b\u5efa\u4e86\u4e24\u4e2a\u56de\u5f52\u6a21\u578b\u3002\u5728\u56de\u5f52\u7684\u6700\u540e\u4e00\u8282\u4e2d\uff0c\u4f60\u5c06\u4e86\u89e3\u903b\u8f91\u56de\u5f52\u4ee5\u786e\u5b9a\u7c7b\u522b\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_13","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u5728\u6b64 notebook \u4e2d\u6d4b\u8bd5\u51e0\u4e2a\u4e0d\u540c\u7684\u53d8\u91cf\uff0c\u4ee5\u67e5\u770b\u76f8\u5173\u6027\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u7684\u5bf9\u5e94\u5173\u7cfb\u3002</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_14","title":"\u8bfe\u540e\u6d4b","text":""},{"location":"2-Regression/3-Linear/README.zh-cn/#_15","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u7ebf\u6027\u56de\u5f52\u3002\u8fd8\u6709\u5176\u4ed6\u91cd\u8981\u7684\u56de\u5f52\u7c7b\u578b\u3002\u4e86\u89e3 Stepwise\u3001Ridge\u3001Lasso \u548c Elasticnet \u6280\u672f\u3002\u5b66\u4e60\u66f4\u591a\u4fe1\u606f\u7684\u597d\u8bfe\u7a0b\u662f \u65af\u5766\u798f\u7edf\u8ba1\u5b66\u4e60\u8bfe\u7a0b</p>"},{"location":"2-Regression/3-Linear/README.zh-cn/#_16","title":"\u4efb\u52a1","text":"<p>\u6784\u5efa\u6a21\u578b</p>"},{"location":"2-Regression/3-Linear/assignment/","title":"Create a Regression Model","text":""},{"location":"2-Regression/3-Linear/assignment/#instructions","title":"Instructions","text":"<p>In this lesson you were shown how to build a model using both Linear and Polynomial Regression. Using this knowledge, find a dataset or use one of Scikit-learn's built-in sets to build a fresh model. Explain in your notebook why you chose the technique you did, and demonstrate your model's accuracy. If it is not accurate, explain why.</p>"},{"location":"2-Regression/3-Linear/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement presents a complete notebook with a well-documented solution the solution is incomplete the solution is flawed or buggy"},{"location":"2-Regression/3-Linear/assignment.zh-cn/","title":"\u521b\u5efa\u81ea\u5df1\u7684\u56de\u5f52\u6a21\u578b","text":""},{"location":"2-Regression/3-Linear/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u8fd9\u8282\u8bfe\u4e2d\u4f60\u5b66\u5230\u4e86\u5982\u4f55\u7528\u7ebf\u6027\u56de\u5f52\u548c\u591a\u9879\u5f0f\u56de\u5f52\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u3002\u5229\u7528\u8fd9\u4e9b\u53ea\u662f\uff0c\u627e\u5230\u4e00\u4e2a\u4f60\u611f\u5174\u8da3\u7684\u6570\u636e\u96c6\u6216\u8005\u662f Scikit-learn \u5185\u7f6e\u7684\u6570\u636e\u96c6\u6765\u5efa\u7acb\u4e00\u4e2a\u5168\u65b0\u7684\u6a21\u578b\u3002\u7528\u4f60\u7684 notebook \u6765\u89e3\u91ca\u4e3a\u4ec0\u4e48\u7528\u4e86\u8fd9\u79cd\u6280\u672f\u6765\u5bf9\u8fd9\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4e14\u8bc1\u660e\u51fa\u4f60\u7684\u6a21\u578b\u7684\u51c6\u786e\u5ea6\u3002\u5982\u679c\u5b83\u6ca1\u4f60\u60f3\u8c61\u4e2d\u51c6\u786e\uff0c\u8bf7\u601d\u8003\u4e00\u4e0b\u5e76\u89e3\u91ca\u4e00\u4e0b\u539f\u56e0\u3002</p>"},{"location":"2-Regression/3-Linear/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684 notebook \u5de5\u7a0b\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u89e3\u96c6\uff0c\u5e76\u4e14\u53ef\u8bfb\u6027\u826f\u597d \u4e0d\u5b8c\u6574\u7684\u89e3\u96c6 \u89e3\u96c6\u662f\u6709\u7f3a\u9677\u6216\u8005\u6709\u9519\u8bef\u7684"},{"location":"2-Regression/3-Linear/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"2-Regression/4-Logistic/","title":"Logistic regression to predict categories","text":""},{"location":"2-Regression/4-Logistic/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"2-Regression/4-Logistic/#this-lesson-is-available-in-r","title":"This lesson is available in R!","text":""},{"location":"2-Regression/4-Logistic/#introduction","title":"Introduction","text":"<p>In this final lesson on Regression, one of the basic classic ML techniques, we will take a look at Logistic Regression. You would use this technique to discover patterns to predict binary categories. Is this candy chocolate or not? Is this disease contagious or not? Will this customer choose this product or not? </p> <p>In this lesson, you will learn:</p> <ul> <li>A new library for data visualization</li> <li>Techniques for logistic regression</li> </ul> <p>\u2705 Deepen your understanding of working with this type of regression in this Learn module</p>"},{"location":"2-Regression/4-Logistic/#prerequisite","title":"Prerequisite","text":"<p>Having worked with the pumpkin data, we are now familiar enough with it to realize that there's one binary category that we can work with: <code>Color</code>.</p> <p>Let's build a logistic regression model to predict that, given some variables, what color a given pumpkin is likely to be (orange \ud83c\udf83 or white \ud83d\udc7b).</p> <p>Why are we talking about binary classification in a lesson grouping about regression? Only for linguistic convenience, as logistic regression is really a classification method, albeit a linear-based one. Learn about other ways to classify data in the next lesson group.</p>"},{"location":"2-Regression/4-Logistic/#define-the-question","title":"Define the question","text":"<p>For our purposes, we will express this as a binary: 'White' or 'Not White'. There is also a 'striped' category in our dataset but there are few instances of it, so we will not use it. It disappears once we remove null values from the dataset, anyway.</p> <p>\ud83c\udf83 Fun fact, we sometimes call white pumpkins 'ghost' pumpkins. They aren't very easy to carve, so they aren't as popular as the orange ones but they are cool looking! So we could also reformulate our question as: 'Ghost' or 'Not Ghost'. \ud83d\udc7b</p>"},{"location":"2-Regression/4-Logistic/#about-logistic-regression","title":"About logistic regression","text":"<p>Logistic regression differs from linear regression, which you learned about previously, in a few important ways.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video overview of logistic regression.</p>"},{"location":"2-Regression/4-Logistic/#binary-classification","title":"Binary classification","text":"<p>Logistic regression does not offer the same features as linear regression. The former offers a prediction about a binary category (\"white or not white\") whereas the latter is capable of predicting continual values, for example given the origin of a pumpkin and the time of harvest, how much its price will rise.</p> <p></p> <p>Infographic by Dasani Madipalli</p>"},{"location":"2-Regression/4-Logistic/#other-classifications","title":"Other classifications","text":"<p>There are other types of logistic regression, including multinomial and ordinal:</p> <ul> <li>Multinomial, which involves having more than one category - \"Orange, White, and Striped\".</li> <li>Ordinal, which involves ordered categories, useful if we wanted to order our outcomes logically, like our pumpkins that are ordered by a finite number of sizes (mini,sm,med,lg,xl,xxl).</li> </ul> <p></p>"},{"location":"2-Regression/4-Logistic/#variables-do-not-have-to-correlate","title":"Variables DO NOT have to correlate","text":"<p>Remember how linear regression worked better with more correlated variables? Logistic regression is the opposite - the variables don't have to align. That works for this data which has somewhat weak correlations.</p>"},{"location":"2-Regression/4-Logistic/#you-need-a-lot-of-clean-data","title":"You need a lot of clean data","text":"<p>Logistic regression will give more accurate results if you use more data; our small dataset is not optimal for this task, so keep that in mind.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video overview of preparing data for linear regression</p> <p>\u2705 Think about the types of data that would lend themselves well to logistic regression</p>"},{"location":"2-Regression/4-Logistic/#exercise-tidy-the-data","title":"Exercise - tidy the data","text":"<p>First, clean the data a bit, dropping null values and selecting only some of the columns:</p> <ol> <li> <p>Add the following code:</p> <pre><code>columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']\npumpkins = full_pumpkins.loc[:, columns_to_select]\n\npumpkins.dropna(inplace=True)\n</code></pre> <p>You can always take a peek at your new dataframe:</p> <pre><code>pumpkins.info\n</code></pre> </li> </ol>"},{"location":"2-Regression/4-Logistic/#visualization-categorical-plot","title":"Visualization - categorical plot","text":"<p>By now you have loaded up the starter notebook with pumpkin data once again and cleaned it so as to preserve a dataset containing a few variables, including <code>Color</code>. Let's visualize the dataframe in the notebook using a different library: Seaborn, which is built on Matplotlib which we used earlier. </p> <p>Seaborn offers some neat ways to visualize your data. For example, you can compare distributions of the data for each <code>Variety</code> and <code>Color</code> in a categorical plot.</p> <ol> <li> <p>Create such a plot by using the <code>catplot</code> function, using our pumpkin data <code>pumpkins</code>, and specifying a color mapping for each pumpkin category (orange or white):</p> <pre><code>import seaborn as sns\n\npalette = {\n'ORANGE': 'orange',\n'WHITE': 'wheat',\n}\n\nsns.catplot(\ndata=pumpkins, y=\"Variety\", hue=\"Color\", kind=\"count\",\npalette=palette, \n)\n</code></pre> <p></p> <p>By observing the data, you can see how the Color data relates to Variety.</p> <p>\u2705 Given this categorical plot, what are some interesting explorations you can envision?</p> </li> </ol>"},{"location":"2-Regression/4-Logistic/#data-pre-processing-feature-and-label-encoding","title":"Data pre-processing: feature and label encoding","text":"<p>Our pumpkins dataset contains string values for all its columns. Working with categorical data is intuitive for humans but not for machines. Machine learning algorithms work well with numbers. That's why encoding is a very important step in the data pre-processing phase, since it enables us to turn categorical data into numerical data, without losing any information. Good encoding leads to building a good model.</p> <p>For feature encoding there are two main types of encoders:</p> <ol> <li> <p>Ordinal encoder: it suits well for ordinal variables, which are categorical variables where their data follows a logical ordering, like the <code>Item Size</code> column in our dataset. It creates a mapping such that each category is represented by a number, which is the order of the category in the column.</p> <pre><code>from sklearn.preprocessing import OrdinalEncoder\n\nitem_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]\nordinal_features = ['Item Size']\nordinal_encoder = OrdinalEncoder(categories=item_size_categories)\n</code></pre> </li> <li> <p>Categorical encoder: it suits well for nominal variables, which are categorical variables where their data does not follow a logical ordering, like all the features different from <code>Item Size</code> in our dataset. It is a one-hot encoding, which means that each category is represented by a binary column: the encoded variable is equal to 1 if the pumpkin belongs to that Variety and 0 otherwise.</p> <p><pre><code>from sklearn.preprocessing import OneHotEncoder\n\ncategorical_features = ['City Name', 'Package', 'Variety', 'Origin']\ncategorical_encoder = OneHotEncoder(sparse_output=False)\n</code></pre> Then, <code>ColumnTransformer</code> is used to combine multiple encoders into a single step and apply them to the appropriate columns.</p> </li> </ol> <p><pre><code>    from sklearn.compose import ColumnTransformer\n\n    ct = ColumnTransformer(transformers=[\n        ('ord', ordinal_encoder, ordinal_features),\n        ('cat', categorical_encoder, categorical_features)\n        ])\n\n    ct.set_output(transform='pandas')\n    encoded_features = ct.fit_transform(pumpkins)\n</code></pre> On the other hand, to encode the label, we use the scikit-learn <code>LabelEncoder</code> class, which is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1 (here, 0 and 1).</p> <p><pre><code>    from sklearn.preprocessing import LabelEncoder\n\n    label_encoder = LabelEncoder()\n    encoded_label = label_encoder.fit_transform(pumpkins['Color'])\n</code></pre> Once we have encoded the features and the label, we can merge them into a new dataframe <code>encoded_pumpkins</code>.</p> <p><pre><code>    encoded_pumpkins = encoded_features.assign(Color=encoded_label)\n</code></pre> \u2705 What are the advantages of using an ordinal encoder for the <code>Item Size</code> column?</p>"},{"location":"2-Regression/4-Logistic/#analyse-relationships-between-variables","title":"Analyse relationships between variables","text":"<p>Now that we have pre-processed our data, we can analyse the relationships between the features and the label to grasp an idea of how well the model will be able to predict the label given the features. The best way to perform this kind of analysis is plotting the data. We'll be using again the Seaborn <code>catplot</code> function, to visualize the relationships between <code>Item Size</code>,  <code>Variety</code> and <code>Color</code> in a categorical plot. To better plot the data we'll be using the encoded <code>Item Size</code> column and the unencoded <code>Variety</code> column.</p> <p><pre><code>    palette = {\n    'ORANGE': 'orange',\n    'WHITE': 'wheat',\n    }\n    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']\n\n    g = sns.catplot(\n        data=pumpkins,\n        x=\"Item Size\", y=\"Color\", row='Variety',\n        kind=\"box\", orient=\"h\",\n        sharex=False, margin_titles=True,\n        height=1.8, aspect=4, palette=palette,\n    )\n    g.set(xlabel=\"Item Size\", ylabel=\"\").set(xlim=(0,6))\n    g.set_titles(row_template=\"{row_name}\")\n</code></pre> </p>"},{"location":"2-Regression/4-Logistic/#use-a-swarm-plot","title":"Use a swarm plot","text":"<p>Since Color is a binary category (White or Not), it needs 'a specialized approach to visualization'. There are other ways to visualize the relationship of this category with other variables. </p> <p>You can visualize variables side-by-side with Seaborn plots.</p> <ol> <li> <p>Try a 'swarm' plot to show the distribution of values:</p> <pre><code>palette = {\n0: 'orange',\n1: 'wheat'\n}\nsns.swarmplot(x=\"Color\", y=\"ord__Item Size\", data=encoded_pumpkins, palette=palette)\n</code></pre> <p></p> </li> </ol> <p>Watch Out: the code above might generate a warning, since seaborn fails to represent such amount of datapoints into a swam plot. A possible solution is decreasing the size of the marker, by using the 'size' parameter. However, be aware that this affects the readability of the plot.</p> <p>\ud83e\uddee Show Me The Math</p> <p>Logistic regression relies on the concept of 'maximum likelihood' using sigmoid functions. A 'Sigmoid Function' on a plot looks like an 'S' shape. It takes a value and maps it to somewhere between 0 and 1. Its curve is also called a 'logistic curve'. Its formula looks like this:</p> <p></p> <p>where the sigmoid's midpoint finds itself at x's 0 point, L is the curve's maximum value, and k is the curve's steepness. If the outcome of the function is more than 0.5, the label in question will be given the class '1' of the binary choice. If not, it will be classified as '0'.</p>"},{"location":"2-Regression/4-Logistic/#build-your-model","title":"Build your model","text":"<p>Building a model to find these binary classification is surprisingly straightforward in Scikit-learn.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a short video overview of building a linear regression model</p> <ol> <li> <p>Select the variables you want to use in your classification model and split the training and test sets calling <code>train_test_split()</code>:</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]\ny = encoded_pumpkins['Color']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n</code></pre> </li> <li> <p>Now you can train your model, by calling <code>fit()</code> with your training data, and print out its result:</p> <pre><code>from sklearn.metrics import f1_score, classification_report \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nprint('Predicted labels: ', predictions)\nprint('F1-score: ', f1_score(y_test, predictions))\n</code></pre> <p>Take a look at your model's scoreboard. It's not bad, considering you have only about 1000 rows of data:</p> <pre><code>                   precision    recall  f1-score   support\n\n                0       0.94      0.98      0.96       166\n                1       0.85      0.67      0.75        33\n\n    accuracy                                0.92       199\n    macro avg           0.89      0.82      0.85       199\n    weighted avg        0.92      0.92      0.92       199\n\n    Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n    0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0\n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0\n    0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n    0 0 0 1 0 0 0 0 0 0 0 0 1 1]\n    F1-score:  0.7457627118644068\n</code></pre> </li> </ol>"},{"location":"2-Regression/4-Logistic/#better-comprehension-via-a-confusion-matrix","title":"Better comprehension via a confusion matrix","text":"<p>While you can get a scoreboard report terms by printing out the items above, you might be able to understand your model more easily by using a confusion matrix to help us understand how the model is performing.</p> <p>\ud83c\udf93 A 'confusion matrix' (or 'error matrix') is a table that expresses your model's true vs. false positives and negatives, thus gauging the accuracy of predictions.</p> <ol> <li> <p>To use a confusion metrics, call <code>confusion_matrix()</code>:</p> <pre><code>from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, predictions)\n</code></pre> <p>Take a look at your model's confusion matrix:</p> <pre><code>array([[162,   4],\n       [ 11,  22]])\n</code></pre> </li> </ol> <p>In Scikit-learn, confusion matrices Rows (axis 0) are actual labels and columns (axis 1) are predicted labels.</p> 0 1 0 TN FP 1 FN TP <p>What's going on here? Let's say our model is asked to classify pumpkins between two binary categories, category 'white' and category 'not-white'.</p> <ul> <li>If your model predicts a pumpkin as not white and it belongs to category 'not-white' in reality we call it a true negative, shown by the top left number.</li> <li>If your model predicts a pumpkin as white and it belongs to category 'not-white' in reality we call it a false negative, shown by the bottom left number. </li> <li>If your model predicts a pumpkin as not white and it belongs to category 'white' in reality we call it a false positive, shown by the top right number. </li> <li>If your model predicts a pumpkin as white and it belongs to category 'white' in reality we call it a true positive, shown by the bottom right number.</li> </ul> <p>As you might have guessed it's preferable to have a larger number of true positives and true negatives and a lower number of false positives and false negatives, which implies that the model performs better.</p> <p>How does the confusion matrix relate to precision and recall? Remember, the classification report printed above showed precision (0.85) and recall (0.67).</p> <p>Precision = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461</p> <p>Recall = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666</p> <p>\u2705 Q: According to the confusion matrix, how did the model do? A: Not bad; there are a good number of true negatives but also a few false negatives. </p> <p>Let's revisit the terms we saw earlier with the help of the confusion matrix's mapping of TP/TN and FP/FN:</p> <p>\ud83c\udf93 Precision: TP/(TP + FP) The fraction of relevant instances among the retrieved instances (e.g. which labels were well-labeled)</p> <p>\ud83c\udf93 Recall: TP/(TP + FN) The fraction of relevant instances that were retrieved, whether well-labeled or not</p> <p>\ud83c\udf93 f1-score: (2 * precision * recall)/(precision + recall) A weighted average of the precision and recall, with best being 1 and worst being 0</p> <p>\ud83c\udf93 Support: The number of occurrences of each label retrieved</p> <p>\ud83c\udf93 Accuracy: (TP + TN)/(TP + TN + FP + FN) The percentage of labels predicted accurately for a sample.</p> <p>\ud83c\udf93 Macro Avg: The calculation of the unweighted mean metrics for each label, not taking label imbalance into account.</p> <p>\ud83c\udf93 Weighted Avg: The calculation of the mean metrics for each label, taking label imbalance into account by weighting them by their support (the number of true instances for each label).</p> <p>\u2705 Can you think which metric you should watch if you want your model to reduce the number of false negatives?</p>"},{"location":"2-Regression/4-Logistic/#visualize-the-roc-curve-of-this-model","title":"Visualize the ROC curve of this model","text":"<p>\ud83c\udfa5 Click the image above for a short video overview of ROC curves</p> <p>Let's do one more visualization to see the so-called 'ROC' curve:</p> <pre><code>from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ny_scores = model.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\nfig = plt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n</code></pre> <p>Using Matplotlib, plot the model's Receiving Operating Characteristic or ROC. ROC curves are often used to get a view of the output of a classifier in terms of its true vs. false positives. \"ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis.\" Thus, the steepness of the curve and the space between the midpoint line and the curve matter: you want a curve that quickly heads up and over the line. In our case, there are false positives to start with, and then the line heads up and over properly:</p> <p></p> <p>Finally, use Scikit-learn's <code>roc_auc_score</code> API to compute the actual 'Area Under the Curve' (AUC):</p> <p><pre><code>auc = roc_auc_score(y_test,y_scores[:,1])\nprint(auc)\n</code></pre> The result is <code>0.9749908725812341</code>. Given that the AUC ranges from 0 to 1, you want a big score, since a model that is 100% correct in its predictions will have an AUC of 1; in this case, the model is pretty good. </p> <p>In future lessons on classifications, you will learn how to iterate to improve your model's scores. But for now, congratulations! You've completed these regression lessons!</p>"},{"location":"2-Regression/4-Logistic/#challenge","title":"\ud83d\ude80Challenge","text":"<p>There's a lot more to unpack regarding logistic regression! But the best way to learn is to experiment. Find a dataset that lends itself to this type of analysis and build a model with it. What do you learn? tip: try Kaggle for interesting datasets.</p>"},{"location":"2-Regression/4-Logistic/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"2-Regression/4-Logistic/#review-self-study","title":"Review &amp; Self Study","text":"<p>Read the first few pages of this paper from Stanford on some practical uses for logistic regression. Think about tasks that are better suited for one or the other type of regression tasks that we have studied up to this point. What would work best?</p>"},{"location":"2-Regression/4-Logistic/#assignment","title":"Assignment","text":"<p>Retrying this regression</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/","title":"\u903b\u8f91\u56de\u5f52\u9884\u6d4b\u5206\u7c7b","text":"<p>\u4f5c\u8005 Dasani Madipalli</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b","text":""},{"location":"2-Regression/4-Logistic/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u5728\u5173\u4e8e\u56de\u5f52\u7684\u6700\u540e\u4e00\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u903b\u8f91\u56de\u5f52\uff0c\u8fd9\u662f\u7ecf\u5178\u7684\u57fa\u672c\u6280\u672f\u4e4b\u4e00\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u6b64\u6280\u672f\u6765\u53d1\u73b0\u9884\u6d4b\u4e8c\u5143\u5206\u7c7b\u7684\u6a21\u5f0f\u3002\u8fd9\u662f\u4e0d\u662f\u5de7\u514b\u529b\u7cd6\uff1f\u8fd9\u79cd\u75c5\u4f1a\u4f20\u67d3\u5417\uff1f\u8fd9\u4e2a\u987e\u5ba2\u4f1a\u9009\u62e9\u8fd9\u4e2a\u4ea7\u54c1\u5417\uff1f</p> <p>\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\uff1a</p> <ul> <li>\u7528\u4e8e\u6570\u636e\u53ef\u89c6\u5316\u7684\u65b0\u5e93</li> <li>\u903b\u8f91\u56de\u5f52\u6280\u672f</li> </ul> <p>\u2705 \u5728\u6b64\u5b66\u4e60\u6a21\u5757 \u4e2d\u52a0\u6df1\u4f60\u5bf9\u4f7f\u7528\u6b64\u7c7b\u56de\u5f52\u7684\u7406\u89e3</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_4","title":"\u524d\u63d0","text":"<p>\u4f7f\u7528\u5357\u74dc\u6570\u636e\u540e\uff0c\u6211\u4eec\u73b0\u5728\u5bf9\u5b83\u5df2\u7ecf\u8db3\u591f\u719f\u6089\u4e86\uff0c\u53ef\u4ee5\u610f\u8bc6\u5230\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u4e8c\u5143\u7c7b\u522b\uff1a<code>Color</code>\u3002</p> <p>\u8ba9\u6211\u4eec\u5efa\u7acb\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u6a21\u578b\u6765\u9884\u6d4b\uff0c\u7ed9\u5b9a\u4e00\u4e9b\u53d8\u91cf\uff0c\u7ed9\u5b9a\u7684\u5357\u74dc\u53ef\u80fd\u662f\u4ec0\u4e48\u989c\u8272\uff08\u6a59\u8272\ud83c\udf83\u6216\u767d\u8272\ud83d\udc7b\uff09\u3002</p> <p>\u4e3a\u4ec0\u4e48\u6211\u4eec\u5728\u5173\u4e8e\u56de\u5f52\u7684\u8bfe\u7a0b\u5206\u7ec4\u4e2d\u8c08\u8bba\u4e8c\u5143\u5206\u7c7b\uff1f \u53ea\u662f\u4e3a\u4e86\u8bed\u8a00\u4e0a\u7684\u65b9\u4fbf\uff0c\u56e0\u4e3a\u903b\u8f91\u56de\u5f52\u771f\u7684\u662f\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u662f\u57fa\u4e8e\u7ebf\u6027\u7684\u3002\u6211\u4eec\u5c06\u5728\u5728\u4e0b\u4e00\u8bfe\u7ec4\u4e2d\u4e86\u89e3\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u7684\u5176\u4ed6\u65b9\u6cd5\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_5","title":"\u5b9a\u4e49\u95ee\u9898","text":"<p>\u51fa\u4e8e\u6211\u4eec\u7684\u76ee\u7684\uff0c\u6211\u4eec\u5c06\u5176\u8868\u793a\u4e3a\u4e8c\u8fdb\u5236\uff1a\u201c\u6a59\u8272\u201d\u6216\u201c\u975e\u6a59\u8272\u201d\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u8fd8\u6709\u4e00\u4e2a\u201c\u6761\u7eb9\u201d\u7c7b\u522b\uff0c\u4f46\u5b83\u7684\u5b9e\u4f8b\u5f88\u5c11\uff0c\u6240\u4ee5\u6211\u4eec\u4e0d\u4f1a\u4f7f\u7528\u5b83\u3002\u65e0\u8bba\u5982\u4f55\uff0c\u4e00\u65e6\u6211\u4eec\u4ece\u6570\u636e\u96c6\u4e2d\u5220\u9664\u7a7a\u503c\uff0c\u5b83\u5c31\u4f1a\u6d88\u5931\u3002</p> <p>\ud83c\udf83 \u6709\u8da3\u7684\u662f\uff0c\u6211\u4eec\u6709\u65f6\u79f0\u767d\u5357\u74dc\u4e3a\u9b3c\u5357\u74dc\u3002\u4ed6\u4eec\u4e0d\u662f\u5f88\u5bb9\u6613\u96d5\u523b\uff0c\u6240\u4ee5\u5b83\u4eec\u4e0d\u50cf\u6a59\u8272\u7684\u90a3\u4e48\u53d7\u6b22\u8fce\uff0c\u4f46\u5b83\u4eec\u770b\u8d77\u6765\u5f88\u9177\uff01</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_6","title":"\u5173\u4e8e\u903b\u8f91\u56de\u5f52","text":"<p>\u903b\u8f91\u56de\u5f52\u5728\u4e00\u4e9b\u91cd\u8981\u65b9\u9762\u4e0e\u4f60\u4e4b\u524d\u4e86\u89e3\u7684\u7ebf\u6027\u56de\u5f52\u4e0d\u540c\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_7","title":"\u4e8c\u5143\u5206\u7c7b","text":"<p>\u903b\u8f91\u56de\u5f52\u4e0d\u63d0\u4f9b\u4e0e\u7ebf\u6027\u56de\u5f52\u76f8\u540c\u7684\u529f\u80fd\u3002\u524d\u8005\u63d0\u4f9b\u5173\u4e8e\u4e8c\u5143\u7c7b\u522b\uff08\u201c\u6a59\u8272\u6216\u975e\u6a59\u8272\u201d\uff09\u7684\u9884\u6d4b\uff0c\u800c\u540e\u8005\u80fd\u591f\u9884\u6d4b\u8fde\u7eed\u503c\uff0c\u4f8b\u5982\uff0c\u7ed9\u5b9a\u5357\u74dc\u7684\u8d77\u6e90\u548c\u6536\u83b7\u65f6\u95f4\uff0c\u5176\u4ef7\u683c\u5c06\u4e0a\u6da8\u591a\u5c11\u3002</p> <p></p> <p>\u4f5c\u8005 Dasani Madipalli</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_8","title":"\u5176\u4ed6\u5206\u7c7b","text":"<p>\u8fd8\u6709\u5176\u4ed6\u7c7b\u578b\u7684\u903b\u8f91\u56de\u5f52\uff0c\u5305\u62ec\u591a\u9879\u548c\u6709\u5e8f\uff1a</p> <ul> <li>\u591a\u9879\uff0c\u6d89\u53ca\u591a\u4e2a\u7c7b\u522b - \u201c\u6a59\u8272\u3001\u767d\u8272\u548c\u6761\u7eb9\u201d\u3002</li> <li>\u6709\u5e8f\uff0c\u6d89\u53ca\u6709\u5e8f\u7c7b\u522b\uff0c\u5982\u679c\u6211\u4eec\u60f3\u5bf9\u6211\u4eec\u7684\u7ed3\u679c\u8fdb\u884c\u903b\u8f91\u6392\u5e8f\u975e\u5e38\u6709\u7528\uff0c\u4f8b\u5982\u6211\u4eec\u7684\u5357\u74dc\u6309\u6709\u9650\u6570\u91cf\u7684\u5927\u5c0f\uff08mini\u3001sm\u3001med\u3001lg\u3001xl\u3001xxl\uff09\u6392\u5e8f\u3002</li> </ul> <p></p> <p>\u4f5c\u8005 Dasani Madipalli</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_9","title":"\u4ecd\u7136\u662f\u7ebf\u6027\u7684","text":"<p>\u5c3d\u7ba1\u8fd9\u79cd\u7c7b\u578b\u7684\u56de\u5f52\u90fd\u662f\u5173\u4e8e\u201c\u7c7b\u522b\u9884\u6d4b\u201d\u7684\uff0c\u4f46\u5f53\u56e0\u53d8\u91cf\uff08\u989c\u8272\uff09\u548c\u5176\u4ed6\u81ea\u53d8\u91cf\uff08\u6570\u636e\u96c6\u7684\u5176\u4f59\u90e8\u5206\uff0c\u5982\u57ce\u5e02\u540d\u79f0\u548c\u5927\u5c0f\uff09\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u7ebf\u6027\u5173\u7cfb\u65f6\uff0c\u5b83\u4ecd\u7136\u6548\u679c\u6700\u597d\u3002\u6700\u597d\u4e86\u89e3\u4e00\u4e0b\u8fd9\u4e9b\u53d8\u91cf\u662f\u5426\u5b58\u5728\u7ebf\u6027\u5212\u5206\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_10","title":"\u53d8\u91cf\u4e0d\u5fc5\u76f8\u5173","text":"<p>\u8fd8\u8bb0\u5f97\u7ebf\u6027\u56de\u5f52\u5982\u4f55\u66f4\u597d\u5730\u5904\u7406\u66f4\u591a\u76f8\u5173\u53d8\u91cf\u5417\uff1f\u903b\u8f91\u56de\u5f52\u662f\u76f8\u53cd\u7684\u2014\u2014\u53d8\u91cf\u4e0d\u5fc5\u5bf9\u9f50\u3002\u8fd9\u9002\u7528\u4e8e\u76f8\u5173\u6027\u8f83\u5f31\u7684\u6570\u636e\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_11","title":"\u4f60\u9700\u8981\u5927\u91cf\u5e72\u51c0\u7684\u6570\u636e","text":"<p>\u5982\u679c\u4f7f\u7528\u66f4\u591a\u6570\u636e\uff0c\u903b\u8f91\u56de\u5f52\u5c06\u7ed9\u51fa\u66f4\u51c6\u786e\u7684\u7ed3\u679c\uff1b\u6211\u4eec\u7684\u5c0f\u6570\u636e\u96c6\u5bf9\u4e8e\u8fd9\u9879\u4efb\u52a1\u4e0d\u662f\u6700\u4f73\u7684\uff0c\u8bf7\u8bb0\u4f4f\u8fd9\u4e00\u70b9\u3002</p> <p>\u2705 \u8003\u8651\u9002\u5408\u903b\u8f91\u56de\u5f52\u7684\u6570\u636e\u7c7b\u578b</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u6574\u7406\u6570\u636e","text":"<p>\u9996\u5148\uff0c\u7a0d\u5fae\u6e05\u7406\u4e00\u4e0b\u6570\u636e\uff0c\u5220\u9664\u7a7a\u503c\u5e76\u4ec5\u9009\u62e9\u5176\u4e2d\u4e00\u4e9b\u5217\uff1a</p> <ol> <li> <p>\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nnew_columns = ['Color','Origin','Item Size','Variety','City Name','Package']\n\nnew_pumpkins = pumpkins.drop([c for c in pumpkins.columns if c not in new_columns], axis=1)\n\nnew_pumpkins.dropna(inplace=True)\n\nnew_pumpkins = new_pumpkins.apply(LabelEncoder().fit_transform)\n</code></pre> <p>\u4f60\u53ef\u4ee5\u968f\u65f6\u67e5\u770b\u65b0\u7684\u6570\u636e\u5e27\uff1a</p> <pre><code>new_pumpkins.info\n</code></pre> </li> </ol>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#-_1","title":"\u53ef\u89c6\u5316 - \u5e76\u5217\u7f51\u683c","text":"<p>\u5230\u73b0\u5728\u4e3a\u6b62\uff0c\u4f60\u5df2\u7ecf\u518d\u6b21\u4f7f\u7528\u5357\u74dc\u6570\u636e\u52a0\u8f7d\u4e86 starter notebook \u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u6e05\u7406\uff0c\u4ee5\u4fdd\u7559\u5305\u542b\u4e00\u4e9b\u53d8\u91cf\uff08\u5305\u62ec <code>Color</code>\uff09\u7684\u6570\u636e\u96c6\u3002\u8ba9\u6211\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u5e93\u6765\u53ef\u89c6\u5316 notebook \u4e2d\u7684\u6570\u636e\u5e27\uff1aSeaborn\uff0c\u5b83\u662f\u57fa\u4e8e\u6211\u4eec\u4e4b\u524d\u4f7f\u7528\u7684 Matplotlib \u6784\u5efa\u7684\u3002</p> <p>Seaborn \u63d0\u4f9b\u4e86\u4e00\u4e9b\u5de7\u5999\u7684\u65b9\u6cd5\u6765\u53ef\u89c6\u5316\u4f60\u7684\u6570\u636e\u3002\u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u6bd4\u8f83\u5e76\u5217\u7f51\u683c\u4e2d\u6bcf\u4e2a\u70b9\u7684\u6570\u636e\u5206\u5e03\u3002</p> <ol> <li> <p>\u901a\u8fc7\u5b9e\u4f8b\u5316\u4e00\u4e2a <code>PairGrid</code>\uff0c\u4f7f\u7528\u6211\u4eec\u7684\u5357\u74dc\u6570\u636e <code>new_pumpkins</code>\uff0c\u7136\u540e\u8c03\u7528 <code>map()</code> \u6765\u521b\u5efa\u8fd9\u6837\u4e00\u4e2a\u7f51\u683c\uff1a</p> <pre><code>import seaborn as sns\n\ng = sns.PairGrid(new_pumpkins)\ng.map(sns.scatterplot)\n</code></pre> <p></p> <p>\u901a\u8fc7\u5e76\u5217\u89c2\u5bdf\u6570\u636e\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u989c\u8272\u6570\u636e\u4e0e\u5176\u4ed6\u5217\u7684\u5173\u7cfb\u3002</p> <p>\u2705 \u9274\u4e8e\u6b64\u6563\u70b9\u56fe\u7f51\u683c\uff0c\u4f60\u53ef\u4ee5\u8bbe\u60f3\u54ea\u4e9b\u6709\u8da3\u7684\u63a2\u7d22\uff1f</p> </li> </ol>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_12","title":"\u4f7f\u7528\u5206\u7c7b\u6563\u70b9\u56fe","text":"<p>\u7531\u4e8e\u989c\u8272\u662f\u4e00\u4e2a\u4e8c\u5143\u7c7b\u522b\uff08\u6a59\u8272\u6216\u975e\u6a59\u8272\uff09\uff0c\u5b83\u88ab\u79f0\u4e3a\u201c\u5206\u7c7b\u6570\u636e\u201d\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4e13\u4e1a\u7684\u65b9\u6cd5\u6765\u53ef\u89c6\u5316\u3002\u8fd8\u6709\u5176\u4ed6\u65b9\u6cd5\u53ef\u4ee5\u53ef\u89c6\u5316\u6b64\u7c7b\u522b\u4e0e\u5176\u4ed6\u53d8\u91cf\u7684\u5173\u7cfb\u3002</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528 Seaborn \u56fe\u5e76\u5217\u53ef\u89c6\u5316\u53d8\u91cf\u3002</p> <ol> <li> <p>\u5c1d\u8bd5\u4f7f\u7528\u201c\u5206\u7c7b\u6563\u70b9\u201d\u56fe\u6765\u663e\u793a\u503c\u7684\u5206\u5e03\uff1a</p> <pre><code>sns.swarmplot(x=\"Color\", y=\"Item Size\", data=new_pumpkins)\n</code></pre> <p></p> </li> </ol>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_13","title":"\u5c0f\u63d0\u7434\u56fe","text":"<p>\u201c\u5c0f\u63d0\u7434\u201d\u7c7b\u578b\u7684\u56fe\u5f88\u6709\u7528\uff0c\u56e0\u4e3a\u4f60\u53ef\u4ee5\u8f7b\u677e\u5730\u53ef\u89c6\u5316\u4e24\u4e2a\u7c7b\u522b\u4e2d\u6570\u636e\u7684\u5206\u5e03\u65b9\u5f0f\u3002\u5c0f\u63d0\u7434\u56fe\u4e0d\u9002\u7528\u4e8e\u8f83\u5c0f\u7684\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u5206\u5e03\u663e\u793a\u5f97\u66f4\u201c\u5e73\u6ed1\u201d\u3002</p> <ol> <li> <p>\u4f5c\u4e3a\u53c2\u6570 <code>x=Color</code>\u3001<code>kind=\"violin\"</code> \u5e76\u8c03\u7528 <code>catplot()</code>\uff1a</p> <pre><code>sns.catplot(x=\"Color\", y=\"Item Size\",\n            kind=\"violin\", data=new_pumpkins)\n</code></pre> <p></p> <p>\u2705 \u5c1d\u8bd5\u4f7f\u7528\u5176\u4ed6\u53d8\u91cf\u521b\u5efa\u6b64\u56fe\u548c\u5176\u4ed6 Seaborn \u56fe\u3002</p> </li> </ol> <p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u989c\u8272\u7684\u4e8c\u5143\u7c7b\u522b\u4e0e\u66f4\u5927\u7684\u5c3a\u5bf8\u7ec4\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8ba9\u6211\u4eec\u63a2\u7d22\u903b\u8f91\u56de\u5f52\u6765\u786e\u5b9a\u7ed9\u5b9a\u5357\u74dc\u7684\u53ef\u80fd\u989c\u8272\u3002</p> <p>\ud83e\uddee \u6570\u5b66\u77e5\u8bc6</p> <p>\u8fd8\u8bb0\u5f97\u7ebf\u6027\u56de\u5f52\u5982\u4f55\u7ecf\u5e38\u4f7f\u7528\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6765\u5f97\u51fa\u4e00\u4e2a\u503c\u5417\uff1f\u903b\u8f91\u56de\u5f52\u4f9d\u8d56\u4e8e\u4f7f\u7528sigmoid \u51fd\u6570 \u7684\u201c\u6700\u5927\u4f3c\u7136\u201d\u6982\u5ff5\u3002\u7ed8\u56fe\u4e0a\u7684\u201cSigmoid \u51fd\u6570\u201d\u770b\u8d77\u6765\u50cf\u201cS\u201d\u5f62\u3002\u5b83\u63a5\u53d7\u4e00\u4e2a\u503c\u5e76\u5c06\u5176\u6620\u5c04\u52300\u548c1\u4e4b\u95f4\u7684\u67d0\u4e2a\u4f4d\u7f6e\u3002\u5b83\u7684\u66f2\u7ebf\u4e5f\u79f0\u4e3a\u201c\u903b\u8f91\u66f2\u7ebf\u201d\u3002\u5b83\u7684\u516c\u5f0f\u5982\u4e0b\u6240\u793a\uff1a</p> <p></p> <p>\u5176\u4e2d sigmoid \u7684\u4e2d\u70b9\u4f4d\u4e8e x \u7684 0 \u70b9\uff0cL \u662f\u66f2\u7ebf\u7684\u6700\u5927\u503c\uff0ck \u662f\u66f2\u7ebf\u7684\u9661\u5ea6\u3002\u5982\u679c\u51fd\u6570\u7684\u7ed3\u679c\u5927\u4e8e 0.5\uff0c\u5219\u6240\u8ba8\u8bba\u7684\u6807\u7b7e\u5c06\u88ab\u8d4b\u4e88\u4e8c\u8fdb\u5236\u9009\u62e9\u7684\u7c7b\u201c1\u201d\u3002\u5426\u5219\uff0c\u5b83\u5c06\u88ab\u5206\u7c7b\u4e3a\u201c0\u201d\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_14","title":"\u5efa\u7acb\u4f60\u7684\u6a21\u578b","text":"<p>\u5728 Scikit-learn \u4e2d\u6784\u5efa\u6a21\u578b\u6765\u67e5\u627e\u8fd9\u4e9b\u4e8c\u5143\u5206\u7c7b\u975e\u5e38\u7b80\u5355\u3002</p> <ol> <li> <p>\u9009\u62e9\u8981\u5728\u5206\u7c7b\u6a21\u578b\u4e2d\u4f7f\u7528\u7684\u53d8\u91cf\uff0c\u5e76\u8c03\u7528 <code>train_test_split()</code> \u62c6\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a</p> <pre><code>from sklearn.model_selection import train_test_split\n\nSelected_features = ['Origin','Item Size','Variety','City Name','Package']\n\nX = new_pumpkins[Selected_features]\ny = new_pumpkins['Color']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n</code></pre> </li> <li> <p>\u73b0\u5728\u4f60\u53ef\u4ee5\u8bad\u7ec3\u4f60\u7684\u6a21\u578b\uff0c\u7528\u4f60\u7684\u8bad\u7ec3\u6570\u636e\u8c03\u7528 <code>fit()</code>\uff0c\u5e76\u6253\u5370\u51fa\u5b83\u7684\u7ed3\u679c\uff1a</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nprint('Predicted labels: ', predictions)\nprint('Accuracy: ', accuracy_score(y_test, predictions))\n</code></pre> <p>\u770b\u770b\u4f60\u7684\u6a21\u578b\u7684\u8bb0\u5206\u677f\u3002\u8003\u8651\u5230\u4f60\u53ea\u6709\u5927\u7ea6 1000 \u884c\u6570\u636e\uff0c\u8fd9\u8fd8\u4e0d\u9519\uff1a</p> <pre><code>                   precision    recall  f1-score   support\n\n           0       0.85      0.95      0.90       166\n           1       0.38      0.15      0.22        33\n\n    accuracy                           0.82       199\n   macro avg       0.62      0.55      0.56       199\nweighted avg       0.77      0.82      0.78       199\n\nPredicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 1 0 0 1 0 0 0 1 0]\n</code></pre> </li> </ol>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_15","title":"\u901a\u8fc7\u6df7\u6dc6\u77e9\u9635\u66f4\u597d\u5730\u7406\u89e3","text":"<p>\u867d\u7136\u4f60\u53ef\u4ee5\u901a\u8fc7\u83b7\u5f97\u8bb0\u5206\u677f\u62a5\u544a\u6761\u76ee\u628a\u4e0a\u9762\u7684\u9879\u76ee\u6253\u5370\u51fa\u6765\uff0c\u901a\u8fc7\u4f7f\u7528\u6df7\u6dc6\u77e9\u9635\u53ef\u4ee5\u66f4\u5bb9\u6613\u5730\u7406\u89e3\u4f60\u7684\u6a21\u578b\uff0c\u5e2e\u52a9\u6211\u4eec\u4e86\u89e3\u6a21\u578b\u7684\u6027\u80fd\u3002</p> <p>\ud83c\udf93 \u201c\u6df7\u6dc6\u77e9\u9635\u201d\uff08\u6216\u201c\u8bef\u5dee\u77e9\u9635\u201d\uff09\u662f\u4e00\u4e2a\u8868\u683c\uff0c\u7528\u4e8e\u8868\u793a\u6a21\u578b\u7684\u771f\u5047\u9633\u6027\u548c\u5047\u9634\u6027\uff0c\u4ece\u800c\u8861\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002</p> <ol> <li> <p>\u8981\u4f7f\u7528\u6df7\u6dc6\u6307\u6807\uff0c\u8bf7\u8c03\u7528 <code>confusin_matrix()</code>\uff1a</p> <pre><code>from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, predictions)\n</code></pre> <p>\u770b\u770b\u4f60\u7684\u6a21\u578b\u7684\u6df7\u6dc6\u77e9\u9635\uff1a</p> <pre><code>array([[162,   4],\n       [ 33,   0]])\n</code></pre> </li> </ol> <p>\u8fd9\u91cc\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f\u5047\u8bbe\u6211\u4eec\u7684\u6a21\u578b\u88ab\u8981\u6c42\u5bf9\u4e24\u4e2a\u4e8c\u5143\u7c7b\u522b\u4e4b\u95f4\u7684\u9879\u76ee\u8fdb\u884c\u5206\u7c7b\uff0c\u5373\u7c7b\u522b\u201c\u5357\u74dc\u201d\u548c\u7c7b\u522b\u201c\u975e\u5357\u74dc\u201d\u3002</p> <ul> <li>\u5982\u679c\u4f60\u7684\u6a21\u578b\u5c06\u67d0\u7269\u9884\u6d4b\u4e3a\u5357\u74dc\u5e76\u4e14\u5b83\u5b9e\u9645\u4e0a\u5c5e\u4e8e\u201c\u5357\u74dc\u201d\u7c7b\u522b\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u771f\u9633\u6027\uff0c\u7531\u5de6\u4e0a\u89d2\u7684\u6570\u5b57\u663e\u793a\u3002</li> <li>\u5982\u679c\u4f60\u7684\u6a21\u578b\u9884\u6d4b\u67d0\u7269\u4e0d\u662f\u5357\u74dc\uff0c\u5e76\u4e14\u5b83\u5b9e\u9645\u4e0a\u5c5e\u4e8e\u201c\u5357\u74dc\u201d\u7c7b\u522b\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u5047\u9633\u6027\uff0c\u5982\u53f3\u4e0a\u89d2\u7684\u6570\u5b57\u6240\u793a\u3002</li> <li>\u5982\u679c\u4f60\u7684\u6a21\u578b\u5c06\u67d0\u7269\u9884\u6d4b\u4e3a\u5357\u74dc\u5e76\u4e14\u5b83\u5b9e\u9645\u4e0a\u5c5e\u4e8e\u201c\u975e\u5357\u74dc\u201d\u7c7b\u522b\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u5047\u9634\u6027\uff0c\u7531\u5de6\u4e0b\u89d2\u7684\u6570\u5b57\u663e\u793a\u3002</li> <li>\u5982\u679c\u4f60\u7684\u6a21\u578b\u9884\u6d4b\u67d0\u7269\u4e0d\u662f\u5357\u74dc\uff0c\u5e76\u4e14\u5b83\u5b9e\u9645\u4e0a\u5c5e\u4e8e\u201c\u975e\u5357\u74dc\u201d\u7c7b\u522b\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u771f\u9634\u6027\uff0c\u5982\u53f3\u4e0b\u89d2\u7684\u6570\u5b57\u6240\u793a\u3002</li> </ul> <p>\u6b63\u5982\u4f60\u53ef\u80fd\u5df2\u7ecf\u731c\u5230\u7684\u90a3\u6837\uff0c\u6700\u597d\u6709\u66f4\u591a\u7684\u771f\u9633\u6027\u548c\u771f\u9634\u6027\u4ee5\u53ca\u8f83\u5c11\u7684\u5047\u9633\u6027\u548c\u5047\u9634\u6027\uff0c\u8fd9\u610f\u5473\u7740\u6a21\u578b\u6027\u80fd\u66f4\u597d\u3002</p> <p>\u2705 Q\uff1a\u6839\u636e\u6df7\u6dc6\u77e9\u9635\uff0c\u6a21\u578b\u600e\u4e48\u6837\uff1f A\uff1a\u8fd8\u4e0d\u9519\uff1b\u6709\u5f88\u591a\u771f\u9633\u6027\uff0c\u4f46\u4e5f\u6709\u4e00\u4e9b\u5047\u9634\u6027\u3002</p> <p>\u8ba9\u6211\u4eec\u501f\u52a9\u6df7\u6dc6\u77e9\u9635\u5bf9TP/TN\u548cFP/FN\u7684\u6620\u5c04\uff0c\u91cd\u65b0\u5ba1\u89c6\u4e00\u4e0b\u6211\u4eec\u4e4b\u524d\u770b\u5230\u7684\u672f\u8bed\uff1a</p> <p>\ud83c\udf93 \u51c6\u786e\u7387\uff1aTP/(TP + FP) \u68c0\u7d22\u5b9e\u4f8b\u4e2d\u76f8\u5173\u5b9e\u4f8b\u7684\u5206\u6570\uff08\u4f8b\u5982\uff0c\u54ea\u4e9b\u6807\u7b7e\u6807\u8bb0\u5f97\u5f88\u597d\uff09</p> <p>\ud83c\udf93 \u53ec\u56de\u7387: TP/(TP + FN) \u68c0\u7d22\u5230\u7684\u76f8\u5173\u5b9e\u4f8b\u7684\u6bd4\u4f8b\uff0c\u65e0\u8bba\u662f\u5426\u6807\u8bb0\u826f\u597d</p> <p>\ud83c\udf93 F1\u5206\u6570: (2 * \u51c6\u786e\u7387 * \u53ec\u56de\u7387)/(\u51c6\u786e\u7387 + \u53ec\u56de\u7387) \u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u52a0\u6743\u5e73\u5747\u503c\uff0c\u6700\u597d\u4e3a1\uff0c\u6700\u5dee\u4e3a0</p> <p>\ud83c\udf93 Support\uff1a\u68c0\u7d22\u5230\u7684\u6bcf\u4e2a\u6807\u7b7e\u7684\u51fa\u73b0\u6b21\u6570</p> <p>\ud83c\udf93 \u51c6\u786e\u5ea6\uff1a(TP + TN)/(TP + TN + FP + FN) \u4e3a\u6837\u672c\u51c6\u786e\u9884\u6d4b\u7684\u6807\u7b7e\u767e\u5206\u6bd4\u3002</p> <p>\ud83c\udf93 \u5b8f\u5e73\u5747\u503c: \u8ba1\u7b97\u6bcf\u4e2a\u6807\u7b7e\u7684\u672a\u52a0\u6743\u5e73\u5747\u6307\u6807\uff0c\u4e0d\u8003\u8651\u6807\u7b7e\u4e0d\u5e73\u8861\u3002</p> <p>\ud83c\udf93 \u52a0\u6743\u5e73\u5747\u503c\uff1a\u8ba1\u7b97\u6bcf\u4e2a\u6807\u7b7e\u7684\u5e73\u5747\u6307\u6807\uff0c\u901a\u8fc7\u6309\u652f\u6301\u5ea6\uff08\u6bcf\u4e2a\u6807\u7b7e\u7684\u771f\u5b9e\u5b9e\u4f8b\u6570\uff09\u52a0\u6743\u6765\u8003\u8651\u6807\u7b7e\u4e0d\u5e73\u8861\u3002</p> <p>\u2705 \u5982\u679c\u4f60\u60f3\u8ba9\u4f60\u7684\u6a21\u578b\u51cf\u5c11\u5047\u9634\u6027\u7684\u6570\u91cf\uff0c\u4f60\u80fd\u60f3\u51fa\u5e94\u8be5\u5173\u6ce8\u54ea\u4e2a\u6307\u6807\u5417\uff1f</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#roc","title":"\u53ef\u89c6\u5316\u8be5\u6a21\u578b\u7684 ROC \u66f2\u7ebf","text":"<p>\u8fd9\u4e0d\u662f\u4e00\u4e2a\u7cdf\u7cd5\u7684\u6a21\u578b\uff1b\u5b83\u7684\u51c6\u786e\u7387\u5728 80% \u8303\u56f4\u5185\uff0c\u56e0\u6b64\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u9884\u6d4b\u7ed9\u5b9a\u4e00\u7ec4\u53d8\u91cf\u7684\u5357\u74dc\u989c\u8272\u3002</p> <p>\u8ba9\u6211\u4eec\u518d\u505a\u4e00\u4e2a\u53ef\u89c6\u5316\u6765\u67e5\u770b\u6240\u8c13\u7684\u201cROC\u201d\u5206\u6570</p> <pre><code>from sklearn.metrics import roc_curve, roc_auc_score\n\ny_scores = model.predict_proba(X_test)\n# calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\nsns.lineplot([0, 1], [0, 1])\nsns.lineplot(fpr, tpr)\n</code></pre> <p>\u518d\u6b21\u4f7f\u7528 Seaborn\uff0c\u7ed8\u5236\u6a21\u578b\u7684\u63a5\u6536\u64cd\u4f5c\u7279\u6027\u6216 ROC\u3002 ROC \u66f2\u7ebf\u901a\u5e38\u7528\u4e8e\u6839\u636e\u5206\u7c7b\u5668\u7684\u771f\u5047\u9633\u6027\u6765\u4e86\u89e3\u5206\u7c7b\u5668\u7684\u8f93\u51fa\u3002\u201cROC \u66f2\u7ebf\u901a\u5e38\u5177\u6709 Y \u8f74\u4e0a\u7684\u771f\u9633\u6027\u7387\u548c X \u8f74\u4e0a\u7684\u5047\u9633\u6027\u7387\u3002\u201d \u56e0\u6b64\uff0c\u66f2\u7ebf\u7684\u9661\u5ea6\u4ee5\u53ca\u4e2d\u70b9\u7ebf\u4e0e\u66f2\u7ebf\u4e4b\u95f4\u7684\u7a7a\u95f4\u5f88\u91cd\u8981\uff1a\u4f60\u9700\u8981\u4e00\u6761\u5feb\u901f\u5411\u4e0a\u5e76\u8d8a\u8fc7\u76f4\u7ebf\u7684\u66f2\u7ebf\u3002\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e00\u5f00\u59cb\u5c31\u6709\u8bef\u62a5\uff0c\u7136\u540e\u8fd9\u6761\u7ebf\u6b63\u786e\u5730\u5411\u4e0a\u548c\u91cd\u590d\uff1a</p> <p></p> <p>\u6700\u540e\uff0c\u4f7f\u7528 Scikit-learn \u7684<code>roc_auc_score</code> API \u6765\u8ba1\u7b97\u5b9e\u9645\u201c\u66f2\u7ebf\u4e0b\u9762\u79ef\u201d\uff08AUC\uff09\uff1a</p> <pre><code>auc = roc_auc_score(y_test,y_scores[:,1])\nprint(auc)\n</code></pre> <p>\u7ed3\u679c\u662f <code>0.6976998904709748</code>\u3002 \u9274\u4e8e AUC \u7684\u8303\u56f4\u4ece 0 \u5230 1\uff0c\u4f60\u9700\u8981\u4e00\u4e2a\u9ad8\u5206\uff0c\u56e0\u4e3a\u9884\u6d4b 100% \u6b63\u786e\u7684\u6a21\u578b\u7684 AUC \u4e3a 1\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b_\u76f8\u5f53\u4e0d\u9519_\u3002</p> <p>\u5728\u4ee5\u540e\u7684\u5206\u7c7b\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5982\u4f55\u8fed\u4ee3\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u5206\u6570\u3002\u4f46\u662f\u73b0\u5728\uff0c\u606d\u559c\uff01\u4f60\u5df2\u7ecf\u5b8c\u6210\u4e86\u8fd9\u4e9b\u56de\u5f52\u8bfe\u7a0b\uff01</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_16","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u5173\u4e8e\u903b\u8f91\u56de\u5f52\uff0c\u8fd8\u6709\u5f88\u591a\u4e1c\u897f\u9700\u8981\u89e3\u5f00\uff01\u4f46\u6700\u597d\u7684\u5b66\u4e60\u65b9\u6cd5\u662f\u5b9e\u9a8c\u3002\u627e\u5230\u9002\u5408\u6b64\u7c7b\u5206\u6790\u7684\u6570\u636e\u96c6\u5e76\u7528\u5b83\u6784\u5efa\u6a21\u578b\u3002\u4f60\u5b66\u5230\u4e86\u4ec0\u4e48\uff1f\u5c0f\u8d34\u58eb\uff1a\u5c1d\u8bd5 Kaggle \u83b7\u53d6\u6709\u8da3\u7684\u6570\u636e\u96c6\u3002</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_17","title":"\u8bfe\u540e\u6d4b","text":""},{"location":"2-Regression/4-Logistic/README.zh-cn/#_18","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u9605\u8bfb\u65af\u5766\u798f\u5927\u5b66\u7684\u8fd9\u7bc7\u8bba\u6587\u7684\u524d\u51e0\u9875\u5173\u4e8e\u903b\u8f91\u56de\u5f52\u7684\u4e00\u4e9b\u5b9e\u9645\u5e94\u7528\u3002\u60f3\u60f3\u90a3\u4e9b\u66f4\u9002\u5408\u4e8e\u6211\u4eec\u76ee\u524d\u6240\u7814\u7a76\u7684\u4e00\u79cd\u6216\u53e6\u4e00\u79cd\u7c7b\u578b\u7684\u56de\u5f52\u4efb\u52a1\u7684\u4efb\u52a1\u3002\u4ec0\u4e48\u6700\u6709\u6548\uff1f</p>"},{"location":"2-Regression/4-Logistic/README.zh-cn/#_19","title":"\u4efb\u52a1","text":"<p>\u91cd\u8bd5\u6b64\u56de\u5f52</p>"},{"location":"2-Regression/4-Logistic/assignment/","title":"Retrying some Regression","text":""},{"location":"2-Regression/4-Logistic/assignment/#instructions","title":"Instructions","text":"<p>In the lesson, you used a subset of the pumpkin data. Now, go back to the original data and try to use all of it, cleaned and standardized, to build a Logistic Regression model.</p>"},{"location":"2-Regression/4-Logistic/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with a well-explained and well-performing model A notebook is presented with a model that performs minimally A notebook is presented with a sub-performing model or none"},{"location":"2-Regression/4-Logistic/assignment.zh-cn/","title":"\u518d\u63a2\u56de\u5f52\u6a21\u578b","text":""},{"location":"2-Regression/4-Logistic/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u8fd9\u8282\u8bfe\u4e2d\uff0c\u4f60\u4f7f\u7528\u4e86 pumpkin \u6570\u636e\u96c6\u7684\u5b50\u96c6\u3002\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u56de\u5230\u539f\u59cb\u6570\u636e\uff0c\u5e76\u5c1d\u8bd5\u4f7f\u7528\u6240\u6709\u6570\u636e\u3002\u7ecf\u8fc7\u4e86\u6570\u636e\u6e05\u7406\u548c\u6807\u51c6\u5316\uff0c\u5efa\u7acb\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002</p>"},{"location":"2-Regression/4-Logistic/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u7528 notebook \u5448\u73b0\u4e86\u4e00\u4e2a\u89e3\u91ca\u6027\u548c\u6027\u80fd\u826f\u597d\u7684\u6a21\u578b \u7528 notebook \u5448\u73b0\u4e86\u4e00\u4e2a\u6027\u80fd\u4e00\u822c\u7684\u6a21\u578b \u7528 notebook \u5448\u73b0\u4e86\u4e00\u4e2a\u6027\u80fd\u5dee\u7684\u6a21\u578b\u6216\u6839\u672c\u6ca1\u6709\u6a21\u578b"},{"location":"2-Regression/4-Logistic/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"3-Web-App/","title":"\u6784\u5efa\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f\u6765\u4f7f\u7528\u60a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b","text":"<p>\u8bfe\u7a0b\u7684\u672c\u7ae0\u8282\u5c06\u4e3a\u60a8\u4ecb\u7ecd\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\uff1a\u5982\u4f55\u4fdd\u5b58\u60a8\u7684 Scikit-learn \u6a21\u578b\u4e3a\u6587\u4ef6\u4ee5\u4fbf\u5728 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u8be5\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002\u6a21\u578b\u4fdd\u5b58\u540e\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u5728\u4e00\u4e2a\u7531 Flask \u6784\u5efa\u7684 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u5b83\u3002\u9996\u5148\uff0c\u60a8\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e9b UFO \u76ee\u51fb\u4e8b\u4ef6\u7684\u6570\u636e\u53bb\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\uff01\u7136\u540e\uff0c\u60a8\u5c06\u6784\u5efa\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd9\u4e2a\u5e94\u7528\u7a0b\u5e8f\u80fd\u8ba9\u60a8\u8f93\u5165\u79d2\u6570\uff0c\u7ecf\u5ea6\uff0c\u7eac\u5ea6\u6765\u9884\u6d4b\u54ea\u4e2a\u56fd\u5bb6\u4f1a\u62a5\u544a UFO \u76ee\u51fb\u4e8b\u4ef6\u3002</p> <p></p> <p>\u56fe\u7247\u7531 Michael Herren \u62cd\u6444\uff0c\u6765\u81ea Unsplash</p>"},{"location":"3-Web-App/#_1","title":"\u6559\u7a0b","text":"<ol> <li>\u6784\u5efa\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f</li> </ol>"},{"location":"3-Web-App/#_2","title":"\u81f4\u8c22","text":"<p>\"\u6784\u5efa\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f\" \u7531 Jen Looper \u7528 \u2665 \u7f16\u5199\ufe0f</p> <p>\u6d4b\u9a8c\u7531 Rohan Raj \u7528 \u2665\ufe0f \u7f16\u5199</p> <p>\u6570\u636e\u96c6\u6765\u81ea Kaggle</p> <p>Web \u5e94\u7528\u7a0b\u5e8f\u7684\u67b6\u6784\u4e00\u90e8\u5206\u53c2\u8003\u4e86 Abhinav Sagar \u7684\u6587\u7ae0\u548c\u4ed3\u5e93 </p>"},{"location":"3-Web-App/1-Web-App/","title":"\u6784\u5efa\u4f7f\u7528 ML \u6a21\u578b\u7684 Web \u5e94\u7528\u7a0b\u5e8f","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a ML \u6a21\u578b\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u6765\u81ea\u4e16\u754c\u5404\u5730\uff1a\u8fc7\u53bb\u4e00\u4e2a\u4e16\u7eaa\u7684 UFO \u76ee\u51fb\u4e8b\u4ef6\uff0c\u6765\u6e90\u4e8e NUFORC \u7684\u6570\u636e\u5e93\u3002</p> <p>\u4f60\u5c06\u5b66\u4f1a\uff1a</p> <ul> <li>\u5982\u4f55\u201cpickle\u201d\u4e00\u4e2a\u8bad\u7ec3\u6709\u7d20\u7684\u6a21\u578b</li> <li>\u5982\u4f55\u5728 Flask \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u8be5\u6a21\u578b</li> </ul> <p>\u6211\u4eec\u5c06\u7ee7\u7eed\u4f7f\u7528 notebook \u6765\u6e05\u7406\u6570\u636e\u548c\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\uff0c\u4f46\u4f60\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5728 web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u6a21\u578b\u3002</p> <p>\u4e3a\u6b64\uff0c\u4f60\u9700\u8981\u4f7f\u7528 Flask \u6784\u5efa\u4e00\u4e2a web \u5e94\u7528\u7a0b\u5e8f\u3002</p>"},{"location":"3-Web-App/1-Web-App/#_1","title":"\u8bfe\u524d\u6d4b","text":""},{"location":"3-Web-App/1-Web-App/#_2","title":"\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u6709\u591a\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6784\u5efa Web \u5e94\u7528\u7a0b\u5e8f\u4ee5\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u4f60\u7684 web \u67b6\u6784\u53ef\u80fd\u4f1a\u5f71\u54cd\u4f60\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u5f0f\u3002\u60f3\u8c61\u4e00\u4e0b\uff0c\u4f60\u5728\u4e00\u5bb6\u4f01\u4e1a\u5de5\u4f5c\uff0c\u5176\u4e2d\u6570\u636e\u79d1\u5b66\u5c0f\u7ec4\u5df2\u7ecf\u8bad\u7ec3\u4e86\u4ed6\u4eec\u5e0c\u671b\u4f60\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u7684\u6a21\u578b\u3002</p>"},{"location":"3-Web-App/1-Web-App/#_3","title":"\u6ce8\u610f\u4e8b\u9879","text":"<p>\u4f60\u9700\u8981\u95ee\u5f88\u591a\u95ee\u9898\uff1a</p> <ul> <li>\u5b83\u662f web \u5e94\u7528\u7a0b\u5e8f\u8fd8\u662f\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\uff1f \u5982\u679c\u4f60\u6b63\u5728\u6784\u5efa\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u6216\u9700\u8981\u5728\u7269\u8054\u7f51\u73af\u5883\u4e2d\u4f7f\u7528\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 TensorFlow Lite \u5e76\u5728 Android \u6216 iOS \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u8be5\u6a21\u578b\u3002</li> <li>\u6a21\u578b\u653e\u5728\u54ea\u91cc\uff1f \u5728\u4e91\u7aef\u8fd8\u662f\u672c\u5730\uff1f</li> <li>\u79bb\u7ebf\u652f\u6301\u3002\u8be5\u5e94\u7528\u7a0b\u5e8f\u662f\u5426\u5fc5\u987b\u79bb\u7ebf\u5de5\u4f5c\uff1f</li> <li>\u4f7f\u7528\u4ec0\u4e48\u6280\u672f\u6765\u8bad\u7ec3\u6a21\u578b\uff1f \u6240\u9009\u7684\u6280\u672f\u53ef\u80fd\u4f1a\u5f71\u54cd\u4f60\u9700\u8981\u4f7f\u7528\u7684\u5de5\u5177\u3002</li> <li>\u4f7f\u7528 TensorFlow\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4f60\u6b63\u5728\u4f7f\u7528 TensorFlow \u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u8be5\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4f7f\u7528 TensorFlow.js \u8f6c\u6362 TensorFlow \u6a21\u578b\u4ee5\u4fbf\u5728Web\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u7684\u80fd\u529b\u3002</li> <li>\u4f7f\u7528 PyTorch\u3002\u5982\u679c\u4f60\u4f7f\u7528 PyTorch \u7b49\u5e93\u6784\u5efa\u6a21\u578b\uff0c\u5219\u53ef\u4ee5\u9009\u62e9\u5c06\u5176\u5bfc\u51fa\u5230 ONNX\uff08\u5f00\u653e\u795e\u7ecf\u7f51\u7edc\u4ea4\u6362\uff09\u683c\u5f0f\uff0c\u7528\u4e8e\u53ef\u4ee5\u4f7f\u7528 Onnx Runtime\u7684JavaScript Web \u5e94\u7528\u7a0b\u5e8f\u3002\u6b64\u9009\u9879\u5c06\u5728 Scikit-learn-trained \u6a21\u578b\u7684\u672a\u6765\u8bfe\u7a0b\u4e2d\u8fdb\u884c\u63a2\u8ba8\u3002</li> <li>\u4f7f\u7528 Lobe.ai \u6216 Azure \u81ea\u5b9a\u4e49\u89c6\u89c9\u3002\u5982\u679c\u4f60\u4f7f\u7528 ML SaaS\uff08\u8f6f\u4ef6\u5373\u670d\u52a1\uff09\u7cfb\u7edf\uff0c\u4f8b\u5982 Lobe.ai \u6216 Azure Custom Vision \u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u79cd\u7c7b\u578b\u7684\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u4e3a\u8bb8\u591a\u5e73\u53f0\u5bfc\u51fa\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u5b9a\u5236A PI\uff0c\u4f9b\u5728\u7ebf\u5e94\u7528\u7a0b\u5e8f\u5728\u4e91\u4e2d\u67e5\u8be2\u3002</li> </ul> <p>\u4f60\u8fd8\u6709\u673a\u4f1a\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684 Flask Web \u5e94\u7528\u7a0b\u5e8f\uff0c\u8be5\u5e94\u7528\u7a0b\u5e8f\u80fd\u591f\u5728 Web\u6d4f\u89c8\u5668\u4e2d\u8bad\u7ec3\u6a21\u578b\u672c\u8eab\u3002\u8fd9\u4e5f\u53ef\u4ee5\u5728 JavaScript \u4e0a\u4e0b\u6587\u4e2d\u4f7f\u7528 TensorFlow.js \u6765\u5b8c\u6210\u3002</p> <p>\u51fa\u4e8e\u6211\u4eec\u7684\u76ee\u7684\uff0c\u65e2\u7136\u6211\u4eec\u4e00\u76f4\u5728\u4f7f\u7528\u57fa\u4e8e Python \u7684 notebook\uff0c\u90a3\u4e48\u5c31\u8ba9\u6211\u4eec\u63a2\u8ba8\u4e00\u4e0b\u5c06\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u4ece notebook \u5bfc\u51fa\u4e3a Python \u6784\u5efa\u7684 web \u5e94\u7528\u7a0b\u5e8f\u53ef\u8bfb\u7684\u683c\u5f0f\u6240\u9700\u8981\u91c7\u53d6\u7684\u6b65\u9aa4\u3002</p>"},{"location":"3-Web-App/1-Web-App/#_4","title":"\u5de5\u5177","text":"<p>\u5bf9\u4e8e\u6b64\u4efb\u52a1\uff0c\u4f60\u9700\u8981\u4e24\u4e2a\u5de5\u5177\uff1aFlask \u548c Pickle\uff0c\u5b83\u4eec\u90fd\u5728 Python \u4e0a\u8fd0\u884c\u3002</p> <p>\u2705 \u4ec0\u4e48\u662f Flask\uff1f Flask \u88ab\u5176\u521b\u5efa\u8005\u5b9a\u4e49\u4e3a\u201c\u5fae\u6846\u67b6\u201d\uff0c\u5b83\u63d0\u4f9b\u4e86\u4f7f\u7528 Python \u548c\u6a21\u677f\u5f15\u64ce\u6784\u5efa\u7f51\u9875\u7684 Web \u6846\u67b6\u7684\u57fa\u672c\u529f\u80fd\u3002\u770b\u770b\u672c\u5b66\u4e60\u5355\u5143\u7ec3\u4e60\u4f7f\u7528 Flask \u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u3002</p> <p>\u2705 \u4ec0\u4e48\u662f Pickle\uff1f Pickle\ud83e\udd52\u662f\u4e00\u4e2a Python \u6a21\u5757\uff0c\u7528\u4e8e\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316 Python \u5bf9\u8c61\u7ed3\u6784\u3002\u5f53\u4f60\u201cpickle\u201d\u4e00\u4e2a\u6a21\u578b\u65f6\uff0c\u4f60\u5c06\u5176\u7ed3\u6784\u5e8f\u5217\u5316\u6216\u5c55\u5e73\u4ee5\u5728 Web \u4e0a\u4f7f\u7528\u3002\u5c0f\u5fc3\uff1apickle \u672c\u8d28\u4e0a\u4e0d\u662f\u5b89\u5168\u7684\uff0c\u6240\u4ee5\u5982\u679c\u63d0\u793a\u201cun-pickle\u201d\u6587\u4ef6\uff0c\u8bf7\u5c0f\u5fc3\u3002\u751f\u4ea7\u7684\u6587\u4ef6\u5177\u6709\u540e\u7f00 <code>.pkl</code>\u3002</p>"},{"location":"3-Web-App/1-Web-App/#-","title":"\u7ec3\u4e60 - \u6e05\u7406\u4f60\u7684\u6570\u636e","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u4f60\u5c06\u4f7f\u7528\u7531 NUFORC\uff08\u56fd\u5bb6 UFO \u62a5\u544a\u4e2d\u5fc3\uff09\u6536\u96c6\u7684 80,000 \u6b21 UFO \u76ee\u51fb\u6570\u636e\u3002\u8fd9\u4e9b\u6570\u636e\u5bf9 UFO \u76ee\u51fb\u4e8b\u4ef6\u6709\u4e00\u4e9b\u6709\u8da3\u7684\u63cf\u8ff0\uff0c\u4f8b\u5982\uff1a</p> <ul> <li>\u8be6\u7ec6\u63cf\u8ff0\u3002\"\u4e00\u540d\u7537\u5b50\u4ece\u591c\u95f4\u7167\u5c04\u5728\u8349\u5730\u4e0a\u7684\u5149\u675f\u4e2d\u51fa\u73b0\uff0c\u4ed6\u671d\u5fb7\u514b\u8428\u65af\u4eea\u5668\u516c\u53f8\u7684\u505c\u8f66\u573a\u8dd1\u53bb\"\u3002</li> <li>\u7b80\u77ed\u63cf\u8ff0\u3002 \u201c\u706f\u5149\u8ffd\u7740\u6211\u4eec\u201d\u3002</li> </ul> <p>ufos.csv \u7535\u5b50\u8868\u683c\u5305\u62ec\u6709\u5173\u76ee\u51fb\u4e8b\u4ef6\u53d1\u751f\u7684 <code>city</code>\u3001<code>state</code> \u548c <code>country</code>\u3001\u5bf9\u8c61\u7684 <code>shape</code> \u53ca\u5176 <code>latitude</code> \u548c <code>longitude</code> \u7684\u5217\u3002</p> <p>\u5728\u5305\u542b\u5728\u672c\u8bfe\u4e2d\u7684\u7a7a\u767d notebook \u4e2d\uff1a</p> <ol> <li> <p>\u50cf\u5728\u4e4b\u524d\u7684\u8bfe\u7a0b\u4e2d\u4e00\u6837\u5bfc\u5165 <code>pandas</code>\u3001<code>matplotlib</code> \u548c <code>numpy</code>\uff0c\u7136\u540e\u5bfc\u5165 ufos \u7535\u5b50\u8868\u683c\u3002\u4f60\u53ef\u4ee5\u67e5\u770b\u4e00\u4e2a\u793a\u4f8b\u6570\u636e\u96c6\uff1a</p> <pre><code>import pandas as pd\nimport numpy as np\n\nufos = pd.read_csv('data/ufos.csv')\nufos.head()\n</code></pre> </li> <li> <p>\u5c06 ufos \u6570\u636e\u8f6c\u6362\u4e3a\u5e26\u6709\u65b0\u6807\u9898\u7684\u5c0f dataframe\u3002\u68c0\u67e5 <code>country</code> \u5b57\u6bb5\u4e2d\u7684\u552f\u4e00\u503c\u3002</p> <pre><code>ufos = pd.DataFrame({'Seconds': ufos['duration (seconds)'], 'Country': ufos['country'],'Latitude': ufos['latitude'],'Longitude': ufos['longitude']})\n\nufos.Country.unique()\n</code></pre> </li> <li> <p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5220\u9664\u4efb\u4f55\u7a7a\u503c\u5e76\u4ec5\u5bfc\u5165 1-60 \u79d2\u4e4b\u95f4\u7684\u76ee\u51fb\u6570\u636e\u6765\u51cf\u5c11\u6211\u4eec\u9700\u8981\u5904\u7406\u7684\u6570\u636e\u91cf\uff1a</p> <pre><code>ufos.dropna(inplace=True)\n\nufos = ufos[(ufos['Seconds'] &gt;= 1) &amp; (ufos['Seconds'] &lt;= 60)]\n\nufos.info()\n</code></pre> </li> <li> <p>\u5bfc\u5165 Scikit-learn \u7684 <code>LabelEncoder</code> \u5e93\uff0c\u5c06\u56fd\u5bb6\u7684\u6587\u672c\u503c\u8f6c\u6362\u4e3a\u6570\u5b57\uff1a</p> </li> </ol> <p>\u2705 LabelEncoder \u6309\u5b57\u6bcd\u987a\u5e8f\u7f16\u7801\u6570\u636e</p> <pre><code>```python\nfrom sklearn.preprocessing import LabelEncoder\n\nufos['Country'] = LabelEncoder().fit_transform(ufos['Country'])\n\nufos.head()\n```\n\n\u4f60\u7684\u6570\u636e\u5e94\u5982\u4e0b\u6240\u793a\uff1a\n\n```output\n    Seconds Country Latitude    Longitude\n2   20.0    3       53.200000   -2.916667\n3   20.0    4       28.978333   -96.645833\n14  30.0    4       35.823889   -80.253611\n23  60.0    4       45.582778   -122.352222\n24  3.0     3       51.783333   -0.783333\n```\n</code></pre>"},{"location":"3-Web-App/1-Web-App/#-_1","title":"\u7ec3\u4e60 - \u5efa\u7acb\u4f60\u7684\u6a21\u578b","text":"<p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5c06\u6570\u636e\u5212\u5206\u4e3a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7ec4\u6765\u51c6\u5907\u8bad\u7ec3\u6a21\u578b\u3002</p> <ol> <li> <p>\u9009\u62e9\u8981\u8bad\u7ec3\u7684\u4e09\u4e2a\u7279\u5f81\u4f5c\u4e3a X \u5411\u91cf\uff0cy \u5411\u91cf\u5c06\u662f <code>Country</code> \u4f60\u5e0c\u671b\u80fd\u591f\u8f93\u5165 <code>Seconds</code>\u3001<code>Latitude</code> \u548c <code>Longitude</code> \u5e76\u83b7\u5f97\u8981\u8fd4\u56de\u7684\u56fd\u5bb6/\u5730\u533a ID\u3002</p> <pre><code>from sklearn.model_selection import train_test_split\n\nSelected_features = ['Seconds','Latitude','Longitude']\n\nX = ufos[Selected_features]\ny = ufos['Country']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n</code></pre> </li> <li> <p>\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u8bad\u7ec3\u6a21\u578b\uff1a</p> <pre><code>from sklearn.metrics import accuracy_score, classification_report \nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nprint('Predicted labels: ', predictions)\nprint('Accuracy: ', accuracy_score(y_test, predictions))\n</code></pre> </li> </ol> <p>\u51c6\u786e\u7387\u8fd8\u4e0d\u9519 \uff08\u5927\u7ea6 95%\uff09\uff0c\u4e0d\u51fa\u6240\u6599\uff0c\u56e0\u4e3a <code>Country</code> \u548c <code>Latitude/Longitude</code> \u76f8\u5173\u3002</p> <p>\u4f60\u521b\u5efa\u7684\u6a21\u578b\u5e76\u4e0d\u662f\u975e\u5e38\u5177\u6709\u9769\u547d\u6027\uff0c\u56e0\u4e3a\u4f60\u5e94\u8be5\u80fd\u591f\u4ece\u5176 <code>Latitude</code> \u548c <code>Longitude</code> \u63a8\u65ad\u51fa <code>Country</code>\uff0c\u4f46\u662f\uff0c\u5c1d\u8bd5\u4ece\u6e05\u7406\u3001\u5bfc\u51fa\u7684\u539f\u59cb\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5728 web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u6b64\u6a21\u578b\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7ec3\u4e60\u3002</p>"},{"location":"3-Web-App/1-Web-App/#-pickle","title":"\u7ec3\u4e60 - \u201cpickle\u201d\u4f60\u7684\u6a21\u578b","text":"<p>\u73b0\u5728\uff0c\u662f\u65f6\u5019 pickle \u4f60\u7684\u6a21\u578b\u4e86\uff01\u4f60\u53ef\u4ee5\u5728\u51e0\u884c\u4ee3\u7801\u4e2d\u505a\u5230\u8fd9\u4e00\u70b9\u3002\u4e00\u65e6\u5b83\u662f pickled\uff0c\u52a0\u8f7d\u4f60\u7684 pickled \u6a21\u578b\u5e76\u9488\u5bf9\u5305\u542b\u79d2\u3001\u7eac\u5ea6\u548c\u7ecf\u5ea6\u503c\u7684\u793a\u4f8b\u6570\u636e\u6570\u7ec4\u5bf9\u5176\u8fdb\u884c\u6d4b\u8bd5\uff0c</p> <pre><code>import pickle\nmodel_filename = 'ufo-model.pkl'\npickle.dump(model, open(model_filename,'wb'))\n\nmodel = pickle.load(open('ufo-model.pkl','rb'))\nprint(model.predict([[50,44,-12]]))\n</code></pre> <p>\u8be5\u6a21\u578b\u8fd4\u56de '3'\uff0c\u8fd9\u662f\u82f1\u56fd\u7684\u56fd\u5bb6\u4ee3\u7801\u3002\ud83d\udc7d</p>"},{"location":"3-Web-App/1-Web-App/#-flask","title":"\u7ec3\u4e60 - \u6784\u5efaFlask\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u73b0\u5728\u4f60\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2aFlask\u5e94\u7528\u7a0b\u5e8f\u6765\u8c03\u7528\u4f60\u7684\u6a21\u578b\u5e76\u8fd4\u56de\u7c7b\u4f3c\u7684\u7ed3\u679c\uff0c\u4f46\u4ee5\u4e00\u79cd\u66f4\u7f8e\u89c2\u7684\u65b9\u5f0f\u3002</p> <ol> <li> <p>\u9996\u5148\u5728\u4f60\u7684 ufo-model.pkl \u6587\u4ef6\u6240\u5728\u7684 notebook.ipynb \u6587\u4ef6\u65c1\u8fb9\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a web-app \u7684\u6587\u4ef6\u5939\u3002</p> </li> <li> <p>\u5728\u8be5\u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u53e6\u5916\u4e09\u4e2a\u6587\u4ef6\u5939\uff1astatic\uff0c\u5176\u4e2d\u6709\u6587\u4ef6\u5939 css \u548c templates\u3002 \u4f60\u73b0\u5728\u5e94\u8be5\u62e5\u6709\u4ee5\u4e0b\u6587\u4ef6\u548c\u76ee\u5f55</p> <pre><code>web-app/\n  static/\n    css/\n  templates/\nnotebook.ipynb\nufo-model.pkl\n</code></pre> </li> </ol> <p>\u2705 \u8bf7\u53c2\u9605\u89e3\u51b3\u65b9\u6848\u6587\u4ef6\u5939\u4ee5\u67e5\u770b\u5df2\u5b8c\u6210\u7684\u5e94\u7528\u7a0b\u5e8f</p> <ol> <li> <p>\u5728 web-app \u6587\u4ef6\u5939\u4e2d\u521b\u5efa\u7684\u7b2c\u4e00\u4e2a\u6587\u4ef6\u662f requirements.txt \u6587\u4ef6\u3002\u4e0e JavaScript \u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684 package.json \u4e00\u6837\uff0c\u6b64\u6587\u4ef6\u5217\u51fa\u4e86\u5e94\u7528\u7a0b\u5e8f\u6240\u9700\u7684\u4f9d\u8d56\u9879\u3002\u5728 requirements.txt \u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u51e0\u884c\uff1a  </p> <pre><code>scikit-learn\npandas\nnumpy\nflask\n</code></pre> </li> <li> <p>\u73b0\u5728\uff0c\u8fdb\u5165 web-app \u6587\u4ef6\u5939\uff1a</p> </li> </ol> <pre><code>cd web-app\n</code></pre> <ol> <li>\u5728\u4f60\u7684\u7ec8\u7aef\u4e2d\u8f93\u5165 <code>pip install</code>\uff0c\u4ee5\u5b89\u88c5 reuirements.txt \u4e2d\u5217\u51fa\u7684\u5e93\uff1a</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li> <p>\u73b0\u5728\uff0c\u4f60\u5df2\u51c6\u5907\u597d\u521b\u5efa\u53e6\u5916\u4e09\u4e2a\u6587\u4ef6\u6765\u5b8c\u6210\u5e94\u7528\u7a0b\u5e8f\uff1a</p> <ol> <li>\u5728\u6839\u76ee\u5f55\u4e2d\u521b\u5efa app.py\u3002</li> <li>\u5728 templates \u76ee\u5f55\u4e2d\u521b\u5efaindex.html\u3002</li> <li>\u5728 static/css \u76ee\u5f55\u4e2d\u521b\u5efastyles.css\u3002</li> </ol> </li> <li> <p>\u4f7f\u7528\u4e00\u4e9b\u6837\u5f0f\u6784\u5efa styles.css \u6587\u4ef6\uff1a</p> <pre><code>body {\n    width: 100%;\n    height: 100%;\n    font-family: 'Helvetica';\n    background: black;\n    color: #fff;\n    text-align: center;\n    letter-spacing: 1.4px;\n    font-size: 30px;\n}\n\ninput {\n    min-width: 150px;\n}\n\n.grid {\n    width: 300px;\n    border: 1px solid #2d2d2d;\n    display: grid;\n    justify-content: center;\n    margin: 20px auto;\n}\n\n.box {\n    color: #fff;\n    background: #2d2d2d;\n    padding: 12px;\n    display: inline-block;\n}\n</code></pre> </li> <li> <p>\u63a5\u4e0b\u6765\uff0c\u6784\u5efa index.html \u6587\u4ef6\uff1a</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;\ud83d\udef8 UFO Appearance Prediction! \ud83d\udc7d&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/styles.css') }}\"&gt; \n&lt;/head&gt;\n\n&lt;body&gt;\n &lt;div class=\"grid\"&gt;\n\n  &lt;div class=\"box\"&gt;\n\n  &lt;p&gt;According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?&lt;/p&gt;\n\n    &lt;form action=\"{{ url_for('predict')}}\" method=\"post\"&gt;\n        &lt;input type=\"number\" name=\"seconds\" placeholder=\"Seconds\" required=\"required\" min=\"0\" max=\"60\" /&gt;\n      &lt;input type=\"text\" name=\"latitude\" placeholder=\"Latitude\" required=\"required\" /&gt;\n          &lt;input type=\"text\" name=\"longitude\" placeholder=\"Longitude\" required=\"required\" /&gt;\n      &lt;button type=\"submit\" class=\"btn\"&gt;Predict country where the UFO is seen&lt;/button&gt;\n    &lt;/form&gt;\n\n\n   &lt;p&gt;{{ prediction_text }}&lt;/p&gt;\n\n &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>\u770b\u770b\u8fd9\u4e2a\u6587\u4ef6\u4e2d\u7684\u6a21\u677f\u3002\u8bf7\u6ce8\u610f\u5e94\u7528\u7a0b\u5e8f\u5c06\u63d0\u4f9b\u7684\u53d8\u91cf\u5468\u56f4\u7684\u201cmustache\u201d\u8bed\u6cd5\uff0c\u4f8b\u5982\u9884\u6d4b\u6587\u672c\uff1a<code>{{}}</code>\u3002\u8fd8\u6709\u4e00\u4e2a\u8868\u5355\u53ef\u4ee5\u5c06\u9884\u6d4b\u53d1\u5e03\u5230 <code>/predict</code> \u8def\u7531\u3002</p> <p>\u6700\u540e\uff0c\u4f60\u5df2\u51c6\u5907\u597d\u6784\u5efa\u4f7f\u7528\u6a21\u578b\u548c\u663e\u793a\u9884\u6d4b\u7684 python \u6587\u4ef6\uff1a</p> </li> <li> <p>\u5728<code>app.py</code>\u4e2d\u6dfb\u52a0:</p> <pre><code>import numpy as np\nfrom flask import Flask, request, render_template\nimport pickle\n\napp = Flask(__name__)\n\nmodel = pickle.load(open(\"./ufo-model.pkl\", \"rb\"))\n\n\n@app.route(\"/\")\ndef home():\n    return render_template(\"index.html\")\n\n\n@app.route(\"/predict\", methods=[\"POST\"])\ndef predict():\n\n    int_features = [int(x) for x in request.form.values()]\n    final_features = [np.array(int_features)]\n    prediction = model.predict(final_features)\n\n    output = prediction[0]\n\n    countries = [\"Australia\", \"Canada\", \"Germany\", \"UK\", \"US\"]\n\n    return render_template(\n        \"index.html\", prediction_text=\"Likely country: {}\".format(countries[output])\n    )\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre> </li> </ol> <p>\ud83d\udca1 \u63d0\u793a\uff1a\u5f53\u4f60\u5728\u4f7f\u7528 Flask \u8fd0\u884c Web \u5e94\u7528\u7a0b\u5e8f\u65f6\u6dfb\u52a0 <code>debug=True</code>\u65f6\u4f60\u5bf9\u5e94\u7528\u7a0b\u5e8f\u6240\u505a\u7684\u4efb\u4f55\u66f4\u6539\u5c06\u7acb\u5373\u53cd\u6620\uff0c\u65e0\u9700\u91cd\u65b0\u542f\u52a8\u670d\u52a1\u5668\u3002\u6ce8\u610f\uff01\u4e0d\u8981\u5728\u751f\u4ea7\u5e94\u7528\u7a0b\u5e8f\u4e2d\u542f\u7528\u6b64\u6a21\u5f0f</p> <p>\u5982\u679c\u4f60\u8fd0\u884c <code>python app.py</code> \u6216 <code>python3 app.py</code> - \u4f60\u7684\u7f51\u7edc\u670d\u52a1\u5668\u5728\u672c\u5730\u542f\u52a8\uff0c\u4f60\u53ef\u4ee5\u586b\u5199\u4e00\u4e2a\u7b80\u77ed\u7684\u8868\u683c\u6765\u56de\u7b54\u4f60\u5173\u4e8e\u5728\u54ea\u91cc\u770b\u5230 UFO \u7684\u95ee\u9898\uff01</p> <p>\u5728\u6b64\u4e4b\u524d\uff0c\u5148\u770b\u4e00\u4e0b <code>app.py</code> \u7684\u5b9e\u73b0\uff1a</p> <ol> <li>\u9996\u5148\uff0c\u52a0\u8f7d\u4f9d\u8d56\u9879\u5e76\u542f\u52a8\u5e94\u7528\u7a0b\u5e8f\u3002</li> <li>\u7136\u540e\uff0c\u5bfc\u5165\u6a21\u578b\u3002</li> <li>\u7136\u540e\uff0c\u5728 home \u8def\u7531\u4e0a\u6e32\u67d3 index.html\u3002</li> </ol> <p>\u5728 <code>/predict</code> \u8def\u7531\u4e0a\uff0c\u5f53\u8868\u5355\u88ab\u53d1\u5e03\u65f6\u4f1a\u53d1\u751f\u51e0\u4ef6\u4e8b\u60c5\uff1a </p> <ol> <li>\u6536\u96c6\u8868\u5355\u53d8\u91cf\u5e76\u8f6c\u6362\u4e3a numpy \u6570\u7ec4\u3002\u7136\u540e\u5c06\u5b83\u4eec\u53d1\u9001\u5230\u6a21\u578b\u5e76\u8fd4\u56de\u9884\u6d4b\u3002</li> <li>\u6211\u4eec\u5e0c\u671b\u663e\u793a\u7684\u56fd\u5bb6/\u5730\u533a\u6839\u636e\u5176\u9884\u6d4b\u7684\u56fd\u5bb6/\u5730\u533a\u4ee3\u7801\u91cd\u65b0\u5448\u73b0\u4e3a\u53ef\u8bfb\u6587\u672c\uff0c\u5e76\u5c06\u8be5\u503c\u53d1\u9001\u56de index.html \u4ee5\u5728\u6a21\u677f\u4e2d\u5448\u73b0\u3002</li> </ol> <p>\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u4f7f\u7528\u6a21\u578b\uff0c\u5305\u62ec Flask \u548c pickled \u6a21\u578b\uff0c\u662f\u76f8\u5bf9\u7b80\u5355\u7684\u3002\u6700\u56f0\u96be\u7684\u662f\u8981\u7406\u89e3\u6570\u636e\u662f\u4ec0\u4e48\u5f62\u72b6\u7684\uff0c\u8fd9\u4e9b\u6570\u636e\u5fc5\u987b\u53d1\u9001\u5230\u6a21\u578b\u4e2d\u624d\u80fd\u5f97\u5230\u9884\u6d4b\u3002\u8fd9\u5b8c\u5168\u53d6\u51b3\u4e8e\u6a21\u578b\u662f\u5982\u4f55\u8bad\u7ec3\u7684\u3002\u6709\u4e09\u4e2a\u6570\u636e\u8981\u8f93\u5165\uff0c\u4ee5\u4fbf\u5f97\u5230\u4e00\u4e2a\u9884\u6d4b\u3002</p> <p>\u5728\u4e00\u4e2a\u4e13\u4e1a\u7684\u73af\u5883\u4e2d\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u8bad\u7ec3\u6a21\u578b\u7684\u4eba\u548c\u5728 Web \u6216\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u6a21\u578b\u7684\u4eba\u4e4b\u95f4\u7684\u826f\u597d\u6c9f\u901a\u662f\u591a\u4e48\u7684\u5fc5\u8981\u3002\u5728\u6211\u4eec\u7684\u60c5\u51b5\u4e0b\uff0c\u53ea\u6709\u4e00\u4e2a\u4eba\uff0c\u4f60\uff01</p>"},{"location":"3-Web-App/1-Web-App/#_5","title":"\ud83d\ude80 \u6311\u6218","text":"<p>\u4f60\u53ef\u4ee5\u5728 Flask \u5e94\u7528\u7a0b\u5e8f\u4e2d\u8bad\u7ec3\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u5728 notebook \u4e0a\u5de5\u4f5c\u5e76\u5c06\u6a21\u578b\u5bfc\u5165 Flask \u5e94\u7528\u7a0b\u5e8f\uff01\u5c1d\u8bd5\u5728 notebook \u4e2d\u8f6c\u6362 Python \u4ee3\u7801\uff0c\u53ef\u80fd\u662f\u5728\u6e05\u9664\u6570\u636e\u4e4b\u540e\uff0c\u4ece\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u4e00\u4e2a\u540d\u4e3a <code>train</code> \u7684\u8def\u5f84\u8bad\u7ec3\u6a21\u578b\u3002\u91c7\u7528\u8fd9\u79cd\u65b9\u6cd5\u7684\u5229\u5f0a\u662f\u4ec0\u4e48\uff1f</p>"},{"location":"3-Web-App/1-Web-App/#_6","title":"\u8bfe\u540e\u6d4b","text":""},{"location":"3-Web-App/1-Web-App/#_7","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u6709\u5f88\u591a\u65b9\u6cd5\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2aWeb\u5e94\u7528\u7a0b\u5e8f\u6765\u4f7f\u7528ML\u6a21\u578b\u3002\u5217\u51fa\u53ef\u4ee5\u4f7f\u7528JavaScript\u6216Python\u6784\u5efaWeb\u5e94\u7528\u7a0b\u5e8f\u4ee5\u5229\u7528\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u8003\u8651\u67b6\u6784\uff1a\u6a21\u578b\u5e94\u8be5\u7559\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u8fd8\u662f\u5b58\u5728\u4e8e\u4e91\u4e2d\uff1f\u5982\u679c\u662f\u540e\u8005\uff0c\u4f60\u5c06\u5982\u4f55\u8bbf\u95ee\u5b83\uff1f\u4e3a\u5e94\u7528\u7684ML Web\u89e3\u51b3\u65b9\u6848\u7ed8\u5236\u67b6\u6784\u6a21\u578b\u3002</p>"},{"location":"3-Web-App/1-Web-App/#_8","title":"\u4efb\u52a1","text":"<p>\u5c1d\u8bd5\u4e0d\u540c\u7684\u6a21\u578b</p>"},{"location":"3-Web-App/1-Web-App/assignment/","title":"\u5c1d\u8bd5\u4e0d\u540c\u7684\u6a21\u578b","text":""},{"location":"3-Web-App/1-Web-App/assignment/#_2","title":"\u8bf4\u660e","text":"<p>\u73b0\u5728\uff0c\u4f60\u5df2\u7ecf\u80fd\u591f\u4f7f\u7528\u4e00\u4e2a\u7ecf\u8fc7\u8bad\u7ec3\u7684\u56de\u5f52\u6a21\u578b\u6765\u642d\u5efaweb\u5e94\u7528\u7a0b\u5e8f\uff0c\u90a3\u4e48\u8bf7\u4f60\u4ece\u524d\u9762\u7684\u56de\u5f52\u8bfe\u7a0b\u4e2d\u91cd\u65b0\u9009\u62e9\u4e00\u4e2a\u6a21\u578b\u6765\u91cd\u505a\u4e00\u904dweb\u5e94\u7528\u7a0b\u5e8f\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u539f\u6765\u7684\u98ce\u683c\u6216\u8005\u5176\u4ed6\u4e0d\u540c\u7684\u98ce\u683c\u8fdb\u884c\u8bbe\u8ba1\uff0c\u6765\u5c55\u793apumpkin\u6570\u636e\u3002\u6ce8\u610f\u66f4\u6539\u8f93\u5165\u4ee5\u53cd\u6620\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002</p>"},{"location":"3-Web-App/1-Web-App/assignment/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b web\u5e94\u7528\u7a0b\u5e8f\u6309\u9884\u671f\u8fd0\u884c\uff0c\u5e76\u90e8\u7f72\u5230\u4e91\u7aef web\u5e94\u7528\u7a0b\u5e8f\u5b58\u5728\u7f3a\u9677\u6216\u8005\u663e\u793a\u610f\u60f3\u4e0d\u5230\u7684\u7ed3\u679c web\u5e94\u7528\u7a0b\u5e8f\u65e0\u6cd5\u6b63\u5e38\u8fd0\u884c"},{"location":"4-Classification/","title":"Getting started with classification","text":""},{"location":"4-Classification/#regional-topic-delicious-asian-and-indian-cuisines","title":"Regional topic: Delicious Asian and Indian Cuisines \ud83c\udf5c","text":"<p>In Asia and India, food traditions are extremely diverse, and very delicious! Let's look at data about regional cuisines to try to understand their ingredients.</p> <p></p> <p>Photo by Lisheng Chang on Unsplash</p>"},{"location":"4-Classification/#what-you-will-learn","title":"What you will learn","text":"<p>In this section, you will build on your earlier study of Regression and learn about other classifiers that you can use to better understand the data.</p> <p>There are useful low-code tools that can help you learn about working with classification models. Try Azure ML for this task</p>"},{"location":"4-Classification/#lessons","title":"Lessons","text":"<ol> <li>Introduction to classification</li> <li>More classifiers</li> <li>Yet other classifiers</li> <li>Applied ML: build a web app</li> </ol>"},{"location":"4-Classification/#credits","title":"Credits","text":"<p>\"Getting started with classification\" was written with \u2665\ufe0f by Cassie Breviu and Jen Looper</p> <p>The delicious cuisines dataset was sourced from Kaggle.</p>"},{"location":"4-Classification/README.zh-cn/","title":"\u5f00\u59cb\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5","text":""},{"location":"4-Classification/README.zh-cn/#_2","title":"\u5730\u65b9\u6027\u7684\u8bdd\u9898\uff1a\u7f8e\u5473\u7684\u4e9a\u6d32\u4e0e\u5370\u5ea6\u83dc\u80b4 \ud83c\udf5c","text":"<p>\u65e0\u8bba\u662f\u5728\u4e9a\u6d32\u4ea6\u6216\u662f\u5728\u5370\u5ea6\uff0c\u996e\u98df\u98ce\u4fd7\u5728\u7f8e\u5473\u65e0\u6bd4\u7684\u540c\u65f6\uff0c\u53c8\u5728\u4e0d\u540c\u7684\u5730\u533a\u5404\u5177\u7279\u8272\u3002\u8ba9\u6211\u4eec\u6765\u770b\u4e00\u4e9b\u5730\u65b9\u7f8e\u98df\u7684\u6570\u636e\uff0c\u5e76\u5c1d\u8bd5\u7406\u89e3\u4e00\u4e0b\u4ed6\u4eec\u7684\u539f\u6599\u3002</p> <p></p> <p>\u56fe\u7247\u7531 Lisheng Chang \u63d0\u4f9b\uff0c\u6765\u81ea Unsplash</p>"},{"location":"4-Classification/README.zh-cn/#_3","title":"\u4f60\u4f1a\u5b66\u5230\u4ec0\u4e48","text":"<p>\u5efa\u7acb\u5728\u6211\u4eec\u4e4b\u524d\u5173\u4e8e\u56de\u5f52\u95ee\u9898\u7684\u8ba8\u8bba\u57fa\u7840\u4e0a\uff0c\u5728\u672c\u5c0f\u8282\u4e2d\uff0c\u4f60\u5c06\u7ee7\u7eed\u5b66\u4e60\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u597d\u5730\u7406\u89e3\u6570\u636e\u7684\u5404\u79cd\u5206\u7c7b\u5668\u3002</p> <p>\u8fd9\u91cc\u6709\u4e00\u4e9b\u4e0d\u592a\u6d89\u53ca\u4ee3\u7801\uff0c\u4e14\u80fd\u5e2e\u52a9\u4f60\u4e86\u89e3\u5982\u4f55\u4f7f\u7528\u5206\u7c7b\u6a21\u578b\u7684\u5c0f\u5de5\u5177\u3002\u53ef\u4ee5\u8bd5\u8bd5\u7528 Azure \u6765\u5b8c\u6210\u8fd9\u4e2a\u5c0f\u4efb\u52a1\u3002</p>"},{"location":"4-Classification/README.zh-cn/#_4","title":"\u8bfe\u7a0b","text":"<ol> <li>\u4ecb\u7ecd\u5206\u7c7b\u65b9\u6cd5</li> <li>\u5176\u4ed6\u5206\u7c7b\u5668</li> <li>\u66f4\u591a\u5176\u4ed6\u7684\u5206\u7c7b\u5668</li> <li>\u5e94\u7528\u673a\u5668\u5b66\u4e60\uff1a\u505a\u4e00\u4e2a\u7f51\u9875 APP</li> </ol>"},{"location":"4-Classification/README.zh-cn/#_5","title":"\u81f4\u8c22","text":"<p>\u201c\u5f00\u59cb\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\u201d\u90e8\u5206\u7531 Cassie Breviu \u548c Jen Looper \u7528 \u2665\ufe0f \u5199\u4f5c\u3002</p> <p>\u8fd9\u4e9b\u7f8e\u98df\u7684\u6570\u636e\u96c6\u6765\u6e90\u4e8e Kaggle\u3002</p>"},{"location":"4-Classification/1-Introduction/","title":"Introduction to classification","text":"<p>In these four lessons, you will explore a fundamental focus of classic machine learning - classification. We will walk through using various classification algorithms with a dataset about all the brilliant cuisines of Asia and India. Hope you're hungry!</p> <p></p> <p>Celebrate pan-Asian cuisines in these lessons! Image by Jen Looper</p> <p>Classification is a form of supervised learning that bears a lot in common with regression techniques. If machine learning is all about predicting values or names to things by using datasets, then classification generally falls into two groups: binary classification and multiclass classification.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: MIT's John Guttag introduces classification</p> <p>Remember:</p> <ul> <li>Linear regression helped you predict relationships between variables and make accurate predictions on where a new datapoint would fall in relationship to that line. So, you could predict what price a pumpkin would be in September vs. December, for example.</li> <li>Logistic regression helped you discover \"binary categories\": at this price point, is this pumpkin orange or not-orange?</li> </ul> <p>Classification uses various algorithms to determine other ways of determining a data point's label or class. Let's work with this cuisine data to see whether, by observing a group of ingredients, we can determine its cuisine of origin.</p>"},{"location":"4-Classification/1-Introduction/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"4-Classification/1-Introduction/#this-lesson-is-available-in-r","title":"This lesson is available in R!","text":""},{"location":"4-Classification/1-Introduction/#introduction","title":"Introduction","text":"<p>Classification is one of the fundamental activities of the machine learning researcher and data scientist. From basic classification of a binary value (\"is this email spam or not?\"), to complex image classification and segmentation using computer vision, it's always useful to be able to sort data into classes and ask questions of it.</p> <p>To state the process in a more scientific way, your classification method creates a predictive model that enables you to map the relationship between input variables to output variables.</p> <p></p> <p>Binary vs. multiclass problems for classification algorithms to handle. Infographic by Jen Looper</p> <p>Before starting the process of cleaning our data, visualizing it, and prepping it for our ML tasks, let's learn a bit about the various ways machine learning can be leveraged to classify data.</p> <p>Derived from statistics, classification using classic machine learning uses features, such as <code>smoker</code>, <code>weight</code>, and <code>age</code> to determine likelihood of developing X disease. As a supervised learning technique similar to the regression exercises you performed earlier, your data is labeled and the ML algorithms use those labels to classify and predict classes (or 'features') of a dataset and assign them to a group or outcome.</p> <p>\u2705 Take a moment to imagine a dataset about cuisines. What would a multiclass model be able to answer? What would a binary model be able to answer? What if you wanted to determine whether a given cuisine was likely to use fenugreek? What if you wanted to see if, given a present of a grocery bag full of star anise, artichokes, cauliflower, and horseradish, you could create a typical Indian dish?</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video.The whole premise of the show 'Chopped' is the 'mystery basket' where chefs have to make some dish out of a random choice of ingredients. Surely a ML model would have helped!</p>"},{"location":"4-Classification/1-Introduction/#hello-classifier","title":"Hello 'classifier'","text":"<p>The question we want to ask of this cuisine dataset is actually a multiclass question, as we have several potential national cuisines to work with. Given a batch of ingredients, which of these many classes will the data fit?</p> <p>Scikit-learn offers several different algorithms to use to classify data, depending on the kind of problem you want to solve. In the next two lessons, you'll learn about several of these algorithms.</p>"},{"location":"4-Classification/1-Introduction/#exercise-clean-and-balance-your-data","title":"Exercise - clean and balance your data","text":"<p>The first task at hand, before starting this project, is to clean and balance your data to get better results. Start with the blank notebook.ipynb file in the root of this folder.</p> <p>The first thing to install is imblearn. This is a Scikit-learn package that will allow you to better balance the data (you will learn more about this task in a minute).</p> <ol> <li> <p>To install <code>imblearn</code>, run <code>pip install</code>, like so:</p> <pre><code>pip install imblearn\n</code></pre> </li> <li> <p>Import the packages you need to import your data and visualize it, also import <code>SMOTE</code> from <code>imblearn</code>.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\n</code></pre> <p>Now you are set up to read import the data next.</p> </li> <li> <p>The next task will be to import the data:</p> <pre><code>df  = pd.read_csv('../data/cuisines.csv')\n</code></pre> </li> </ol> <p>Using <code>read_csv()</code> will read the content of the csv file cusines.csv and place it in the variable <code>df</code>.</p> <ol> <li> <p>Check the data's shape:</p> <pre><code>df.head()\n</code></pre> </li> </ol> <p>The first five rows look like this:</p> <pre><code>```output\n|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n| 0   | 65         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 1   | 66         | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 2   | 67         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 3   | 68         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 4   | 69         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n```\n</code></pre> <ol> <li> <p>Get info about this data by calling <code>info()</code>:</p> <pre><code>df.info()\n</code></pre> <p>Your out resembles:</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2448 entries, 0 to 2447\nColumns: 385 entries, Unnamed: 0 to zucchini\ndtypes: int64(384), object(1)\nmemory usage: 7.2+ MB\n</code></pre> </li> </ol>"},{"location":"4-Classification/1-Introduction/#exercise-learning-about-cuisines","title":"Exercise - learning about cuisines","text":"<p>Now the work starts to become more interesting. Let's discover the distribution of data, per cuisine </p> <ol> <li> <p>Plot the data as bars by calling <code>barh()</code>:</p> <pre><code>df.cuisine.value_counts().plot.barh()\n</code></pre> <p></p> <p>There are a finite number of cuisines, but the distribution of data is uneven. You can fix that! Before doing so, explore a little more. </p> </li> <li> <p>Find out how much data is available per cuisine and print it out:</p> <pre><code>thai_df = df[(df.cuisine == \"thai\")]\njapanese_df = df[(df.cuisine == \"japanese\")]\nchinese_df = df[(df.cuisine == \"chinese\")]\nindian_df = df[(df.cuisine == \"indian\")]\nkorean_df = df[(df.cuisine == \"korean\")]\n\nprint(f'thai df: {thai_df.shape}')\nprint(f'japanese df: {japanese_df.shape}')\nprint(f'chinese df: {chinese_df.shape}')\nprint(f'indian df: {indian_df.shape}')\nprint(f'korean df: {korean_df.shape}')\n</code></pre> <p>the output looks like so:</p> <pre><code>thai df: (289, 385)\njapanese df: (320, 385)\nchinese df: (442, 385)\nindian df: (598, 385)\nkorean df: (799, 385)\n</code></pre> </li> </ol>"},{"location":"4-Classification/1-Introduction/#discovering-ingredients","title":"Discovering ingredients","text":"<p>Now you can dig deeper into the data and learn what are the typical ingredients per cuisine. You should clean out recurrent data that creates confusion between cuisines, so let's learn about this problem.</p> <ol> <li> <p>Create a function <code>create_ingredient()</code> in Python to create an ingredient dataframe. This function will start by dropping an unhelpful column and sort through ingredients by their count:</p> <pre><code>def create_ingredient_df(df):\n    ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')\n    ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]\n    ingredient_df = ingredient_df.sort_values(by='value', ascending=False,\n    inplace=False)\n    return ingredient_df\n</code></pre> </li> </ol> <p>Now you can use that function to get an idea of top ten most popular ingredients by cuisine.</p> <ol> <li> <p>Call <code>create_ingredient()</code> and plot it calling <code>barh()</code>:</p> <pre><code>thai_ingredient_df = create_ingredient_df(thai_df)\nthai_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>Do the same for the japanese data:</p> <pre><code>japanese_ingredient_df = create_ingredient_df(japanese_df)\njapanese_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>Now for the chinese ingredients:</p> <pre><code>chinese_ingredient_df = create_ingredient_df(chinese_df)\nchinese_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>Plot the indian ingredients:</p> <pre><code>indian_ingredient_df = create_ingredient_df(indian_df)\nindian_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>Finally, plot the korean ingredients:</p> <pre><code>korean_ingredient_df = create_ingredient_df(korean_df)\nkorean_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>Now, drop the most common ingredients that create confusion between distinct cuisines, by calling <code>drop()</code>: </p> </li> </ol> <p>Everyone loves rice, garlic and ginger!</p> <pre><code>```python\nfeature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)\nlabels_df = df.cuisine #.unique()\nfeature_df.head()\n```\n</code></pre>"},{"location":"4-Classification/1-Introduction/#balance-the-dataset","title":"Balance the dataset","text":"<p>Now that you have cleaned the data, use SMOTE - \"Synthetic Minority Over-sampling Technique\" - to balance it.</p> <ol> <li> <p>Call <code>fit_resample()</code>, this strategy generates new samples by interpolation.</p> <pre><code>oversample = SMOTE()\ntransformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)\n</code></pre> <p>By balancing your data, you'll have better results when classifying it. Think about a binary classification. If most of your data is one class, a ML model is going to predict that class more frequently, just because there is more data for it. Balancing the data takes any skewed data and helps remove this imbalance. </p> </li> <li> <p>Now you can check the numbers of labels per ingredient:</p> <pre><code>print(f'new label count: {transformed_label_df.value_counts()}')\nprint(f'old label count: {df.cuisine.value_counts()}')\n</code></pre> <p>Your output looks like so:</p> <pre><code>new label count: korean      799\nchinese     799\nindian      799\njapanese    799\nthai        799\nName: cuisine, dtype: int64\nold label count: korean      799\nindian      598\nchinese     442\njapanese    320\nthai        289\nName: cuisine, dtype: int64\n</code></pre> <p>The data is nice and clean, balanced, and very delicious! </p> </li> <li> <p>The last step is to save your balanced data, including labels and features, into a new dataframe that can be exported into a file:</p> <pre><code>transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')\n</code></pre> </li> <li> <p>You can take one more look at the data using <code>transformed_df.head()</code> and <code>transformed_df.info()</code>. Save a copy of this data for use in future lessons:</p> <pre><code>transformed_df.head()\ntransformed_df.info()\ntransformed_df.to_csv(\"../data/cleaned_cuisines.csv\")\n</code></pre> <p>This fresh CSV can now be found in the root data folder.</p> </li> </ol>"},{"location":"4-Classification/1-Introduction/#challenge","title":"\ud83d\ude80Challenge","text":"<p>This curriculum contains several interesting datasets. Dig through the <code>data</code> folders and see if any contain datasets that would be appropriate for binary or multi-class classification? What questions would you ask of this dataset?</p>"},{"location":"4-Classification/1-Introduction/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"4-Classification/1-Introduction/#review-self-study","title":"Review &amp; Self Study","text":"<p>Explore SMOTE's API. What use cases is it best used for? What problems does it solve?</p>"},{"location":"4-Classification/1-Introduction/#assignment","title":"Assignment","text":"<p>Explore classification methods</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/","title":"\u5bf9\u5206\u7c7b\u65b9\u6cd5\u7684\u4ecb\u7ecd","text":"<p>\u5728\u8fd9\u56db\u8282\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u5c06\u4f1a\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u57fa\u672c\u7684\u91cd\u70b9 - \u5206\u7c7b\u3002 \u6211\u4eec\u4f1a\u5728\u5173\u4e8e\u4e9a\u6d32\u548c\u5370\u5ea6\u7684\u795e\u5947\u7684\u7f8e\u98df\u7684\u6570\u636e\u96c6\u4e0a\u5c1d\u8bd5\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u7b97\u6cd5\u3002\u5e0c\u671b\u4f60\u6709\u70b9\u997f\u4e86\u3002</p> <p></p> <p>\u5728\u5b66\u4e60\u7684\u8bfe\u7a0b\u4e2d\u8d5e\u53f9\u6cdb\u4e9a\u5730\u533a\u7684\u7f8e\u98df\u5427\uff01 \u56fe\u7247\u7531 Jen Looper \u63d0\u4f9b</p> <p>\u5206\u7c7b\u7b97\u6cd5\u662f\u76d1\u7763\u5b66\u4e60\u7684\u4e00\u79cd\u3002\u5b83\u4e0e\u56de\u5f52\u7b97\u6cd5\u5728\u5f88\u591a\u65b9\u9762\u90fd\u6709\u76f8\u540c\u4e4b\u5904\u3002\u5982\u679c\u673a\u5668\u5b66\u4e60\u6240\u6709\u7684\u76ee\u6807\u90fd\u662f\u4f7f\u7528\u6570\u636e\u96c6\u6765\u9884\u6d4b\u6570\u503c\u6216\u7269\u54c1\u7684\u540d\u5b57\uff0c\u90a3\u4e48\u5206\u7c7b\u7b97\u6cd5\u901a\u5e38\u53ef\u4ee5\u5206\u4e3a\u4e24\u7c7b \u4e8c\u5143\u5206\u7c7b \u548c \u591a\u5143\u5206\u7c7b\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u65b9\u7684\u56fe\u7247\u53ef\u4ee5\u8df3\u8f6c\u5230\u4e00\u4e2a\u89c6\u9891-MIT \u7684 John \u5bf9\u5206\u7c7b\u7b97\u6cd5\u7684\u4ecb\u7ecd</p> <p>\u8bf7\u8bb0\u4f4f\uff1a</p> <ul> <li>\u7ebf\u6027\u56de\u5f52  \u5e2e\u52a9\u4f60\u9884\u6d4b\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u5e76\u5bf9\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u70b9\u4f1a\u843d\u5728\u54ea\u6761\u7ebf\u4e0a\u505a\u51fa\u7cbe\u786e\u7684\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u4f60\u53ef\u4ee5\u9884\u6d4b \u5357\u74dc\u5728\u4e5d\u6708\u7684\u4ef7\u683c\u548c\u5341\u6708\u7684\u4ef7\u683c\u3002</li> <li>\u903b\u8f91\u56de\u5f52  \u5e2e\u52a9\u4f60\u53d1\u73b0\u201c\u4e8c\u5143\u8303\u7574\u201d\uff1a\u5373\u5728\u5f53\u524d\u8fd9\u4e2a\u4ef7\u683c\uff0c \u8fd9\u4e2a\u5357\u74dc\u662f\u4e0d\u662f\u6a59\u8272\uff1f</li> </ul> <p>\u5206\u7c7b\u65b9\u6cd5\u91c7\u7528\u591a\u79cd\u7b97\u6cd5\u6765\u786e\u5b9a\u5176\u4ed6\u53ef\u4ee5\u7528\u6765\u786e\u5b9a\u4e00\u4e2a\u6570\u636e\u70b9\u7684\u6807\u7b7e\u6216\u7c7b\u522b\u7684\u65b9\u6cd5\u3002\u8ba9\u6211\u4eec\u6765\u7814\u7a76\u4e00\u4e0b\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u770b\u770b\u6211\u4eec\u80fd\u5426\u901a\u8fc7\u89c2\u5bdf\u83dc\u80b4\u7684\u539f\u6599\u6765\u786e\u5b9a\u5b83\u7684\u6e90\u5934\u3002</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#_2","title":"\u8bfe\u7a0b\u524d\u7684\u5c0f\u95ee\u9898","text":"<p>\u5206\u7c7b\u662f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8005\u548c\u6570\u636e\u79d1\u5b66\u5bb6\u4f7f\u7528\u7684\u4e00\u79cd\u57fa\u672c\u65b9\u6cd5\u3002\u4ece\u57fa\u672c\u7684\u4e8c\u5143\u5206\u7c7b\uff08\u8fd9\u662f\u4e0d\u662f\u4e00\u4efd\u5783\u573e\u90ae\u4ef6\uff1f\uff09\u5230\u590d\u6742\u7684\u56fe\u7247\u5206\u7c7b\u548c\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5206\u5272\u6280\u672f\uff0c\u5b83\u90fd\u662f\u5c06\u6570\u636e\u5206\u7c7b\u5e76\u63d0\u51fa\u76f8\u5173\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\u3002</p> <p></p> <p>\u9700\u8981\u5206\u7c7b\u7b97\u6cd5\u89e3\u51b3\u7684\u4e8c\u5143\u5206\u7c7b\u548c\u591a\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5bf9\u6bd4. \u4fe1\u606f\u56fe\u7531 Jen Looper \u63d0\u4f9b</p> <p>\u5728\u5f00\u59cb\u6e05\u6d17\u6570\u636e\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u8c03\u6574\u6570\u636e\u4ee5\u9002\u5e94\u673a\u5668\u5b66\u4e60\u7684\u4efb\u52a1\u524d\uff0c\u8ba9\u6211\u4eec\u6765\u4e86\u89e3\u4e00\u4e0b\u591a\u79cd\u53ef\u7528\u6765\u6570\u636e\u5206\u7c7b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002</p> <p>\u6d3e\u751f\u81ea\u7edf\u8ba1\u6570\u5b66\uff0c\u5206\u7c7b\u7b97\u6cd5\u4f7f\u7528\u7ecf\u5178\u7684\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e9b\u7279\u5f81\uff0c\u6bd4\u5982\u901a\u8fc7'\u5438\u70df\u8005'\u3001'\u4f53\u91cd'\u548c'\u5e74\u9f84'\u6765\u63a8\u65ad \u7f79\u60a3\u67d0\u79cd\u75be\u75c5\u7684\u53ef\u80fd\u6027\u3002\u4f5c\u4e3a\u4e00\u4e2a\u4e0e\u4f60\u521a\u521a\u5b9e\u8df5\u8fc7\u7684\u56de\u5f52\u7b97\u6cd5\u5f88\u76f8\u4f3c\u7684\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f60\u7684\u6570\u636e\u662f\u88ab\u6807\u8bb0\u8fc7\u7684\u5e76\u4e14\u7b97\u6cd5\u901a\u8fc7\u91c7\u96c6\u8fd9\u4e9b\u6807\u7b7e\u6765\u8fdb\u884c\u5206\u7c7b\u548c\u9884\u6d4b\u5e76\u8fdb\u884c\u8f93\u51fa\u3002</p> <p>\u2705 \u82b1\u4e00\u70b9\u65f6\u95f4\u6765\u60f3\u8c61\u4e00\u4e0b\u4e00\u4e2a\u5173\u4e8e\u83dc\u80b4\u7684\u6570\u636e\u96c6\u3002\u4e00\u4e2a\u591a\u5143\u5206\u7c7b\u7684\u6a21\u578b\u5e94\u8be5\u80fd\u56de\u7b54\u4ec0\u4e48\u95ee\u9898\uff1f\u4e00\u4e2a\u4e8c\u5143\u5206\u7c7b\u7684\u6a21\u578b\u53c8\u5e94\u8be5\u80fd\u56de\u7b54\u4ec0\u4e48\uff1f\u5982\u679c\u4f60\u60f3\u786e\u5b9a\u4e00\u4e2a\u7ed9\u5b9a\u7684\u83dc\u80b4\u662f\u5426\u4f1a\u7528\u5230\u846b\u82a6\u5df4\uff08\u4e00\u79cd\u690d\u7269\uff0c\u79cd\u5b50\u7528\u6765\u8c03\u5473\uff09\u8be5\u600e\u4e48\u505a\uff1f\u5982\u679c\u4f60\u60f3\u77e5\u9053\u7ed9\u4f60\u4e00\u4e2a\u88c5\u6ee1\u4e86\u516b\u89d2\u8334\u9999\u3001\u82b1\u6930\u83dc\u548c\u8fa3\u6839\u7684\u8d2d\u7269\u888b\u4f60\u80fd\u5426\u505a\u51fa\u4e00\u9053\u4ee3\u8868\u6027\u7684\u5370\u5ea6\u83dc\u53c8\u8be5\u600e\u4e48\u505a\uff1f</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u56fe\u50cf\u89c2\u770b\u89c6\u9891\u3002\u6574\u4e2a'Chopped'\u8282\u76ee\u7684\u524d\u63d0\u90fd\u662f\u5efa\u7acb\u5728\u795e\u79d8\u7684\u7bee\u5b50\u4e0a\uff0c\u5728\u8fd9\u4e2a\u8282\u76ee\u4e2d\u53a8\u5e08\u5fc5\u987b\u5229\u7528\u968f\u673a\u7ed9\u5b9a\u7684\u98df\u6750\u505a\u83dc\u3002\u53ef\u89c1\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u8d77\u5230\u4e0d\u5c0f\u7684\u4f5c\u7528</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#-","title":"\u521d\u89c1-\u5206\u7c7b\u5668","text":"<p>\u6211\u4eec\u5173\u4e8e\u8fd9\u4e2a\u83dc\u80b4\u6570\u636e\u96c6\u60f3\u8981\u63d0\u51fa\u7684\u95ee\u9898\u5176\u5b9e\u662f\u4e00\u4e2a \u591a\u5143\u95ee\u9898\uff0c\u56e0\u4e3a\u6211\u4eec\u6709\u5f88\u591a\u6f5c\u5728\u7684\u5177\u6709\u4ee3\u8868\u6027\u7684\u83dc\u80b4\u3002\u7ed9\u5b9a\u4e00\u7cfb\u5217\u98df\u6750\u6570\u636e\uff0c\u6570\u636e\u80fd\u591f\u7b26\u5408\u8fd9\u4e9b\u7c7b\u522b\u4e2d\u7684\u54ea\u4e00\u7c7b\uff1f</p> <p>Scikit-learn \u9879\u76ee\u63d0\u4f9b\u591a\u79cd\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u7684\u7b97\u6cd5\uff0c\u4f60\u9700\u8981\u6839\u636e\u95ee\u9898\u7684\u5177\u4f53\u7c7b\u578b\u6765\u8fdb\u884c\u9009\u62e9\u3002\u5728\u4e0b\u4e24\u8282\u8bfe\u7a0b\u4e2d\u4f60\u4f1a\u5b66\u5230\u8fd9\u4e9b\u7b97\u6cd5\u4e2d\u7684\u51e0\u4e2a\u3002</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u6e05\u6d17\u5e76\u5e73\u8861\u4f60\u7684\u6570\u636e","text":"<p>\u5728\u4f60\u5f00\u59cb\u8fdb\u884c\u8fd9\u4e2a\u9879\u76ee\u524d\u7684\u7b2c\u4e00\u4e2a\u4e0a\u624b\u7684\u4efb\u52a1\u5c31\u662f\u6e05\u6d17\u548c \u5e73\u8861\u4f60\u7684\u6570\u636e\u6765\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002\u4ece\u5f53\u524d\u76ee\u5f55\u7684\u6839\u76ee\u5f55\u4e2d\u7684 nodebook.ipynb \u5f00\u59cb\u3002</p> <p>\u7b2c\u4e00\u4e2a\u9700\u8981\u5b89\u88c5\u7684\u4e1c\u897f\u662f imblearn \u8fd9\u662f\u4e00\u4e2a Scikit-learn \u9879\u76ee\u4e2d\u7684\u4e00\u4e2a\u5305\uff0c\u5b83\u53ef\u4ee5\u8ba9\u4f60\u66f4\u597d\u7684\u5e73\u8861\u6570\u636e (\u5173\u4e8e\u8fd9\u4e2a\u4efb\u52a1\u4f60\u5f88\u5feb\u4f60\u5c31\u4f1a\u5b66\u5230\u66f4\u591a)\u3002</p> <ol> <li> <p>\u5b89\u88c5 <code>imblearn</code>, \u8fd0\u884c\u547d\u4ee4 <code>pip install</code>:</p> <pre><code>pip install imblearn\n</code></pre> </li> <li> <p>\u4e3a\u4e86\u5bfc\u5165\u548c\u53ef\u89c6\u5316\u6570\u636e\u4f60\u9700\u8981\u5bfc\u5165\u4e0b\u9762\u7684\u8fd9\u4e9b\u5305, \u4f60\u8fd8\u9700\u8981\u4ece <code>imblearn</code> \u5bfc\u5165 <code>SMOTE</code></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\n</code></pre> <p>\u73b0\u5728\u4f60\u5df2\u7ecf\u51c6\u5907\u597d\u5bfc\u5165\u6570\u636e\u4e86\u3002</p> </li> <li> <p>\u4e0b\u4e00\u9879\u4efb\u52a1\u662f\u5bfc\u5165\u6570\u636e:</p> <pre><code>df  = pd.read_csv('../data/cuisines.csv')\n</code></pre> </li> </ol> <p>\u4f7f\u7528\u51fd\u6570 <code>read_csv()</code> \u4f1a\u8bfb\u53d6 csv \u6587\u4ef6\u7684\u5185\u5bb9 cusines.csv \u5e76\u5c06\u5185\u5bb9\u653e\u7f6e\u5728 \u53d8\u91cf <code>df</code> \u4e2d\u3002</p> <ol> <li> <p>\u68c0\u67e5\u6570\u636e\u7684\u5f62\u72b6\u662f\u5426\u6b63\u786e:</p> <pre><code>df.head()\n</code></pre> </li> </ol> <p>\u524d\u4e94\u884c\u8f93\u51fa\u5e94\u8be5\u662f\u8fd9\u6837\u7684:</p> <pre><code>```output\n|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n| 0   | 65         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 1   | 66         | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 2   | 67         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 3   | 68         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 4   | 69         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n```\n</code></pre> <ol> <li> <p>\u8c03\u7528\u51fd\u6570 <code>info()</code> \u53ef\u4ee5\u83b7\u5f97\u6709\u5173\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u4fe1\u606f:</p> <pre><code>df.info()\n</code></pre> <p>\u4f60\u7684\u8f93\u51fa\u5e94\u8be5\u50cf\u8fd9\u6837\uff1a</p> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2448 entries, 0 to 2447\nColumns: 385 entries, Unnamed: 0 to zucchini\ndtypes: int64(384), object(1)\nmemory usage: 7.2+ MB\n</code></pre> </li> </ol>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#-_2","title":"\u7ec3\u4e60 - \u4e86\u89e3\u8fd9\u4e9b\u83dc\u80b4","text":"<p>\u73b0\u5728\u4efb\u52a1\u53d8\u5f97\u66f4\u6709\u8da3\u4e86\uff0c\u8ba9\u6211\u4eec\u6765\u63a2\u7d22\u5982\u4f55\u5c06\u6570\u636e\u5206\u914d\u7ed9\u5404\u4e2a\u83dc\u80b4</p> <ol> <li> <p>\u8c03\u7528\u51fd\u6570 <code>barh()</code> \u53ef\u4ee5\u7ed8\u5236\u51fa\u6570\u636e\u7684\u6761\u5f62\u56fe:</p> <pre><code>df.cuisine.value_counts().plot.barh()\n</code></pre> <p></p> <p>\u8fd9\u91cc\u6709\u6709\u9650\u7684\u4e00\u4e9b\u83dc\u80b4\uff0c\u4f46\u662f\u6570\u636e\u7684\u5206\u914d\u662f\u4e0d\u5e73\u5747\u7684\u3002\u4f46\u662f\u4f60\u53ef\u4ee5\u4fee\u6b63\u8fd9\u4e00\u73b0\u8c61\uff01\u5728\u8fd9\u6837\u505a\u4e4b\u524d\u518d\u7a0d\u5fae\u63a2\u7d22\u4e00\u4e0b\u3002</p> </li> <li> <p>\u627e\u51fa\u5bf9\u4e8e\u6bcf\u4e2a\u83dc\u80b4\u6709\u591a\u5c11\u6570\u636e\u662f\u6709\u6548\u7684\u5e76\u5c06\u5176\u6253\u5370\u51fa\u6765:</p> <pre><code>thai_df = df[(df.cuisine == \"thai\")]\njapanese_df = df[(df.cuisine == \"japanese\")]\nchinese_df = df[(df.cuisine == \"chinese\")]\nindian_df = df[(df.cuisine == \"indian\")]\nkorean_df = df[(df.cuisine == \"korean\")]\n\nprint(f'thai df: {thai_df.shape}')\nprint(f'japanese df: {japanese_df.shape}')\nprint(f'chinese df: {chinese_df.shape}')\nprint(f'indian df: {indian_df.shape}')\nprint(f'korean df: {korean_df.shape}')\n</code></pre> <p>\u8f93\u51fa\u5e94\u8be5\u662f\u8fd9\u6837\u7684 :</p> <pre><code>thai df: (289, 385)\njapanese df: (320, 385)\nchinese df: (442, 385)\nindian df: (598, 385)\nkorean df: (799, 385)\n</code></pre> </li> </ol> <p>\u73b0\u5728\u4f60\u53ef\u4ee5\u5728\u6570\u636e\u4e2d\u63a2\u7d22\u7684\u66f4\u6df1\u4e00\u70b9\u5e76\u4e86\u89e3\u6bcf\u9053\u83dc\u80b4\u7684\u4ee3\u8868\u6027\u98df\u6750\u3002\u4f60\u9700\u8981\u5c06\u53cd\u590d\u51fa\u73b0\u7684\u3001\u5bb9\u6613\u9020\u6210\u6df7\u6dc6\u7684\u6570\u636e\u6e05\u7406\u51fa\u53bb\uff0c\u90a3\u4e48\u8ba9\u6211\u4eec\u6765\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p> <ol> <li> <p>\u5728 Python \u4e2d\u521b\u5efa\u4e00\u4e2a\u51fd\u6570 <code>create_ingredient_df()</code> \u6765\u521b\u5efa\u4e00\u4e2a\u98df\u6750\u7684\u6570\u636e\u5e27\u3002\u8fd9\u4e2a\u51fd\u6570\u4f1a\u53bb\u6389\u6570\u636e\u4e2d\u65e0\u7528\u7684\u5217\u5e76\u6309\u98df\u6750\u7684\u6570\u91cf\u8fdb\u884c\u5206\u7c7b\u3002</p> <p><pre><code>def create_ingredient_df(df):\n    ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')\n    ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]\n    ingredient_df = ingredient_df.sort_values(by='value', ascending=False,\n    inplace=False)\n    return ingredient_df\n</code></pre> \u73b0\u5728\u4f60\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570\u6765\u5f97\u5230\u7406\u60f3\u7684\u6bcf\u9053\u83dc\u80b4\u6700\u91cd\u8981\u7684 10 \u79cd\u98df\u6750\u3002</p> </li> <li> <p>\u8c03\u7528\u51fd\u6570 <code>create_ingredient_df()</code> \u7136\u540e\u901a\u8fc7\u51fd\u6570 <code>barh()</code> \u6765\u7ed8\u5236\u56fe\u50cf:</p> <pre><code>thai_ingredient_df = create_ingredient_df(thai_df)\nthai_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>\u5bf9\u65e5\u672c\u7684\u6570\u636e\u8fdb\u884c\u76f8\u540c\u7684\u64cd\u4f5c:</p> <pre><code>japanese_ingredient_df = create_ingredient_df(japanese_df)\njapanese_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>\u73b0\u5728\u5904\u7406\u4e2d\u56fd\u7684\u6570\u636e:</p> <pre><code>chinese_ingredient_df = create_ingredient_df(chinese_df)\nchinese_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>\u7ed8\u5236\u5370\u5ea6\u98df\u6750\u7684\u6570\u636e:</p> <pre><code>indian_ingredient_df = create_ingredient_df(indian_df)\nindian_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>\u6700\u540e\uff0c\u7ed8\u5236\u97e9\u56fd\u7684\u98df\u6750\u7684\u6570\u636e:</p> <pre><code>korean_ingredient_df = create_ingredient_df(korean_df)\nkorean_ingredient_df.head(10).plot.barh()\n</code></pre> <p></p> </li> <li> <p>\u73b0\u5728\uff0c\u53bb\u9664\u5728\u4e0d\u540c\u7684\u83dc\u80b4\u95f4\u6700\u666e\u904d\u7684\u5bb9\u6613\u9020\u6210\u6df7\u4e71\u7684\u98df\u6750\uff0c\u8c03\u7528\u51fd\u6570 <code>drop()</code>: </p> </li> </ol> <p>\u5927\u5bb6\u90fd\u559c\u6b22\u7c73\u996d\u3001\u5927\u849c\u548c\u751f\u59dc</p> <pre><code>```python\nfeature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)\nlabels_df = df.cuisine #.unique()\nfeature_df.head()\n```\n</code></pre>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#_3","title":"\u63a2\u7d22\u6709\u5173\u98df\u6750\u7684\u5185\u5bb9","text":""},{"location":"4-Classification/1-Introduction/README.zh-cn/#_4","title":"\u5e73\u8861\u6570\u636e\u96c6","text":"<p>\u73b0\u5728\u4f60\u5df2\u7ecf\u6e05\u7406\u8fc7\u6570\u636e\u96c6\u4e86, \u4f7f\u7528 SMOTE - \"Synthetic Minority Over-sampling Technique\" - \u6765\u5e73\u8861\u6570\u636e\u96c6\u3002</p> <ol> <li> <p>\u8c03\u7528\u51fd\u6570 <code>fit_resample()</code>, \u6b64\u65b9\u6cd5\u901a\u8fc7\u63d2\u5165\u6570\u636e\u6765\u751f\u6210\u65b0\u7684\u6837\u672c</p> <pre><code>oversample = SMOTE()\ntransformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)\n</code></pre> <p>\u901a\u8fc7\u5bf9\u6570\u636e\u96c6\u7684\u5e73\u8861\uff0c\u5f53\u4f60\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u65f6\u80fd\u591f\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002\u73b0\u5728\u8003\u8651\u4e00\u4e2a\u4e8c\u5143\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u5982\u679c\u4f60\u7684\u6570\u636e\u96c6\u4e2d\u7684\u5927\u90e8\u5206\u6570\u636e\u90fd\u5c5e\u4e8e\u5176\u4e2d\u4e00\u4e2a\u7c7b\u522b\uff0c\u90a3\u4e48\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\u5c31\u4f1a\u56e0\u4e3a\u5728\u90a3\u4e2a\u7c7b\u522b\u7684\u6570\u636e\u66f4\u591a\u800c\u5224\u65ad\u90a3\u4e2a\u7c7b\u522b\u66f4\u4e3a\u5e38\u89c1\u3002\u5e73\u8861\u6570\u636e\u80fd\u591f\u53bb\u9664\u4e0d\u516c\u5e73\u7684\u6570\u636e\u70b9\u3002</p> </li> <li> <p>\u73b0\u5728\u4f60\u53ef\u4ee5\u67e5\u770b\u6bcf\u4e2a\u98df\u6750\u7684\u6807\u7b7e\u6570\u91cf:</p> <pre><code>print(f'new label count: {transformed_label_df.value_counts()}')\nprint(f'old label count: {df.cuisine.value_counts()}')\n</code></pre> <p>\u8f93\u51fa\u5e94\u8be5\u662f\u8fd9\u6837\u7684 :</p> <pre><code>new label count: korean      799\nchinese     799\nindian      799\njapanese    799\nthai        799\nName: cuisine, dtype: int64\nold label count: korean      799\nindian      598\nchinese     442\njapanese    320\nthai        289\nName: cuisine, dtype: int64\n</code></pre> <p>\u73b0\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0d\u4ec5\u5e72\u51c0\u3001\u5e73\u8861\u800c\u4e14\u8fd8\u5f88\u201c\u7f8e\u5473\u201d ! </p> </li> <li> <p>\u6700\u540e\u4e00\u6b65\u662f\u4fdd\u5b58\u4f60\u5904\u7406\u8fc7\u540e\u7684\u5e73\u8861\u7684\u6570\u636e\uff08\u5305\u62ec\u6807\u7b7e\u548c\u7279\u5f81\uff09\uff0c\u5c06\u5176\u4fdd\u5b58\u4e3a\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8f93\u51fa\u5230\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u5e27\u3002</p> <pre><code>transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')\n</code></pre> </li> <li> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528\u51fd\u6570 <code>transformed_df.head()</code> \u548c <code>transformed_df.info()</code> \u518d\u68c0\u67e5\u4e00\u4e0b\u4f60\u7684\u6570\u636e\u3002 \u63a5\u4e0b\u6765\u8981\u5c06\u6570\u636e\u4fdd\u5b58\u4ee5\u4f9b\u5728\u672a\u6765\u7684\u8bfe\u7a0b\u4e2d\u4f7f\u7528:</p> <pre><code>transformed_df.head()\ntransformed_df.info()\ntransformed_df.to_csv(\"../data/cleaned_cuisines.csv\")\n</code></pre> <p>\u8fd9\u4e2a\u5168\u65b0\u7684 CSV \u6587\u4ef6\u53ef\u4ee5\u5728\u6570\u636e\u6839\u76ee\u5f55\u4e2d\u88ab\u627e\u5230\u3002</p> </li> </ol>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#_5","title":"\ud83d\ude80\u5c0f\u7ec3\u4e60","text":"<p>\u672c\u9879\u76ee\u7684\u5168\u90e8\u8bfe\u7a0b\u542b\u6709\u5f88\u591a\u6709\u8da3\u7684\u6570\u636e\u96c6\u3002 \u63a2\u7d22\u4e00\u4e0b <code>data</code> \u6587\u4ef6\u5939\uff0c\u770b\u770b\u8fd9\u91cc\u9762\u6709\u6ca1\u6709\u9002\u5408\u4e8c\u5143\u5206\u7c7b\u3001\u591a\u5143\u5206\u7c7b\u7b97\u6cd5\u7684\u6570\u636e\u96c6\uff0c\u518d\u60f3\u4e00\u4e0b\u4f60\u5bf9\u8fd9\u4e9b\u6570\u636e\u96c6\u6709\u6ca1\u6709\u4ec0\u4e48\u60f3\u95ee\u7684\u95ee\u9898\u3002</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#_6","title":"\u8bfe\u540e\u7ec3\u4e60","text":""},{"location":"4-Classification/1-Introduction/README.zh-cn/#_7","title":"\u56de\u987e &amp; \u81ea\u5b66","text":"<p>\u63a2\u7d22\u4e00\u4e0b SMOTE \u7684 API \u6587\u6863\u3002\u601d\u8003\u4e00\u4e0b\u5b83\u6700\u9002\u5408\u4e8e\u4ec0\u4e48\u6837\u7684\u60c5\u51b5\u3001\u5b83\u80fd\u591f\u89e3\u51b3\u4ec0\u4e48\u6837\u7684\u95ee\u9898\u3002</p>"},{"location":"4-Classification/1-Introduction/README.zh-cn/#_8","title":"\u8bfe\u540e\u4f5c\u4e1a","text":"<p>\u63a2\u7d22\u4e00\u4e0b\u5206\u7c7b\u65b9\u6cd5</p>"},{"location":"4-Classification/1-Introduction/assignment/","title":"Explore classification methods","text":""},{"location":"4-Classification/1-Introduction/assignment/#instructions","title":"Instructions","text":"<p>In Scikit-learn documentation you'll find a large list of ways to classify data. Do a little scavenger hunt in these docs: your goals is to look for classification methods and match a dataset in this curriculum, a question you can ask of it, and a technique of classification. Create a spreadsheet or table in a .doc file and explain how the dataset would work with the classification algorithm.</p>"},{"location":"4-Classification/1-Introduction/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement a document is presented overviewing 5 algorithms alongside a classification technique. The overview is well-explained and detailed. a document is presented overviewing 3 algorithms alongside a classification technique. The overview is well-explained and detailed. a document is presented overviewing fewer than three algorithms alongside a classification technique and the overview is neither well-explained nor detailed."},{"location":"4-Classification/1-Introduction/assignment.zh-cn/","title":"\u63a2\u7d22\u5206\u7c7b\u65b9\u6cd5","text":""},{"location":"4-Classification/1-Introduction/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728 Scikit-learn \u6587\u6863 \u4e2d\u4f60\u4f1a\u627e\u5230\u4e00\u5927\u4e32\u7684\u6570\u636e\u5206\u7c7b\u7684\u65b9\u6cd5\u3002\u5728\u8fd9\u4e9b\u6587\u6863\u4e2d\u505a\u4e00\u4e2a\u5bfb\u5b9d\u6e38\u620f\uff1a\u4f60\u7684\u76ee\u6807\u662f\u5bfb\u627e\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5728\u672c\u8bfe\u7a0b\u4e2d\u5339\u914d\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u4f60\u80fd\u5bf9\u5b83\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4e00\u79cd\u5206\u7c7b\u6280\u672f\u3002\u5728\u4e00\u4e2a .doc \u6587\u4ef6\u4e2d\u521b\u5efa\u4e00\u4e2a\u7535\u5b50\u8868\u683c\u6216\u8868\u683c\uff0c\u5e76\u89e3\u91ca\u8be5\u6570\u636e\u96c6\u5982\u4f55\u4e0e\u5206\u7c7b\u7b97\u6cd5\u4e00\u8d77\u5de5\u4f5c\u3002</p>"},{"location":"4-Classification/1-Introduction/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u4e00\u4efd\u6587\u4ef6\uff0c\u6982\u8ff0\u4e86 5 \u79cd\u7b97\u6cd5\u548c\u4e00\u79cd\u5206\u7c7b\u6280\u672f\u3002\u6982\u8ff0\u89e3\u91ca\u5f97\u6e05\u695a\u4e14\u8be6\u7ec6\u3002 \u63d0\u4ea4\u4e86\u4e00\u4efd\u6587\u4ef6\uff0c\u6982\u8ff0\u4e86 3 \u79cd\u7b97\u6cd5\u548c\u4e00\u79cd\u5206\u7c7b\u6280\u672f\u3002\u6982\u8ff0\u89e3\u91ca\u5f97\u6e05\u695a\u4e14\u8be6\u7ec6\u3002 \u63d0\u4ea4\u4e86\u4e00\u4efd\u6587\u4ef6\uff0c\u6982\u8ff0\u4e86\u5c11\u4e8e 3 \u79cd\u7b97\u6cd5\u548c\u4e00\u79cd\u5206\u7c7b\u6280\u672f\uff0c\u800c\u4e14\u6982\u8ff0\u65e2\u6ca1\u6709\u5f88\u597d\u7684\u89e3\u91ca\u4e5f\u6ca1\u6709\u8be6\u7ec6\u8bf4\u660e\u3002"},{"location":"4-Classification/1-Introduction/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"4-Classification/2-Classifiers-1/","title":"Cuisine classifiers 1","text":"<p>In this lesson, you will use the dataset you saved from the last lesson full of balanced, clean data all about cuisines.</p> <p>You will use this dataset with a variety of classifiers to predict a given national cuisine based on a group of ingredients. While doing so, you'll learn more about some of the ways that algorithms can be leveraged for classification tasks.</p>"},{"location":"4-Classification/2-Classifiers-1/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"4-Classification/2-Classifiers-1/#preparation","title":"Preparation","text":"<p>Assuming you completed Lesson 1, make sure that a cleaned_cuisines.csv file exists in the root <code>/data</code> folder for these four lessons.</p>"},{"location":"4-Classification/2-Classifiers-1/#exercise-predict-a-national-cuisine","title":"Exercise - predict a national cuisine","text":"<ol> <li> <p>Working in this lesson's notebook.ipynb folder, import that file along with the Pandas library:</p> <pre><code>import pandas as pd\ncuisines_df = pd.read_csv(\"../data/cleaned_cuisines.csv\")\ncuisines_df.head()\n</code></pre> <p>The data looks like this:</p> </li> </ol> Unnamed: 0 cuisine almond angelica anise anise_seed apple apple_brandy apricot armagnac ... whiskey white_bread white_wine whole_grain_wheat_flour wine wood yam yeast yogurt zucchini 0 0 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 1 indian 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 2 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 3 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 4 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 <ol> <li> <p>Now, import several more libraries:</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\nfrom sklearn.svm import SVC\nimport numpy as np\n</code></pre> </li> <li> <p>Divide the X and y coordinates into two dataframes for training. <code>cuisine</code> can be the labels dataframe:</p> <pre><code>cuisines_label_df = cuisines_df['cuisine']\ncuisines_label_df.head()\n</code></pre> <p>It will look like this:</p> <pre><code>0    indian\n1    indian\n2    indian\n3    indian\n4    indian\nName: cuisine, dtype: object\n</code></pre> </li> <li> <p>Drop that <code>Unnamed: 0</code> column and the <code>cuisine</code> column, calling <code>drop()</code>. Save the rest of the data as trainable features:</p> <pre><code>cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\ncuisines_feature_df.head()\n</code></pre> <p>Your features look like this:</p> </li> </ol> almond angelica anise anise_seed apple apple_brandy apricot armagnac artemisia artichoke ... whiskey white_bread white_wine whole_grain_wheat_flour wine wood yam yeast yogurt zucchini 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 <p>Now you are ready to train your model!</p>"},{"location":"4-Classification/2-Classifiers-1/#choosing-your-classifier","title":"Choosing your classifier","text":"<p>Now that your data is clean and ready for training, you have to decide which algorithm to use for the job. </p> <p>Scikit-learn groups classification under Supervised Learning, and in that category you will find many ways to classify. The variety is quite bewildering at first sight. The following methods all include classification techniques:</p> <ul> <li>Linear Models</li> <li>Support Vector Machines</li> <li>Stochastic Gradient Descent</li> <li>Nearest Neighbors</li> <li>Gaussian Processes</li> <li>Decision Trees</li> <li>Ensemble methods (voting Classifier)</li> <li>Multiclass and multioutput algorithms (multiclass and multilabel classification, multiclass-multioutput classification)</li> </ul> <p>You can also use neural networks to classify data, but that is outside the scope of this lesson.</p>"},{"location":"4-Classification/2-Classifiers-1/#what-classifier-to-go-with","title":"What classifier to go with?","text":"<p>So, which classifier should you choose? Often, running through several and looking for a good result is a way to test. Scikit-learn offers a side-by-side comparison on a created dataset, comparing KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB and QuadraticDiscrinationAnalysis, showing the results visualized: </p> <p></p> <p>Plots generated on Scikit-learn's documentation</p> <p>AutoML solves this problem neatly by running these comparisons in the cloud, allowing you to choose the best algorithm for your data. Try it here</p>"},{"location":"4-Classification/2-Classifiers-1/#a-better-approach","title":"A better approach","text":"<p>A better way than wildly guessing, however, is to follow the ideas on this downloadable ML Cheat sheet. Here, we discover that, for our multiclass problem, we have some choices:</p> <p></p> <p>A section of Microsoft's Algorithm Cheat Sheet, detailing multiclass classification options</p> <p>\u2705 Download this cheat sheet, print it out, and hang it on your wall!</p>"},{"location":"4-Classification/2-Classifiers-1/#reasoning","title":"Reasoning","text":"<p>Let's see if we can reason our way through different approaches given the constraints we have:</p> <ul> <li>Neural networks are too heavy. Given our clean, but minimal dataset, and the fact that we are running training locally via notebooks, neural networks are too heavyweight for this task.</li> <li>No two-class classifier. We do not use a two-class classifier, so that rules out one-vs-all. </li> <li>Decision tree or logistic regression could work. A decision tree might work, or logistic regression for multiclass data. </li> <li>Multiclass Boosted Decision Trees solve a different problem. The multiclass boosted decision tree is most suitable for nonparametric tasks, e.g. tasks designed to build rankings, so it is not useful for us.</li> </ul>"},{"location":"4-Classification/2-Classifiers-1/#using-scikit-learn","title":"Using Scikit-learn","text":"<p>We will be using Scikit-learn to analyze our data. However, there are many ways to use logistic regression in Scikit-learn. Take a look at the parameters to pass.  </p> <p>Essentially there are two important parameters - <code>multi_class</code> and <code>solver</code> - that we need to specify, when we ask Scikit-learn to perform a logistic regression. The <code>multi_class</code> value applies a certain behavior. The value of the solver is what algorithm to use. Not all solvers can be paired with all <code>multi_class</code> values.</p> <p>According to the docs, in the multiclass case, the training algorithm:</p> <ul> <li>Uses the one-vs-rest (OvR) scheme, if the <code>multi_class</code> option is set to <code>ovr</code></li> <li>Uses the cross-entropy loss, if the <code>multi_class</code> option is set to <code>multinomial</code>. (Currently the <code>multinomial</code> option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018newton-cg\u2019 solvers.)\"</li> </ul> <p>\ud83c\udf93 The 'scheme' here can either be 'ovr' (one-vs-rest) or 'multinomial'. Since logistic regression is really designed to support binary classification, these schemes allow it to better handle multiclass classification tasks. source</p> <p>\ud83c\udf93 The 'solver' is defined as \"the algorithm to use in the optimization problem\". source.</p> <p>Scikit-learn offers this table to explain how solvers handle different challenges presented by different kinds of data structures:</p> <p></p>"},{"location":"4-Classification/2-Classifiers-1/#exercise-split-the-data","title":"Exercise - split the data","text":"<p>We can focus on logistic regression for our first training trial since you recently learned about the latter in a previous lesson. Split your data into training and testing groups by calling <code>train_test_split()</code>:</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n</code></pre>"},{"location":"4-Classification/2-Classifiers-1/#exercise-apply-logistic-regression","title":"Exercise - apply logistic regression","text":"<p>Since you are using the multiclass case, you need to choose what scheme to use and what solver to set. Use LogisticRegression with a multiclass setting and the liblinear solver to train.</p> <ol> <li> <p>Create a logistic regression with multi_class set to <code>ovr</code> and the solver set to <code>liblinear</code>:</p> <pre><code>lr = LogisticRegression(multi_class='ovr',solver='liblinear')\nmodel = lr.fit(X_train, np.ravel(y_train))\n\naccuracy = model.score(X_test, y_test)\nprint (\"Accuracy is {}\".format(accuracy))\n</code></pre> <p>\u2705 Try a different solver like <code>lbfgs</code>, which is often set as default</p> <p>Note, use Pandas <code>ravel</code> function to flatten your data when needed.</p> <p>The accuracy is good at over 80%!</p> </li> <li> <p>You can see this model in action by testing one row of data (#50):</p> <pre><code>print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\nprint(f'cuisine: {y_test.iloc[50]}')\n</code></pre> <p>The result is printed:</p> </li> </ol> <pre><code>ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\ncuisine: indian\n</code></pre> <p>\u2705 Try a different row number and check the results</p> <ol> <li> <p>Digging deeper, you can check for the accuracy of this prediction:</p> <pre><code>test= X_test.iloc[50].values.reshape(-1, 1).T\nproba = model.predict_proba(test)\nclasses = model.classes_\nresultdf = pd.DataFrame(data=proba, columns=classes)\n\ntopPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\ntopPrediction.head()\n</code></pre> <p>The result is printed - Indian cuisine is its best guess, with good probability:</p> 0 indian 0.715851 chinese 0.229475 japanese 0.029763 korean 0.017277 thai 0.007634 <p>\u2705 Can you explain why the model is pretty sure this is an Indian cuisine?</p> </li> <li> <p>Get more detail by printing a classification report, as you did in the regression lessons:</p> <pre><code>y_pred = model.predict(X_test)\nprint(classification_report(y_test,y_pred))\n</code></pre> precision recall f1-score support chinese 0.73 0.71 0.72 229 indian 0.91 0.93 0.92 254 japanese 0.70 0.75 0.72 220 korean 0.86 0.76 0.81 242 thai 0.79 0.85 0.82 254 accuracy 0.80 1199 macro avg 0.80 0.80 0.80 1199 weighted avg 0.80 0.80 0.80 1199 </li> </ol>"},{"location":"4-Classification/2-Classifiers-1/#challenge","title":"\ud83d\ude80Challenge","text":"<p>In this lesson, you used your cleaned data to build a machine learning model that can predict a national cuisine based on a series of ingredients. Take some time to read through the many options Scikit-learn provides to classify data. Dig deeper into the concept of 'solver' to understand what goes on behind the scenes.</p>"},{"location":"4-Classification/2-Classifiers-1/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"4-Classification/2-Classifiers-1/#review-self-study","title":"Review &amp; Self Study","text":"<p>Dig a little more into the math behind logistic regression in this lesson</p>"},{"location":"4-Classification/2-Classifiers-1/#assignment","title":"Assignment","text":"<p>Study the solvers</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/","title":"\u83dc\u54c1\u5206\u7c7b\u5668 1","text":"<p>\u672c\u8282\u8bfe\u7a0b\u5c06\u4f7f\u7528\u4f60\u5728\u4e0a\u4e00\u4e2a\u8bfe\u7a0b\u4e2d\u6240\u4fdd\u5b58\u7684\u5168\u90e8\u7ecf\u8fc7\u5747\u8861\u548c\u6e05\u6d17\u7684\u83dc\u54c1\u6570\u636e\u3002</p> <p>\u4f60\u5c06\u4f7f\u7528\u6b64\u6570\u636e\u96c6\u548c\u5404\u79cd\u5206\u7c7b\u5668\uff0c\u6839\u636e\u4e00\u7ec4\u914d\u6599\u9884\u6d4b\u8fd9\u662f\u54ea\u4e00\u56fd\u5bb6\u7684\u7f8e\u98df\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u5230\u66f4\u591a\u7528\u6765\u6743\u8861\u5206\u7c7b\u4efb\u52a1\u7b97\u6cd5\u7684\u65b9\u6cd5  </p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_2","title":"\u51c6\u5907\u5de5\u4f5c","text":"<p>\u5047\u5982\u4f60\u5df2\u7ecf\u5b8c\u6210\u4e86\u8bfe\u7a0b 1, \u786e\u4fdd\u5728\u6839\u76ee\u5f55\u7684 <code>/data</code> \u6587\u4ef6\u5939\u4e2d\u6709 cleaned_cuisines.csv \u8fd9\u4efd\u6587\u4ef6\u6765\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u56db\u8282\u8bfe\u7a0b\u3002</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u9884\u6d4b\u67d0\u56fd\u7684\u83dc\u54c1","text":"<ol> <li> <p>\u5728\u672c\u8282\u8bfe\u7684 notebook.ipynb \u6587\u4ef6\u4e2d\uff0c\u5bfc\u5165 Pandas\uff0c\u5e76\u8bfb\u53d6\u76f8\u5e94\u7684\u6570\u636e\u6587\u4ef6\uff1a</p> <pre><code>import pandas as pd\ncuisines_df = pd.read_csv(\"../../data/cleaned_cuisines.csv\")\ncuisines_df.head()\n</code></pre> <p>\u6570\u636e\u5982\u4e0b\u6240\u793a:</p> Unnamed: 0 cuisine almond angelica anise anise_seed apple apple_brandy apricot armagnac ... whiskey white_bread white_wine whole_grain_wheat_flour wine wood yam yeast yogurt zucchini 0 0 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 1 indian 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 2 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 3 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 4 indian 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 </li> <li> <p>\u73b0\u5728\uff0c\u518d\u591a\u5bfc\u5165\u4e00\u4e9b\u5e93\uff1a</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\nfrom sklearn.svm import SVC\nimport numpy as np\n</code></pre> </li> <li> <p>\u63a5\u4e0b\u6765\u9700\u8981\u5c06\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684 X\uff08\u8bd1\u8005\u6ce8\uff1a\u4ee3\u8868\u7279\u5f81\u6570\u636e\uff09\u548c y\uff08\u8bd1\u8005\u6ce8\uff1a\u4ee3\u8868\u6807\u7b7e\u6570\u636e\uff09\u4e24\u4e2a dataframe\u3002\u9996\u5148\u53ef\u5c06 <code>cuisine</code> \u5217\u7684\u6570\u636e\u5355\u72ec\u4fdd\u5b58\u4e3a\u7684\u4e00\u4e2a dataframe \u4f5c\u4e3a\u6807\u7b7e\uff08label\uff09\u3002</p> <pre><code>cuisines_label_df = cuisines_df['cuisine']\ncuisines_label_df.head()\n</code></pre> <p>\u8f93\u51fa\u5982\u4e0b:</p> <pre><code>0    indian\n1    indian\n2    indian\n3    indian\n4    indian\nName: cuisine, dtype: object\n</code></pre> </li> <li> <p>\u8c03\u7528 <code>drop()</code> \u65b9\u6cd5\u5c06 <code>Unnamed: 0</code> \u548c <code>cuisine</code> \u5217\u5220\u9664\uff0c\u5e76\u5c06\u4f59\u4e0b\u7684\u6570\u636e\u4f5c\u4e3a\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u7684\u7279\u8bc1\uff08feature\uff09\u6570\u636e:</p> <pre><code>cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\ncuisines_feature_df.head()\n</code></pre> <p>\u4f60\u7684\u7279\u5f81\u96c6\u770b\u4e0a\u53bb\u5c06\u4f1a\u662f\u8fd9\u6837:</p> almond angelica anise anise_seed apple apple_brandy apricot armagnac artemisia artichoke ... whiskey white_bread white_wine whole_grain_wheat_flour wine wood yam yeast yogurt zucchini 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 </li> </ol> <p>\u73b0\u5728\uff0c\u4f60\u5df2\u7ecf\u51c6\u5907\u597d\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u4f60\u7684\u6a21\u578b\u4e86\uff01</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_3","title":"\u9009\u62e9\u4f60\u7684\u5206\u7c7b\u5668","text":"<p>\u4f60\u7684\u6570\u636e\u5df2\u7ecf\u6e05\u6d17\u5e72\u51c0\u5e76\u5df2\u7ecf\u51c6\u5907\u597d\u53ef\u4ee5\u8fdb\u884c\u8bad\u7ec3\u4e86\uff0c\u73b0\u5728\u9700\u8981\u51b3\u5b9a\u4f60\u60f3\u8981\u4f7f\u7528\u7684\u7b97\u6cd5\u6765\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002</p> <p>Scikit_learn \u5c06\u5206\u7c7b\u4efb\u52a1\u5f52\u5728\u4e86\u76d1\u7763\u5b66\u4e60\u7c7b\u522b\u4e2d\uff0c\u5728\u8fd9\u4e2a\u7c7b\u522b\u4e2d\u4f60\u53ef\u4ee5\u627e\u5230\u5f88\u591a\u53ef\u4ee5\u7528\u6765\u5206\u7c7b\u7684\u65b9\u6cd5\u3002\u4e4d\u4e00\u770b\u4e0a\u53bb\uff0c\u6709\u70b9\u7433\u7405\u6ee1\u76ee\u3002\u4ee5\u4e0b\u8fd9\u4e9b\u7b97\u6cd5\u90fd\u53ef\u4ee5\u7528\u4e8e\u5206\u7c7b\uff1a</p> <ul> <li>\u7ebf\u6027\u6a21\u578b\uff08Linear Models\uff09</li> <li>\u652f\u6301\u5411\u91cf\u673a\uff08Support Vector Machines\uff09</li> <li>\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08Stochastic Gradient Descent\uff09</li> <li>\u6700\u8fd1\u90bb\uff08Nearest Neighbors\uff09</li> <li>\u9ad8\u65af\u8fc7\u7a0b\uff08Gaussian Processes\uff09</li> <li>\u51b3\u7b56\u6811\uff08Decision Trees\uff09</li> <li>\u96c6\u6210\u65b9\u6cd5\uff08\u6295\u7968\u5206\u7c7b\u5668\uff09\uff08Ensemble methods\uff08voting classifier\uff09\uff09 </li> <li>\u591a\u7c7b\u522b\u591a\u8f93\u51fa\u7b97\u6cd5\uff08\u591a\u7c7b\u522b\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u591a\u7c7b\u522b\u591a\u8f93\u51fa\u5206\u7c7b\uff09\uff08Multiclass and multioutput algorithms (multiclass and multilabel classification, multiclass-multioutput classification)\uff09</li> </ul> <p>\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u5206\u7c7b\u6570\u636e, \u4f46\u8fd9\u5bf9\u4e8e\u672c\u8bfe\u7a0b\u6765\u8bf4\u6709\u70b9\u8d85\u7eb2\u4e86\u3002</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_4","title":"\u5982\u4f55\u9009\u62e9\u5206\u7c7b\u5668?","text":"<p>\u90a3\u4e48\uff0c\u4f60\u5e94\u8be5\u5982\u4f55\u4ece\u4e2d\u9009\u62e9\u5206\u7c7b\u5668\u5462\uff1f\u4e00\u822c\u6765\u8bf4\uff0c\u53ef\u4ee5\u9009\u62e9\u591a\u4e2a\u5206\u7c7b\u5668\u5e76\u5bf9\u6bd4\u4ed6\u4eec\u7684\u8fd0\u884c\u7ed3\u679c\u3002Scikit-learn \u63d0\u4f9b\u4e86\u5404\u79cd\u7b97\u6cd5\uff08\u5305\u62ec KNeighbors\u3001 SVC two ways\u3001 GaussianProcessClassifier\u3001 DecisionTreeClassifier\u3001 RandomForestClassifier\u3001 MLPClassifier\u3001 AdaBoostClassifier\u3001 GaussianNB \u4ee5\u53ca QuadraticDiscrinationAnalysis\uff09\u7684\u5bf9\u6bd4\uff0c\u5e76\u4e14\u5c06\u7ed3\u679c\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u7684\u5c55\u793a\uff1a</p> <p></p> <p>\u56fe\u8868\u6765\u6e90\u4e8e Scikit-learn \u7684\u5b98\u65b9\u6587\u6863</p> <p>AutoML \u901a\u8fc7\u5728\u4e91\u7aef\u8fd0\u884c\u8fd9\u4e9b\u7b97\u6cd5\u5e76\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u975e\u5e38\u5de7\u5999\u5730\u89e3\u51b3\u7684\u7b97\u6cd5\u9009\u62e9\u7684\u95ee\u9898\uff0c\u80fd\u5e2e\u52a9\u4f60\u6839\u636e\u6570\u636e\u96c6\u7684\u7279\u70b9\u6765\u9009\u62e9\u6700\u4f73\u7684\u7b97\u6cd5\u3002\u8bd5\u8bd5\u70b9\u51fb\u8fd9\u91cc\u4e86\u89e3\u66f4\u591a\u3002</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_5","title":"\u53e6\u5916\u4e00\u79cd\u6548\u679c\u66f4\u4f73\u7684\u5206\u7c7b\u5668\u9009\u62e9\u65b9\u6cd5","text":"<p>\u6bd4\u8d77\u65e0\u8111\u5730\u731c\u6d4b\uff0c\u4f60\u53ef\u4ee5\u4e0b\u8f7d\u8fd9\u4efd\u673a\u5668\u5b66\u4e60\u901f\u67e5\u8868\uff08cheatsheet\uff09\u3002\u8fd9\u91cc\u9762\u5c06\u5404\u7b97\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u80fd\u66f4\u6709\u6548\u5730\u5e2e\u52a9\u6211\u4eec\u9009\u62e9\u7b97\u6cd5\u3002\u6839\u636e\u8fd9\u4efd\u901f\u67e5\u8868\uff0c\u6211\u4eec\u53ef\u4ee5\u627e\u5230\u8981\u5b8c\u6210\u672c\u8bfe\u7a0b\u4e2d\u6d89\u53ca\u7684\u591a\u7c7b\u578b\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u53ef\u4ee5\u6709\u4ee5\u4e0b\u8fd9\u4e9b\u9009\u62e9\uff1a</p> <p></p> <p>\u5fae\u8f6f\u7b97\u6cd5\u5c0f\u6284\u4e2d\u90e8\u5206\u5173\u4e8e\u591a\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u53ef\u9009\u7b97\u6cd5</p> <p>\u2705 \u4e0b\u8f7d\u8fd9\u4efd\u5c0f\u6284\uff0c\u5e76\u6253\u5370\u51fa\u6765\uff0c\u6302\u5728\u4f60\u7684\u5899\u4e0a\u5427\uff01</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_6","title":"\u9009\u62e9\u7684\u6d41\u7a0b","text":"<p>\u8ba9\u6211\u4eec\u6839\u636e\u6240\u6709\u9650\u5236\u6761\u4ef6\u4f9d\u6b21\u5bf9\u5404\u79cd\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u8fdb\u884c\u5224\u65ad\uff1a</p> <ul> <li>\u795e\u7ecf\u7f51\u7edc\uff08Neural Network\uff09\u592a\u8fc7\u590d\u6742\u4e86\u3002\u6211\u4eec\u7684\u6570\u636e\u5f88\u6e05\u6670\u4f46\u6570\u636e\u91cf\u6bd4\u8f83\u5c0f\uff0c\u6b64\u5916\u6211\u4eec\u662f\u901a\u8fc7 notebook \u5728\u672c\u5730\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e8e\u8fd9\u4e2a\u4efb\u52a1\u6765\u8bf4\u8fc7\u4e8e\u590d\u6742\u4e86\u3002</li> <li>\u4e8c\u5206\u7c7b\u6cd5\uff08two-class classifier\uff09\u662f\u4e0d\u53ef\u884c\u7684\u3002\u6211\u4eec\u4e0d\u80fd\u4f7f\u7528\u4e8c\u5206\u7c7b\u6cd5,\u6240\u4ee5\u8fd9\u5c31\u6392\u9664\u4e86\u4e00\u5bf9\u591a\uff08one-vs-all\uff09\u7b97\u6cd5\u3002 </li> <li>\u53ef\u4ee5\u9009\u62e9\u51b3\u7b56\u6811\u4ee5\u53ca\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u3002\u51b3\u7b56\u6811\u5e94\u8be5\u662f\u53ef\u884c\u7684\uff0c\u6b64\u5916\u4e5f\u53ef\u4ee5\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u6765\u5904\u7406\u591a\u7c7b\u578b\u6570\u636e\u3002</li> <li>\u591a\u7c7b\u578b\u589e\u5f3a\u51b3\u7b56\u6811\u662f\u7528\u4e8e\u89e3\u51b3\u5176\u4ed6\u95ee\u9898\u7684. \u591a\u7c7b\u578b\u589e\u5f3a\u51b3\u7b56\u6811\u6700\u9002\u5408\u7684\u662f\u975e\u53c2\u6570\u5316\u7684\u4efb\u52a1\uff0c\u5373\u4efb\u52a1\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u6392\u5e8f\uff0c\u8fd9\u5bf9\u6211\u4eec\u5f53\u524d\u7684\u4efb\u52a1\u5e76\u6ca1\u6709\u4f5c\u7528\u3002</li> </ul>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#scikit-learn","title":"\u4f7f\u7528 Scikit-learn","text":"<p>\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528 Scikit-learn \u6765\u5bf9\u6211\u4eec\u7684\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002\u7136\u800c\u5728 Scikit-learn \u4e2d\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u4e5f\u6709\u5f88\u591a\u65b9\u6cd5\u3002\u53ef\u4ee5\u5148\u4e86\u89e3\u4e00\u4e0b\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\u9700\u8981\u4f20\u9012\u7684\u53c2\u6570\u3002</p> <p>\u5f53\u6211\u4eec\u9700\u8981 Scikit-learn \u8fdb\u884c\u903b\u8f91\u56de\u5f52\u8fd0\u7b97\u65f6\uff0c<code>multi_class</code> \u4ee5\u53ca <code>solver</code>\u662f\u6700\u91cd\u8981\u7684\u4e24\u4e2a\u53c2\u6570\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u7279\u522b\u8bf4\u660e\u4e00\u4e0b\u3002 <code>multi_class</code> \u662f\u5206\u7c7b\u65b9\u5f0f\u9009\u62e9\u53c2\u6570\uff0c\u800c<code>solver</code>\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u53c2\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5e76\u4e0d\u662f\u6240\u6709\u7684 solvers \u90fd\u53ef\u4ee5\u4e0e<code>multi_class</code>\u53c2\u6570\u8fdb\u884c\u5339\u914d\u7684\u3002</p> <p>\u6839\u636e\u5b98\u65b9\u6587\u6863\uff0c\u5728\u591a\u7c7b\u578b\u5206\u7c7b\u95ee\u9898\u4e2d:</p> <ul> <li>\u5f53 <code>multi_class</code> \u88ab\u8bbe\u7f6e\u4e3a <code>ovr</code> \u65f6\uff0c\u5c06\u4f7f\u7528 \u201c\u4e00\u5bf9\u5176\u4f59\u201d(OvR)\u7b56\u7565\uff08scheme\uff09\u3002</li> <li>\u5f53 <code>multi_class</code> \u88ab\u8bbe\u7f6e\u4e3a <code>multinomial</code> \u65f6\uff0c\u5219\u4f7f\u7528\u7684\u662f\u4ea4\u53c9\u71b5\u635f\u5931\uff08cross entropy loss\uff09 \u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u3002(\u6ce8\u610f\uff0c\u76ee\u524d<code>multinomial</code>\u53ea\u652f\u6301\u2018lbfgs\u2019, \u2018sag\u2019, \u2018saga\u2019\u4ee5\u53ca\u2018newton-cg\u2019\u7b49 solver \u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u7684\u4f18\u5316\u65b9\u6cd5)</li> </ul> <p>\ud83c\udf93 \u5728\u672c\u8bfe\u7a0b\u7684\u4efb\u52a1\u4e2d\u201cscheme\u201d\u53ef\u4ee5\u662f\u201covr(one-vs-rest)\u201d\u4e5f\u53ef\u4ee5\u662f\u201cmultinomial\u201d\u3002\u56e0\u4e3a\u903b\u8f91\u56de\u5f52\u672c\u6765\u662f\u8bbe\u8ba1\u6765\u7528\u4e8e\u8fdb\u884c\u4e8c\u5206\u7c7b\u4efb\u52a1\u7684\uff0c\u8fd9\u4e24\u4e2a scheme \u53c2\u6570\u7684\u9009\u62e9\u90fd\u53ef\u4ee5\u4f7f\u5f97\u903b\u8f91\u56de\u5f52\u5f88\u597d\u7684\u5b8c\u6210\u591a\u7c7b\u578b\u5206\u7c7b\u4efb\u52a1\u3002\u6765\u6e90</p> <p>\ud83c\udf93 \u201csolver\u201d\u88ab\u5b9a\u4e49\u4e3a\u662f\"\u7528\u4e8e\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u7684\u7b97\u6cd5\"\u3002\u6765\u6e90.</p> <p>Scikit-learn\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u8fd9\u4e2a\u8868\u683c\u6765\u89e3\u91ca\u5404\u79cdsolver\u662f\u5982\u4f55\u5e94\u5bf9\u7684\u4e0d\u540c\u7684\u6570\u636e\u7ed3\u6784\u6240\u5e26\u6765\u7684\u4e0d\u540c\u7684\u6311\u6218\u7684:</p> <p></p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u5206\u5272\u6570\u636e","text":"<p>\u56e0\u4e3a\u4f60\u521a\u521a\u5728\u4e0a\u4e00\u8282\u8bfe\u4e2d\u5b66\u4e60\u4e86\u903b\u8f91\u56de\u5f52\uff0c\u6211\u4eec\u8fd9\u91cc\u5c31\u901a\u8fc7\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff0c\u6765\u6f14\u7ec3\u4e00\u4e0b\u5982\u4f55\u8fdb\u884c\u4f60\u7684\u7b2c\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u9996\u5148\uff0c\u9700\u8981\u901a\u8fc7\u8c03\u7528<code>train_test_split()</code>\u65b9\u6cd5\u53ef\u4ee5\u628a\u4f60\u7684\u6570\u636e\u5206\u5272\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n</code></pre>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#-_2","title":"\u7ec3\u4e60 - \u8c03\u7528\u903b\u8f91\u56de\u5f52\u7b97\u6cd5","text":"<p>\u63a5\u4e0b\u6765\uff0c\u4f60\u9700\u8981\u51b3\u5b9a\u9009\u7528\u4ec0\u4e48 scheme \u4ee5\u53ca solver \u6765\u8fdb\u884c\u6211\u4eec\u8fd9\u4e2a\u591a\u7c7b\u578b\u5206\u7c7b\u7684\u6848\u4f8b\u3002\u5728\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 LogisticRegression \u65b9\u6cd5\uff0c\u5e76\u8bbe\u7f6e\u76f8\u5e94\u7684 multi_class \u53c2\u6570\uff0c\u540c\u65f6\u5c06 solver \u8bbe\u7f6e\u4e3a liblinear \u6765\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <ol> <li> <p>\u521b\u5efa\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5c06 multi_class \u8bbe\u7f6e\u4e3a <code>ovr</code>\uff0c\u540c\u65f6\u5c06 solver \u8bbe\u7f6e\u4e3a <code>liblinear</code>:</p> <pre><code>lr = LogisticRegression(multi_class='ovr',solver='liblinear')\nmodel = lr.fit(X_train, np.ravel(y_train))\n\naccuracy = model.score(X_test, y_test)\nprint (\"Accuracy is {}\".format(accuracy))\n</code></pre> <p>\u2705 \u4e5f\u53ef\u4ee5\u8bd5\u8bd5\u5176\u4ed6 solver \u6bd4\u5982 <code>lbfgs</code>, \u8fd9\u4e5f\u662f\u9ed8\u8ba4\u53c2\u6570</p> <p>\u6ce8\u610f, \u4f7f\u7528 Pandas \u7684 <code>ravel</code> \u65b9\u6cd5\u53ef\u4ee5\u5728\u9700\u8981\u7684\u65f6\u5019\u5c06\u4f60\u7684\u6570\u636e\u8fdb\u884c\u964d\u7ef4</p> <p>\u8fd0\u7b97\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230\u51c6\u786e\u7387\u9ad8\u8fbe 80%!</p> </li> <li> <p>\u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u67e5\u770b\u67d0\u4e00\u884c\u6570\u636e\uff08\u6bd4\u5982\u7b2c 50 \u884c\uff09\u6765\u89c2\u6d4b\u5230\u6a21\u578b\u8fd0\u884c\u7684\u60c5\u51b5:</p> <pre><code>print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\nprint(f'cuisine: {y_test.iloc[50]}')\n</code></pre> <p>\u8fd0\u884c\u540e\u7684\u8f93\u51fa\u5982\u4e0b:</p> </li> </ol> <pre><code>ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\ncuisine: indian\n</code></pre> <p>\u2705 \u8bd5\u8bd5\u4e0d\u540c\u7684\u884c\u7d22\u5f15\u6765\u68c0\u67e5\u4e00\u4e0b\u8ba1\u7b97\u7684\u7ed3\u679c\u5427</p> <ol> <li> <p>\u6211\u4eec\u53ef\u4ee5\u518d\u8fdb\u884c\u4e00\u90e8\u6df1\u5165\u7684\u7814\u7a76\uff0c\u68c0\u67e5\u4e00\u4e0b\u672c\u8f6e\u9884\u6d4b\u7ed3\u679c\u7684\u51c6\u786e\u7387:</p> <pre><code>test= X_test.iloc[50].values.reshape(-1, 1).T\nproba = model.predict_proba(test)\nclasses = model.classes_\nresultdf = pd.DataFrame(data=proba, columns=classes)\n\ntopPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\ntopPrediction.head()\n</code></pre> <p>\u8fd0\u884c\u540e\u7684\u8f93\u51fa\u5982\u4e0b\u2014\u2014\u2014\u53ef\u4ee5\u53d1\u73b0\u8fd9\u662f\u4e00\u9053\u5370\u5ea6\u83dc\u7684\u53ef\u80fd\u6027\u6700\u5927\uff0c\u662f\u6700\u5408\u7406\u7684\u731c\u6d4b:</p> 0 indian 0.715851 chinese 0.229475 japanese 0.029763 korean 0.017277 thai 0.007634 <p>\u2705 \u4f60\u80fd\u89e3\u91ca\u4e0b\u4e3a\u4ec0\u4e48\u6a21\u578b\u4f1a\u5982\u6b64\u786e\u5b9a\u8fd9\u662f\u4e00\u9053\u5370\u5ea6\u83dc\u4e48\uff1f</p> </li> <li> <p>\u548c\u4f60\u5728\u4e4b\u524d\u7684\u56de\u5f52\u7684\u8bfe\u7a0b\u4e2d\u6240\u505a\u7684\u4e00\u6837\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8f93\u51fa\u5206\u7c7b\u7684\u62a5\u544a\u5f97\u5230\u5173\u4e8e\u6a21\u578b\u7684\u66f4\u591a\u7684\u7ec6\u8282\uff1a</p> <pre><code>y_pred = model.predict(X_test)\nprint(classification_report(y_test,y_pred))\n</code></pre> precision recall f1-score support chinese 0.73 0.71 0.72 229 indian 0.91 0.93 0.92 254 japanese 0.70 0.75 0.72 220 korean 0.86 0.76 0.81 242 thai 0.79 0.85 0.82 254 accuracy 0.80 1199 macro avg 0.80 0.80 0.80 1199 weighted avg 0.80 0.80 0.80 1199 </li> </ol>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_7","title":"\u6311\u6218","text":"<p>\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u4f60\u4f7f\u7528\u4e86\u6e05\u6d17\u540e\u7684\u6570\u636e\u5efa\u7acb\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u8fd9\u4e2a\u6a21\u578b\u80fd\u591f\u6839\u636e\u8f93\u5165\u7684\u4e00\u7cfb\u5217\u7684\u914d\u6599\u6765\u9884\u6d4b\u83dc\u54c1\u6765\u81ea\u4e8e\u54ea\u4e2a\u56fd\u5bb6\u3002\u8bf7\u518d\u82b1\u70b9\u65f6\u95f4\u9605\u8bfb\u4e00\u4e0b Scikit-learn \u6240\u63d0\u4f9b\u7684\u5173\u4e8e\u53ef\u4ee5\u7528\u6765\u5206\u7c7b\u6570\u636e\u7684\u5176\u4ed6\u65b9\u6cd5\u7684\u8d44\u6599\u3002\u6b64\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u6df1\u5165\u7814\u7a76\u4e00\u4e0b\u201csolver\u201d\u7684\u6982\u5ff5\u5e76\u5c1d\u8bd5\u4e00\u4e0b\u7406\u89e3\u5176\u80cc\u540e\u7684\u539f\u7406\u3002</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_8","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_9","title":"\u56de\u987e\u4e0e\u81ea\u5b66","text":"<p>\u8fd9\u4e2a\u8bfe\u7a0b\u5c06\u5bf9\u903b\u8f91\u56de\u5f52\u80cc\u540e\u7684\u6570\u5b66\u539f\u7406\u8fdb\u884c\u66f4\u52a0\u6df1\u5165\u7684\u8bb2\u89e3</p>"},{"location":"4-Classification/2-Classifiers-1/README.zh-cn/#_10","title":"\u4f5c\u4e1a","text":"<p>\u5b66\u4e60 solver</p>"},{"location":"4-Classification/2-Classifiers-1/assignment/","title":"Study the solvers","text":""},{"location":"4-Classification/2-Classifiers-1/assignment/#instructions","title":"Instructions","text":"<p>In this lesson you learned about the various solvers that pair algorithms with a machine learning process to create an accurate model. Walk through the solvers listed in the lesson and pick two. In your own words, compare and contrast these two solvers. What kind of problem do they address? How do they work with various data structures? Why would you pick one over another? </p>"},{"location":"4-Classification/2-Classifiers-1/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A .doc file is presented with two paragraphs, one on each solver, comparing them thoughtfully. A .doc file is presented with only one paragraph The assignment is incomplete"},{"location":"4-Classification/2-Classifiers-1/assignment.zh-cn/","title":"\u5b66\u4e60\u4e0d\u540c\u7684 solvers","text":""},{"location":"4-Classification/2-Classifiers-1/assignment.zh-cn/#_1","title":"\u8bf4\u660e","text":"<p>\u5728\u672c\u8bfe\u7a0b\uff0c\u60a8\u5b66\u4e60\u4e86\u4f17\u591a\u5c06\u7b97\u6cd5\u4e0e\u673a\u5668\u5b66\u4e60\u8fc7\u7a0b\u7ed3\u5408\u5728\u4e00\u8d77\u6765\u4ea7\u751f\u7cbe\u786e\u6a21\u578b\u7684 solvers \u3002\u56de\u987e\u672c\u8bfe\u7a0b\u4e2d\u5217\u51fa\u7684\u6240\u6709 solvers \u5e76\u6311\u9009\u4e24\u4e2a\uff0c\u7528\u60a8\u81ea\u5df1\u7684\u8bed\u8a00\u6bd4\u8f83\u548c\u5bf9\u6bd4\u8fd9\u4e24\u4e2a solver\u3002\u5b83\u4eec\u89e3\u51b3\u4e86\u4ec0\u4e48\u6837\u7684\u95ee\u9898\uff1f\u5b83\u4eec\u5982\u4f55\u5904\u7406\u5404\u79cd\u6570\u636e\u7ed3\u6784\uff1f\u60a8\u4e3a\u4ec0\u4e48\u8981\u9009\u62e9\u67d0\u4e2a solver \u800c\u4e0d\u662f\u53e6\u4e00\u4e2a\uff1f</p>"},{"location":"4-Classification/2-Classifiers-1/assignment.zh-cn/#_2","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u4e00\u4efd\u6709\u4e24\u4e2a\u6bb5\u843d\u7684 .doc \u6587\u4ef6\uff0c\u6bcf\u4e2a\u6bb5\u843d\u63cf\u8ff0\u4e00\u4e2a solver, \u5e76\u8be6\u7ec6\u6bd4\u8f83\u5b83\u4eec\u95f4\u7684\u5f02\u540c \u63d0\u4ea4\u4e86\u4e00\u4efd\u4ec5\u6709\u4e00\u4e2a\u6bb5\u843d\u7684 .doc \u6587\u4ef6 \u6ca1\u6709\u5b8c\u6210"},{"location":"4-Classification/2-Classifiers-1/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"4-Classification/3-Classifiers-2/","title":"Cuisine classifiers 2","text":"<p>In this second classification lesson, you will explore more ways to classify numeric data. You will also learn about the ramifications for choosing one classifier over the other.</p>"},{"location":"4-Classification/3-Classifiers-2/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"4-Classification/3-Classifiers-2/#prerequisite","title":"Prerequisite","text":"<p>We assume that you have completed the previous lessons and have a cleaned dataset in your <code>data</code> folder called cleaned_cuisines.csv in the root of this 4-lesson folder.</p>"},{"location":"4-Classification/3-Classifiers-2/#preparation","title":"Preparation","text":"<p>We have loaded your notebook.ipynb file with the cleaned dataset and have divided it into X and y dataframes, ready for the model building process.</p>"},{"location":"4-Classification/3-Classifiers-2/#a-classification-map","title":"A classification map","text":"<p>Previously, you learned about the various options you have when classifying data using Microsoft's cheat sheet. Scikit-learn offers a similar, but more granular cheat sheet that can further help narrow down your estimators (another term for classifiers):</p> <p></p> <p>Tip: visit this map online and click along the path to read documentation.</p>"},{"location":"4-Classification/3-Classifiers-2/#the-plan","title":"The plan","text":"<p>This map is very helpful once you have a clear grasp of your data, as you can 'walk' along its paths to a decision:</p> <ul> <li>We have &gt;50 samples</li> <li>We want to predict a category</li> <li>We have labeled data</li> <li>We have fewer than 100K samples</li> <li>\u2728 We can choose a Linear SVC</li> <li>If that doesn't work, since we have numeric data<ul> <li>We can try a \u2728 KNeighbors Classifier </li> <li>If that doesn't work, try \u2728 SVC and \u2728 Ensemble Classifiers</li> </ul> </li> </ul> <p>This is a very helpful trail to follow.</p>"},{"location":"4-Classification/3-Classifiers-2/#exercise-split-the-data","title":"Exercise - split the data","text":"<p>Following this path, we should start by importing some libraries to use.</p> <ol> <li> <p>Import the needed libraries:</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\nimport numpy as np\n</code></pre> </li> <li> <p>Split your training and test data:</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n</code></pre> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/#linear-svc-classifier","title":"Linear SVC classifier","text":"<p>Support-Vector clustering (SVC) is a child of the Support-Vector machines family of ML techniques (learn more about these below). In this method, you can choose a 'kernel' to decide how to cluster the labels. The 'C' parameter refers to 'regularization' which regulates the influence of parameters. The kernel can be one of several; here we set it to 'linear' to ensure that we leverage linear SVC. Probability defaults to 'false'; here we set it to 'true' to gather probability estimates. We set the random state to '0' to shuffle the data to get probabilities.</p>"},{"location":"4-Classification/3-Classifiers-2/#exercise-apply-a-linear-svc","title":"Exercise - apply a linear SVC","text":"<p>Start by creating an array of classifiers. You will add progressively to this array as we test. </p> <ol> <li> <p>Start with a Linear SVC:</p> <pre><code>C = 10\n# Create different classifiers.\nclassifiers = {\n    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)\n}\n</code></pre> </li> <li> <p>Train your model using the Linear SVC and print out a report:</p> <pre><code>n_classifiers = len(classifiers)\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X_train, np.ravel(y_train))\n\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n    print(classification_report(y_test,y_pred))\n</code></pre> <p>The result is pretty good:</p> <pre><code>Accuracy (train) for Linear SVC: 78.6% \n              precision    recall  f1-score   support\n\n     chinese       0.71      0.67      0.69       242\n      indian       0.88      0.86      0.87       234\n    japanese       0.79      0.74      0.76       254\n      korean       0.85      0.81      0.83       242\n        thai       0.71      0.86      0.78       227\n\n    accuracy                           0.79      1199\n   macro avg       0.79      0.79      0.79      1199\nweighted avg       0.79      0.79      0.79      1199\n</code></pre> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/#k-neighbors-classifier","title":"K-Neighbors classifier","text":"<p>K-Neighbors is part of the \"neighbors\" family of ML methods, which can be used for both supervised and unsupervised learning. In this method, a predefined number of points is created and data are gathered around these points such that generalized labels can be predicted for the data.</p>"},{"location":"4-Classification/3-Classifiers-2/#exercise-apply-the-k-neighbors-classifier","title":"Exercise - apply the K-Neighbors classifier","text":"<p>The previous classifier was good, and worked well with the data, but maybe we can get better accuracy. Try a K-Neighbors classifier.</p> <ol> <li> <p>Add a line to your classifier array (add a comma after the Linear SVC item):</p> <pre><code>'KNN classifier': KNeighborsClassifier(C),\n</code></pre> <p>The result is a little worse:</p> <pre><code>Accuracy (train) for KNN classifier: 73.8% \n              precision    recall  f1-score   support\n\n     chinese       0.64      0.67      0.66       242\n      indian       0.86      0.78      0.82       234\n    japanese       0.66      0.83      0.74       254\n      korean       0.94      0.58      0.72       242\n        thai       0.71      0.82      0.76       227\n\n    accuracy                           0.74      1199\n   macro avg       0.76      0.74      0.74      1199\nweighted avg       0.76      0.74      0.74      1199\n</code></pre> <p>\u2705 Learn about K-Neighbors</p> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/#support-vector-classifier","title":"Support Vector Classifier","text":"<p>Support-Vector classifiers are part of the Support-Vector Machine family of ML methods that are used for classification and regression tasks. SVMs \"map training examples to points in space\" to maximize the distance between two categories. Subsequent data is mapped into this space so their category can be predicted.</p>"},{"location":"4-Classification/3-Classifiers-2/#exercise-apply-a-support-vector-classifier","title":"Exercise - apply a Support Vector Classifier","text":"<p>Let's try for a little better accuracy with a Support Vector Classifier.</p> <ol> <li> <p>Add a comma after the K-Neighbors item, and then add this line:</p> <pre><code>'SVC': SVC(),\n</code></pre> <p>The result is quite good!</p> <pre><code>Accuracy (train) for SVC: 83.2% \n              precision    recall  f1-score   support\n\n     chinese       0.79      0.74      0.76       242\n      indian       0.88      0.90      0.89       234\n    japanese       0.87      0.81      0.84       254\n      korean       0.91      0.82      0.86       242\n        thai       0.74      0.90      0.81       227\n\n    accuracy                           0.83      1199\n   macro avg       0.84      0.83      0.83      1199\nweighted avg       0.84      0.83      0.83      1199\n</code></pre> <p>\u2705 Learn about Support-Vectors</p> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/#ensemble-classifiers","title":"Ensemble Classifiers","text":"<p>Let's follow the path to the very end, even though the previous test was quite good. Let's try some 'Ensemble Classifiers, specifically Random Forest and AdaBoost:</p> <pre><code>  'RFST': RandomForestClassifier(n_estimators=100),\n  'ADA': AdaBoostClassifier(n_estimators=100)\n</code></pre> <p>The result is very good, especially for Random Forest:</p> <pre><code>Accuracy (train) for RFST: 84.5% \n              precision    recall  f1-score   support\n\n     chinese       0.80      0.77      0.78       242\n      indian       0.89      0.92      0.90       234\n    japanese       0.86      0.84      0.85       254\n      korean       0.88      0.83      0.85       242\n        thai       0.80      0.87      0.83       227\n\n    accuracy                           0.84      1199\n   macro avg       0.85      0.85      0.84      1199\nweighted avg       0.85      0.84      0.84      1199\n\nAccuracy (train) for ADA: 72.4% \n              precision    recall  f1-score   support\n\n     chinese       0.64      0.49      0.56       242\n      indian       0.91      0.83      0.87       234\n    japanese       0.68      0.69      0.69       254\n      korean       0.73      0.79      0.76       242\n        thai       0.67      0.83      0.74       227\n\n    accuracy                           0.72      1199\n   macro avg       0.73      0.73      0.72      1199\nweighted avg       0.73      0.72      0.72      1199\n</code></pre> <p>\u2705 Learn about Ensemble Classifiers</p> <p>This method of Machine Learning \"combines the predictions of several base estimators\" to improve the model's quality. In our example, we used Random Trees and AdaBoost. </p> <ul> <li> <p>Random Forest, an averaging method, builds a 'forest' of 'decision trees' infused with randomness to avoid overfitting. The n_estimators parameter is set to the number of trees.</p> </li> <li> <p>AdaBoost fits a classifier to a dataset and then fits copies of that classifier to the same dataset. It focuses on the weights of incorrectly classified items and adjusts the fit for the next classifier to correct.</p> </li> </ul>"},{"location":"4-Classification/3-Classifiers-2/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Each of these techniques has a large number of parameters that you can tweak. Research each one's default parameters and think about what tweaking these parameters would mean for the model's quality.</p>"},{"location":"4-Classification/3-Classifiers-2/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"4-Classification/3-Classifiers-2/#review-self-study","title":"Review &amp; Self Study","text":"<p>There's a lot of jargon in these lessons, so take a minute to review this list of useful terminology!</p>"},{"location":"4-Classification/3-Classifiers-2/#assignment","title":"Assignment","text":"<p>Parameter play</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/","title":"\u83dc\u54c1\u5206\u7c7b\u5668 2","text":"<p>\u5728\u7b2c\u4e8c\u8282\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u5c06\u63a2\u7d22\u66f4\u591a\u65b9\u6cd5\u6765\u5bf9\u6570\u503c\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002\u60a8\u8fd8\u5c06\u4e86\u89e3\u9009\u62e9\u4e0d\u540c\u7684\u5206\u7c7b\u5668\u6240\u5e26\u6765\u7684\u7ed3\u679c\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_2","title":"\u5148\u51b3\u6761\u4ef6","text":"<p>\u6211\u4eec\u5047\u8bbe\u60a8\u5df2\u7ecf\u5b8c\u6210\u4e86\u524d\u9762\u7684\u8bfe\u7a0b\uff0c\u5e76\u4e14\u5728\u672c\u6b21\u8bfe\u7a0b\u6587\u4ef6\u5939\u6839\u8def\u5f84\u4e0b\u7684 <code>data</code> \u6587\u4ef6\u5939\u4e2d\u6709\u4e00\u4e2a\u7ecf\u8fc7\u6e05\u6d17\u7684\u540d\u4e3a cleaned_cuisines.csv \u6570\u636e\u96c6\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_3","title":"\u51c6\u5907\u5de5\u4f5c","text":"<p>\u6211\u4eec\u5df2\u7ecf\u5c06\u6e05\u6d17\u8fc7\u7684\u6570\u636e\u96c6\u52a0\u8f7d\u8fdb\u60a8\u7684 notebook.ipynb \u6587\u4ef6\uff0c\u5e76\u5206\u4e3a X \u548c Y dataframe\uff0c\u4e3a\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u505a\u597d\u51c6\u5907\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_4","title":"\u5206\u7c7b\u5b66\u4e60\u8def\u7ebf\u56fe","text":"<p>\u5728\u6b64\u4e4b\u524d\uff0c\u60a8\u5df2\u7ecf\u4e86\u89e3\u4f7f\u7528 Microsoft \u901f\u67e5\u8868\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u65f6\u53ef\u4ee5\u4f7f\u7528\u5230\u7684\u5404\u79cd\u9009\u9879\u3002Scikit-learn \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u7684\uff0c\u4f46\u66f4\u7ec6\u7c92\u5ea6\u7684\u901f\u67e5\u8868\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5e2e\u52a9\u60a8\u8c03\u6574\u4f30\u8ba1\u5668(\u5206\u7c7b\u5668\u7684\u53e6\u4e00\u4e2a\u672f\u8bed)\uff1a</p> <p></p> <p>\u63d0\u793a\uff1a\u5728\u7ebf\u67e5\u770b\u8def\u7ebf\u56fe\u5e76\u6cbf\u7740\u8def\u7ebf\u9605\u8bfb\u6587\u6863\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_5","title":"\u8ba1\u5212","text":"<p>\u4e00\u65e6\u60a8\u6e05\u695a\u4e86\u89e3\u4e86\u60a8\u7684\u6570\u636e\uff0c\u8fd9\u5f20\u8def\u7ebf\u56fe\u5c31\u975e\u5e38\u6709\u7528\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u6cbf\u7740\u8def\u7ebf\u5e76\u505a\u51fa\u51b3\u5b9a\uff1a</p> <ul> <li>\u6211\u4eec\u6709\u8d85\u8fc7 50 \u4e2a\u6837\u672c</li> <li>\u6211\u4eec\u60f3\u8981\u9884\u6d4b\u4e00\u4e2a\u7c7b\u522b</li> <li>\u6211\u4eec\u6709\u6807\u8bb0\u8fc7\u7684\u6570\u636e</li> <li>\u6211\u4eec\u7684\u6837\u672c\u6570\u5c11\u4e8e 100000</li> <li>\u2728 \u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u7ebf\u6027 SVC</li> <li>\u5982\u679c\u90a3\u4e0d\u8d77\u4f5c\u7528\uff0c\u65e2\u7136\u6211\u4eec\u6709\u6570\u503c\u6570\u636e</li> <li>\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5 \u2728 K-\u8fd1\u90bb\u5206\u7c7b\u5668<ul> <li>\u5982\u679c\u90a3\u4e0d\u8d77\u4f5c\u7528\uff0c\u8bd5\u8bd5 \u2728 SVC \u548c \u2728 \u96c6\u6210\u5206\u7c7b\u5668</li> </ul> </li> </ul> <p>\u8fd9\u662f\u4e00\u6761\u975e\u5e38\u6709\u7528\u7684\u7ebf\u7d22\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u62c6\u5206\u6570\u636e","text":"<p>\u6309\u7167\u8fd9\u4e2a\u8def\u7ebf\uff0c\u6211\u4eec\u5e94\u8be5\u4ece\u5bfc\u5165\u4e00\u4e9b\u8981\u4f7f\u7528\u7684\u5e93\u6765\u5f00\u59cb\u3002</p> <ol> <li> <p>\u5bfc\u5165\u9700\u8981\u7684\u5e93\uff1a</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\nimport numpy as np\n</code></pre> </li> <li> <p>\u62c6\u5206\u60a8\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\uff1a</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n</code></pre> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#svc","title":"\u7ebf\u6027 SVC \u5206\u7c7b\u5668","text":"<p>\u652f\u6301\u5411\u91cf\u5206\u7c7b\uff08SVC\uff09\u662f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u652f\u6301\u5411\u91cf\u673a\u5bb6\u65cf\u4e2d\u7684\u4e00\u4e2a\u5b50\u7c7b\uff08\u53c2\u9605\u4e0b\u65b9\u5185\u5bb9\uff0c\u5b66\u4e60\u66f4\u591a\u76f8\u5173\u77e5\u8bc6\uff09\u3002\u7528\u8fd9\u79cd\u65b9\u6cd5\u60a8\u53ef\u4ee5\u9009\u62e9\u4e00\u4e2a  kernel \u53bb\u51b3\u5b9a\u5982\u4f55\u805a\u7c7b\u6807\u7b7e\u3002C \u53c2\u6570\u6307\u7684\u662f\u201c\u6b63\u5219\u5316\u201d\uff0c\u5b83\u5c06\u53c2\u6570\u7684\u5f71\u54cd\u6b63\u5219\u5316\u3002kernel \u53ef\u4ee5\u662f\u5176\u4e2d\u7684\u4e00\u9879\uff1b\u8fd9\u91cc\u6211\u4eec\u5c06 kernel \u8bbe\u7f6e\u4e3a linear \u6765\u4f7f\u7528\u7ebf\u6027 SVC\u3002probability \u9ed8\u8ba4\u4e3a false\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u5176\u8bbe\u7f6e\u4e3a true \u6765\u6536\u96c6\u6982\u7387\u4f30\u8ba1\u3002\u6211\u4eec\u8fd8\u5c06 random_state \u8bbe\u7f6e\u4e3a 0 \u53bb\u6253\u4e71\u6570\u636e\u6765\u83b7\u5f97\u6982\u7387\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#-svc","title":"\u7ec3\u4e60 - \u4f7f\u7528\u7ebf\u6027 SVC","text":"<p>\u6211\u4eec\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5206\u7c7b\u5668\u6570\u7ec4\u6765\u5f00\u59cb\u3002\u5728\u6211\u4eec\u6d4b\u8bd5\u65f6\u60a8\u53ef\u4ee5\u9010\u6b65\u5411\u8fd9\u4e2a\u6570\u7ec4\u4e2d\u6dfb\u52a0\u5206\u7c7b\u5668\u3002</p> <ol> <li> <p>\u4ece\u4e00\u4e2a\u7ebf\u6027 SVC \u5f00\u59cb\uff1a</p> <pre><code>C = 10\n# \u521b\u5efa\u4e0d\u540c\u7684\u5206\u7c7b\u5668\nclassifiers = {\n    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)\n}\n</code></pre> </li> <li> <p>\u4f7f\u7528\u7ebf\u6027 SVC \u8bad\u7ec3\u60a8\u7684\u6a21\u578b\u5e76\u6253\u5370\u62a5\u544a\uff1a</p> <pre><code>n_classifiers = len(classifiers)\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X_train, np.ravel(y_train))\n\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n    print(classification_report(y_test,y_pred))\n</code></pre> <p>\u7ed3\u679c\u770b\u4e0a\u53bb\u4e0d\u9519\uff1a</p> <pre><code>Accuracy (train) for Linear SVC: 78.6% \n              precision    recall  f1-score   support\n\n     chinese       0.71      0.67      0.69       242\n      indian       0.88      0.86      0.87       234\n    japanese       0.79      0.74      0.76       254\n      korean       0.85      0.81      0.83       242\n        thai       0.71      0.86      0.78       227\n\n    accuracy                           0.79      1199\n   macro avg       0.79      0.79      0.79      1199\nweighted avg       0.79      0.79      0.79      1199\n</code></pre> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#k-","title":"K-\u8fd1\u90bb\u5206\u7c7b\u5668","text":"<p>K-\u8fd1\u90bb\u662f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6700\u8fd1\u90bb\u5bb6\u65cf\u7684\u4e00\u90e8\u5206\uff0c\u53ef\u4ee5\u7528\u6765\u8fdb\u884c\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u3002\u8fd9\u79cd\u65b9\u6cd5\u521b\u5efa\u4e86\u9884\u5b9a\u4e2a\u6570\u7684\u70b9\uff0c\u5e76\u4e14\u6570\u636e\u88ab\u805a\u96c6\u5728\u8fd9\u4e9b\u70b9\u7684\u56db\u5468\uff0c\u8fd9\u6837\u6570\u636e\u7684\u5927\u81f4\u6807\u7b7e\u53ef\u4ee5\u88ab\u9884\u6d4b\u51fa\u6765\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#-k-","title":"\u7ec3\u4e60 - \u4f7f\u7528 K-\u8fd1\u90bb\u5206\u7c7b\u5668","text":"<p>\u524d\u9762\u7684\u5206\u7c7b\u5668\u90fd\u5f88\u4e0d\u9519\uff0c\u5e76\u4e14\u80fd\u5728\u6570\u636e\u96c6\u4e0a\u8d77\u4f5c\u7528\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u80fd\u9700\u8981\u66f4\u597d\u7684\u7cbe\u5ea6\u3002\u6765\u8bd5\u8bd5 K-\u8fd1\u90bb\u5206\u7c7b\u5668\u3002</p> <ol> <li> <p>\u7ed9\u60a8\u7684\u5206\u7c7b\u5668\u6570\u7ec4\u6dfb\u52a0\u4e00\u884c\uff08\u5728\u7ebf\u6027 SVC \u5206\u7c7b\u5668\u540e\u6dfb\u52a0\u9017\u53f7\uff09\uff1a</p> <pre><code>'KNN classifier': KNeighborsClassifier(C),\n</code></pre> <p>\u7ed3\u679c\u6709\u70b9\u7cdf\u7cd5\uff1a</p> <pre><code>Accuracy (train) for KNN classifier: 73.8% \n              precision    recall  f1-score   support\n\n     chinese       0.64      0.67      0.66       242\n      indian       0.86      0.78      0.82       234\n    japanese       0.66      0.83      0.74       254\n      korean       0.94      0.58      0.72       242\n        thai       0.71      0.82      0.76       227\n\n    accuracy                           0.74      1199\n   macro avg       0.76      0.74      0.74      1199\nweighted avg       0.76      0.74      0.74      1199\n</code></pre> <p>\u2705 \u4e86\u89e3 K-\u8fd1\u90bb</p> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#support-vector","title":"Support Vector \u5206\u7c7b\u5668","text":"<p>Support-Vector \u5206\u7c7b\u5668\u662f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u652f\u6301\u5411\u91cf\u673a\u5bb6\u65cf\u7684\u4e00\u90e8\u5206\uff0c\u88ab\u7528\u4e8e\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u3002\u4e3a\u4e86\u6700\u5927\u5316\u4e24\u4e2a\u7c7b\u522b\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u652f\u6301\u5411\u91cf\u673a\u5c06\u201c\u8bad\u7ec3\u6837\u4f8b\u6620\u5c04\u4e3a\u7a7a\u95f4\u4e2d\u4e0d\u540c\u7684\u70b9\u201d\u3002\u7136\u540e\u6570\u636e\u88ab\u6620\u5c04\u4e3a\u8ddd\u79bb\uff0c\u6240\u4ee5\u5b83\u4eec\u7684\u7c7b\u522b\u53ef\u4ee5\u5f97\u5230\u9884\u6d4b\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#-support-vector","title":"\u7ec3\u4e60 - \u4f7f\u7528 Support Vector \u5206\u7c7b\u5668","text":"<p>\u4e3a\u4e86\u66f4\u597d\u7684\u7cbe\u5ea6\uff0c\u6211\u4eec\u5c1d\u8bd5 Support Vector \u5206\u7c7b\u5668\u3002</p> <ol> <li> <p>\u5728 K-\u8fd1\u90bb\u5206\u7c7b\u5668\u540e\u6dfb\u52a0\u9017\u53f7\uff0c\u7136\u540e\u6dfb\u52a0\u4e0b\u9762\u4e00\u884c\uff1a</p> <pre><code>'SVC': SVC(),\n</code></pre> <p>\u7ed3\u679c\u76f8\u5f53\u4e0d\u9519\uff01</p> <pre><code>Accuracy (train) for SVC: 83.2% \n              precision    recall  f1-score   support\n\n     chinese       0.79      0.74      0.76       242\n      indian       0.88      0.90      0.89       234\n    japanese       0.87      0.81      0.84       254\n      korean       0.91      0.82      0.86       242\n        thai       0.74      0.90      0.81       227\n\n    accuracy                           0.83      1199\n   macro avg       0.84      0.83      0.83      1199\nweighted avg       0.84      0.83      0.83      1199\n</code></pre> <p>\u2705 \u4e86\u89e3 Support-Vectors</p> </li> </ol>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_6","title":"\u96c6\u6210\u5206\u7c7b\u5668","text":"<p>\u5c3d\u7ba1\u4e4b\u524d\u7684\u6d4b\u8bd5\u7ed3\u679c\u76f8\u5f53\u4e0d\u9519\uff0c\u6211\u4eec\u8fd8\u662f\u6cbf\u7740\u8def\u7ebf\u8d70\u5230\u6700\u540e\u5427\u3002\u6211\u4eec\u6765\u5c1d\u8bd5\u4e00\u4e9b\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u7279\u522b\u662f\u968f\u673a\u68ee\u6797\u548c AdaBoost\uff1a</p> <pre><code>  'RFST': RandomForestClassifier(n_estimators=100),\n  'ADA': AdaBoostClassifier(n_estimators=100)\n</code></pre> <p>\u7ed3\u679c\u975e\u5e38\u597d\uff0c\u5c24\u5176\u662f\u968f\u673a\u68ee\u6797\u65b9\u6cd5\u7684\uff1a</p> <pre><code>Accuracy (train) for RFST: 84.5% \n              precision    recall  f1-score   support\n\n     chinese       0.80      0.77      0.78       242\n      indian       0.89      0.92      0.90       234\n    japanese       0.86      0.84      0.85       254\n      korean       0.88      0.83      0.85       242\n        thai       0.80      0.87      0.83       227\n\n    accuracy                           0.84      1199\n   macro avg       0.85      0.85      0.84      1199\nweighted avg       0.85      0.84      0.84      1199\n\nAccuracy (train) for ADA: 72.4% \n              precision    recall  f1-score   support\n\n     chinese       0.64      0.49      0.56       242\n      indian       0.91      0.83      0.87       234\n    japanese       0.68      0.69      0.69       254\n      korean       0.73      0.79      0.76       242\n        thai       0.67      0.83      0.74       227\n\n    accuracy                           0.72      1199\n   macro avg       0.73      0.73      0.72      1199\nweighted avg       0.73      0.72      0.72      1199\n</code></pre> <p>\u2705 \u5b66\u4e60\u96c6\u6210\u5206\u7c7b\u5668</p> <p>\u8fd9\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\"\u7ec4\u5408\u4e86\u5404\u79cd\u57fa\u672c\u4f30\u8ba1\u5668\u7684\u9884\u6d4b\"\u6765\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002\u5728\u6211\u4eec\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548c AdaBoost\u3002</p> <ul> <li> <p>\u968f\u673a\u68ee\u6797\u662f\u4e00\u79cd\u5e73\u5747\u5316\u65b9\u6cd5\uff0c\u5b83\u5efa\u7acb\u4e86\u4e00\u4e2a\u6ce8\u5165\u4e86\u968f\u673a\u6027\u7684\u201c\u51b3\u7b56\u6811\u68ee\u6797\u201d\u4ee5\u907f\u514d\u8fc7\u5ea6\u62df\u5408\u3002n_estimators \u53c2\u6570\u8bbe\u7f6e\u4e86\u968f\u673a\u68ee\u6797\u4e2d\u6811\u7684\u6570\u91cf\u3002</p> </li> <li> <p>AdaBoost \u5728\u6570\u636e\u96c6\u4e0a\u62df\u5408\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u7136\u540e\u5728\u540c\u4e00\u6570\u636e\u96c6\u4e0a\u62df\u5408\u5206\u7c7b\u5668\u7684\u989d\u5916\u526f\u672c\u3002\u5b83\u5173\u6ce8\u5e76\u8c03\u6574\u9519\u8bef\u5206\u7c7b\u5b9e\u4f8b\u7684\u6743\u91cd\uff0c\u4ee5\u4fbf\u540e\u7eed\u7684\u5206\u7c7b\u5668\u66f4\u591a\u5730\u5173\u6ce8\u548c\u4fee\u6b63\u3002</p> </li> </ul>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_7","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u8fd9\u4e9b\u6280\u672f\u65b9\u6cd5\u6bcf\u4e2a\u90fd\u6709\u5f88\u591a\u80fd\u591f\u8ba9\u60a8\u5fae\u8c03\u7684\u53c2\u6570\u3002\u7814\u7a76\u6bcf\u4e00\u4e2a\u7684\u9ed8\u8ba4\u53c2\u6570\uff0c\u5e76\u601d\u8003\u8c03\u6574\u8fd9\u4e9b\u53c2\u6570\u5bf9\u6a21\u578b\u8d28\u91cf\u6709\u4f55\u610f\u4e49\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_8","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_9","title":"\u56de\u987e\u4e0e\u81ea\u5b66","text":"<p>\u8bfe\u7a0b\u4e2d\u51fa\u73b0\u4e86\u5f88\u591a\u672f\u8bed\uff0c\u82b1\u70b9\u65f6\u95f4\u6d4f\u89c8\u672f\u8bed\u8868\u6765\u590d\u4e60\u4e00\u4e0b\u5b83\u4eec\u5427\uff01</p>"},{"location":"4-Classification/3-Classifiers-2/README.zh-cn/#_10","title":"\u4f5c\u4e1a","text":"<p>\u73a9\u8f6c\u53c2\u6570</p>"},{"location":"4-Classification/3-Classifiers-2/assignment/","title":"Parameter Play","text":""},{"location":"4-Classification/3-Classifiers-2/assignment/#instructions","title":"Instructions","text":"<p>There are a lot of parameters that are set by default when working with these classifiers. Intellisense in VS Code can help you dig into them. Adopt one of the ML Classification Techniques in this lesson and retrain models tweaking various parameter values. Build a notebook explaining why some changes help the model quality while others degrade it. Be detailed in your answer.</p>"},{"location":"4-Classification/3-Classifiers-2/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with a classifier fully built up and its parameters tweaked and changes explained in textboxes A notebook is partially presented or poorly explained A notebook is buggy or flawed"},{"location":"4-Classification/3-Classifiers-2/assignment.zh-cn/","title":"\u73a9\u8f6c\u53c2\u6570","text":""},{"location":"4-Classification/3-Classifiers-2/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u4f7f\u7528\u5206\u7c7b\u5668\u65f6\uff0c\u6709\u5f88\u591a\u88ab\u9ed8\u8ba4\u8bbe\u7f6e\u4e86\u7684\u53c2\u6570\u3002Vs Code \u4e2d\u7684 Intellisense \u53ef\u4ee5\u5e2e\u52a9\u60a8\u6df1\u5165\u4e86\u89e3\u5b83\u4eec\u3002\u9009\u7528\u672c\u8bfe\u7a0b\u4e2d\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u65b9\u6cd5\u4e2d\u7684\u4e00\u4e2a\uff0c\u8c03\u6574\u5404\u79cd\u53c2\u6570\uff0c\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002\u6784\u5efa\u4e00\u4e2a notebook \u5de5\u7a0b\u6765\u89e3\u91ca\u4e3a\u4ec0\u4e48\u6709\u4e9b\u53c2\u6570\u6539\u53d8\u6709\u52a9\u4e8e\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\uff0c\u800c\u5176\u4ed6\u6539\u53d8\u4f1a\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u3002\u8bf7\u5728\u60a8\u7684\u56de\u7b54\u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u3002</p>"},{"location":"4-Classification/3-Classifiers-2/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u4e00\u4e2a notebook \u5de5\u7a0b\u6587\u4ef6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u4e14\u53c2\u6570\u8c03\u6574\u8fc7\u7684\u5206\u7c7b\u5668\uff0c\u5e76\u5bf9\u53c2\u6570\u6539\u53d8\u8fdb\u884c\u89e3\u91ca \u63d0\u4ea4\u4e86\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u6216\u6ca1\u6709\u8be6\u7ec6\u89e3\u91ca\u7684 notebook \u5de5\u7a0b\u6587\u4ef6 notebook \u5de5\u7a0b\u6587\u4ef6\u6709\u9519\u8bef\u6216\u8005\u6709\u7f3a\u9677"},{"location":"4-Classification/3-Classifiers-2/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"4-Classification/4-Applied/","title":"Build a Cuisine Recommender Web App","text":"<p>In this lesson, you will build a classification model using some of the techniques you have learned in previous lessons and with the delicious cuisine dataset used throughout this series. In addition, you will build a small web app to use a saved model, leveraging Onnx's web runtime.</p> <p>One of the most useful practical uses of machine learning is building recommendation systems, and you can take the first step in that direction today!</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Jen Looper builds a web app using classified cuisine data</p>"},{"location":"4-Classification/4-Applied/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>In this lesson you will learn:</p> <ul> <li>How to build a model and save it as an Onnx model</li> <li>How to use Netron to inspect the model</li> <li>How to use your model in a web app for inference</li> </ul>"},{"location":"4-Classification/4-Applied/#build-your-model","title":"Build your model","text":"<p>Building applied ML systems is an important part of leveraging these technologies for your business systems. You can use models within your web applications (and thus use them in an offline context if needed) by using Onnx.</p> <p>In a previous lesson, you built a Regression model about UFO sightings, \"pickled\" it, and used it in a Flask app. While this architecture is very useful to know, it is a full-stack Python app, and your requirements may include the use of a JavaScript application. </p> <p>In this lesson, you can build a basic JavaScript-based system for inference. First, however, you need to train a model and convert it for use with Onnx.</p>"},{"location":"4-Classification/4-Applied/#exercise-train-classification-model","title":"Exercise - train classification model","text":"<p>First, train a classification model using the cleaned cuisines dataset we used. </p> <ol> <li> <p>Start by importing useful libraries:</p> <pre><code>!pip install skl2onnx\nimport pandas as pd \n</code></pre> <p>You need 'skl2onnx' to help convert your Scikit-learn model to Onnx format.</p> </li> <li> <p>Then, work with your data in the same way you did in previous lessons, by reading a CSV file using <code>read_csv()</code>:</p> <pre><code>data = pd.read_csv('../data/cleaned_cuisines.csv')\ndata.head()\n</code></pre> </li> <li> <p>Remove the first two unnecessary columns and save the remaining data as 'X':</p> <pre><code>X = data.iloc[:,2:]\nX.head()\n</code></pre> </li> <li> <p>Save the labels as 'y':</p> <pre><code>y = data[['cuisine']]\ny.head()\n</code></pre> </li> </ol>"},{"location":"4-Classification/4-Applied/#commence-the-training-routine","title":"Commence the training routine","text":"<p>We will use the 'SVC' library which has good accuracy.</p> <ol> <li> <p>Import the appropriate libraries from Scikit-learn:</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report\n</code></pre> </li> <li> <p>Separate training and test sets:</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n</code></pre> </li> <li> <p>Build an SVC Classification model as you did in the previous lesson:</p> <pre><code>model = SVC(kernel='linear', C=10, probability=True,random_state=0)\nmodel.fit(X_train,y_train.values.ravel())\n</code></pre> </li> <li> <p>Now, test your model, calling <code>predict()</code>:</p> <pre><code>y_pred = model.predict(X_test)\n</code></pre> </li> <li> <p>Print out a classification report to check the model's quality:</p> <pre><code>print(classification_report(y_test,y_pred))\n</code></pre> <p>As we saw before, the accuracy is good:</p> <pre><code>                precision    recall  f1-score   support\n\n     chinese       0.72      0.69      0.70       257\n      indian       0.91      0.87      0.89       243\n    japanese       0.79      0.77      0.78       239\n      korean       0.83      0.79      0.81       236\n        thai       0.72      0.84      0.78       224\n\n    accuracy                           0.79      1199\n   macro avg       0.79      0.79      0.79      1199\nweighted avg       0.79      0.79      0.79      1199\n</code></pre> </li> </ol>"},{"location":"4-Classification/4-Applied/#convert-your-model-to-onnx","title":"Convert your model to Onnx","text":"<p>Make sure to do the conversion with the proper Tensor number. This dataset has 380 ingredients listed, so you need to notate that number in <code>FloatTensorType</code>:</p> <ol> <li> <p>Convert using a tensor number of 380.</p> <pre><code>from skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\n\ninitial_type = [('float_input', FloatTensorType([None, 380]))]\noptions = {id(model): {'nocl': True, 'zipmap': False}}\n</code></pre> </li> <li> <p>Create the onx and store as a file model.onnx:</p> <pre><code>onx = convert_sklearn(model, initial_types=initial_type, options=options)\nwith open(\"./model.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n</code></pre> <p>Note, you can pass in options in your conversion script. In this case, we passed in 'nocl' to be True and 'zipmap' to be False. Since this is a classification model, you have the option to remove ZipMap which produces a list of dictionaries (not necessary). <code>nocl</code> refers to class information being included in the model. Reduce your model's size by setting <code>nocl</code> to 'True'. </p> </li> </ol> <p>Running the entire notebook will now build an Onnx model and save it to this folder.</p>"},{"location":"4-Classification/4-Applied/#view-your-model","title":"View your model","text":"<p>Onnx models are not very visible in Visual Studio code, but there's a very good free software that many researchers use to visualize the model to ensure that it is properly built. Download Netron and  open your model.onnx file. You can see your simple model visualized, with its 380 inputs and classifier listed:</p> <p></p> <p>Netron is a helpful tool to view your models.</p> <p>Now you are ready to use this neat model in a web app. Let's build an app that will come in handy when you look in your refrigerator and try to figure out which combination of your leftover ingredients you can use to cook a given cuisine, as determined by your model.</p>"},{"location":"4-Classification/4-Applied/#build-a-recommender-web-application","title":"Build a recommender web application","text":"<p>You can use your model directly in a web app. This architecture also allows you to run it locally and even offline if needed. Start by creating an <code>index.html</code> file in the same folder where you stored your <code>model.onnx</code> file.</p> <ol> <li> <p>In this file index.html, add the following markup:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;header&gt;\n        &lt;title&gt;Cuisine Matcher&lt;/title&gt;\n    &lt;/header&gt;\n    &lt;body&gt;\n        ...\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </li> <li> <p>Now, working within the <code>body</code> tags, add a little markup to show a list of checkboxes reflecting some ingredients:</p> <pre><code>&lt;h1&gt;Check your refrigerator. What can you create?&lt;/h1&gt;\n        &lt;div id=\"wrapper\"&gt;\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"4\" class=\"checkbox\"&gt;\n                &lt;label&gt;apple&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"247\" class=\"checkbox\"&gt;\n                &lt;label&gt;pear&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"77\" class=\"checkbox\"&gt;\n                &lt;label&gt;cherry&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"126\" class=\"checkbox\"&gt;\n                &lt;label&gt;fenugreek&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"302\" class=\"checkbox\"&gt;\n                &lt;label&gt;sake&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"327\" class=\"checkbox\"&gt;\n                &lt;label&gt;soy sauce&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"112\" class=\"checkbox\"&gt;\n                &lt;label&gt;cumin&lt;/label&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div style=\"padding-top:10px\"&gt;\n            &lt;button onClick=\"startInference()\"&gt;What kind of cuisine can you make?&lt;/button&gt;\n        &lt;/div&gt; \n</code></pre> <p>Notice that each checkbox is given a value.  This reflects the index where the ingredient is found according to the dataset. Apple, for example, in this alphabetic list, occupies the fifth column, so its value is '4' since we start counting at 0. You can consult the ingredients spreadsheet to discover a given ingredient's index.</p> <p>Continuing your work in the index.html file, add a script block where the model is called after the final closing <code>&lt;/div&gt;</code>. </p> </li> <li> <p>First, import the Onnx Runtime:</p> <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js\"&gt;&lt;/script&gt; \n</code></pre> <p>Onnx Runtime is used to enable running your Onnx models across a wide range of hardware platforms, including optimizations and an API to use.</p> </li> <li> <p>Once the Runtime is in place, you can call it:</p> <pre><code>&lt;script&gt;\n    const ingredients = Array(380).fill(0);\n\n    const checks = [...document.querySelectorAll('.checkbox')];\n\n    checks.forEach(check =&gt; {\n        check.addEventListener('change', function() {\n            // toggle the state of the ingredient\n            // based on the checkbox's value (1 or 0)\n            ingredients[check.value] = check.checked ? 1 : 0;\n        });\n    });\n\n    function testCheckboxes() {\n        // validate if at least one checkbox is checked\n        return checks.some(check =&gt; check.checked);\n    }\n\n    async function startInference() {\n\n        let atLeastOneChecked = testCheckboxes()\n\n        if (!atLeastOneChecked) {\n            alert('Please select at least one ingredient.');\n            return;\n        }\n        try {\n            // create a new session and load the model.\n\n            const session = await ort.InferenceSession.create('./model.onnx');\n\n            const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);\n            const feeds = { float_input: input };\n\n            // feed inputs and run\n            const results = await session.run(feeds);\n\n            // read from results\n            alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')\n\n        } catch (e) {\n            console.log(`failed to inference ONNX model`);\n            console.error(e);\n        }\n    }\n\n&lt;/script&gt;\n</code></pre> </li> </ol> <p>In this code, there are several things happening:</p> <ol> <li>You created an array of 380 possible values (1 or 0) to be set and sent to the model for inference, depending on whether an ingredient checkbox is checked.</li> <li>You created an array of checkboxes and a way to determine whether they were checked in an <code>init</code> function that is called when the application starts. When a checkbox is checked, the <code>ingredients</code> array is altered to reflect the chosen ingredient.</li> <li>You created a <code>testCheckboxes</code> function that checks whether any checkbox was checked.</li> <li>You use <code>startInference</code> function when the button is pressed and, if any checkbox is checked, you start inference.</li> <li>The inference routine includes:</li> <li>Setting up an asynchronous load of the model</li> <li>Creating a Tensor structure to send to the model</li> <li>Creating 'feeds' that reflects the <code>float_input</code> input that you created when training your model (you can use Netron to verify that name)</li> <li>Sending these 'feeds' to the model and waiting for a response</li> </ol>"},{"location":"4-Classification/4-Applied/#test-your-application","title":"Test your application","text":"<p>Open a terminal session in Visual Studio Code in the folder where your index.html file resides. Ensure that you have http-server installed globally, and type <code>http-server</code> at the prompt. A localhost should open and you can view your web app. Check what cuisine is recommended based on various ingredients:</p> <p></p> <p>Congratulations, you have created a 'recommendation' web app  with a few fields. Take some time to build out this system!</p>"},{"location":"4-Classification/4-Applied/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Your web app is very minimal, so continue to build it out using ingredients and their indexes from the ingredient_indexes data. What flavor combinations work to create a given national dish?</p>"},{"location":"4-Classification/4-Applied/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"4-Classification/4-Applied/#review-self-study","title":"Review &amp; Self Study","text":"<p>While this lesson just touched on the utility of creating a recommendation system for food ingredients, this area of ML applications is very rich in examples. Read some more about how these systems are built:</p> <ul> <li>https://www.sciencedirect.com/topics/computer-science/recommendation-engine</li> <li>https://www.technologyreview.com/2014/08/25/171547/the-ultimate-challenge-for-recommendation-engines/</li> <li>https://www.technologyreview.com/2015/03/23/168831/everything-is-a-recommendation/</li> </ul>"},{"location":"4-Classification/4-Applied/#assignment","title":"Assignment","text":"<p>Build a new recommender</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/","title":"\u6784\u5efa\u4e00\u4e2a\u7f8e\u98df\u63a8\u8350 Web \u5e94\u7528\u7a0b\u5e8f","text":"<p>\u5728\u672c\u8282\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4f7f\u7528\u60a8\u5728\u4e4b\u524d\u8bfe\u7a0b\u4e2d\u5b66\u4e60\u5230\u7684\u4e00\u4e9b\u65b9\u6cd5\u548c\u672c\u7cfb\u5217\u8bfe\u7a0b\u7528\u5230\u7684\u7f8e\u98df\u6570\u636e\u96c6\u6765\u6784\u5efa\u4e00\u4e2a\u5206\u7c7b\u6a21\u578b\u3002\u6b64\u5916\uff0c\u60a8\u8fd8\u4f1a\u4f7f\u7528 Onnx Web \u8fd0\u884c\u65f6\u6784\u5efa\u4e00\u4e2a\u5c0f\u7684 Web \u5e94\u7528\u7a0b\u5e8f\u53bb\u4f7f\u7528\u4fdd\u5b58\u7684\u6a21\u578b\u3002 \u673a\u5668\u5b66\u4e60\u6700\u6709\u7528\u7684\u5b9e\u9645\u8fd0\u7528\u4e4b\u4e00\u5c31\u662f\u6784\u5efa\u63a8\u8350\u7cfb\u7edf\uff0c\u4eca\u5929\u60a8\u53ef\u4ee5\u671d\u8fd9\u4e2a\u65b9\u5411\u8fc8\u51fa\u7b2c\u4e00\u6b65\u4e86\uff01</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u9762\u7684\u56fe\u7247\u67e5\u770b\u89c6\u9891</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":"<p>\u672c\u8282\u8bfe\u7a0b\u4e2d\u60a8\u5c06\u4f1a\u5b66\u4e60\uff1a</p> <ul> <li>\u5982\u4f55\u6784\u5efa\u6a21\u578b\u5e76\u5c06\u5b83\u4fdd\u5b58\u4e3a Onnx \u6a21\u578b</li> <li>\u5982\u4f55\u4f7f\u7528 Netron \u53bb\u68c0\u67e5\u6a21\u578b</li> <li>\u5982\u4f55\u5728 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u60a8\u7684\u6a21\u578b\u53bb\u8fdb\u884c\u63a8\u7406</li> </ul>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_2","title":"\u6784\u5efa\u60a8\u7684\u6a21\u578b","text":"<p>\u5efa\u7acb\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u662f\u8ba9\u8fd9\u4e9b\u6280\u672f\u8d4b\u80fd\u60a8\u7684\u4e1a\u52a1\u7cfb\u7edf\u7684\u4e00\u4e2a\u91cd\u8981\u90e8\u5206\u3002\u901a\u8fc7\u4f7f\u7528 Onnx \u60a8\u53ef\u4ee5\u5728 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u6a21\u578b\uff08\u5982\u679c\u9700\u8981\u53ef\u4ee5\u79bb\u7ebf\u4f7f\u7528\u5b83\u4eec\uff09\u3002</p> <p>\u4e4b\u524d\u7684\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u6784\u5efa\u5e76 \u201cpickled\u201d \u4e86\u4e00\u4e2a UFO \u76ee\u51fb\u4e8b\u4ef6\u7684\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5728\u4e00\u4e2a Flask \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u3002\u867d\u7136\u4e86\u89e3\u5b83\u7684\u67b6\u6784\u4f1a\u5f88\u6709\u7528\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u5168\u6808 Python \u5e94\u7528\u7a0b\u5e8f\uff0c\u60a8\u7684\u9700\u6c42\u53ef\u80fd\u5305\u62ec\u4f7f\u7528 JavaScript \u5e94\u7528\u7a0b\u5e8f\u3002</p> <p>\u5728\u672c\u8282\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e JavaScript \u7684\u57fa\u7840\u7cfb\u7edf\u8fdb\u884c\u63a8\u7406\u3002\u4e0d\u8fc7\u65e0\u8bba\u5982\u4f55\uff0c\u9996\u5148\u60a8\u9700\u8981\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u5e76\u5c06\u5176\u8f6c\u6362\u7ed9 Onnx \u4f7f\u7528\u3002</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#-","title":"\u7ec3\u4e60 - \u8bad\u7ec3\u5206\u7c7b\u6a21\u578b","text":"<p>\u9996\u5148\uff0c\u4f7f\u7528\u4e4b\u524d\u6211\u4eec\u4f7f\u7528\u7684\u6e05\u6d17\u540e\u7684\u83dc\u54c1\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u6a21\u578b\u3002</p> <ol> <li> <p>\u4ece\u5bfc\u5165\u5e93\u5f00\u59cb\uff1a</p> <pre><code>!pip install skl2onnx\nimport pandas as pd \n</code></pre> <p>\u60a8\u9700\u8981 skl2onnx \u6765\u5e2e\u52a9\u60a8\u5c06 Scikit-learn \u6a21\u578b\u8f6c\u6362\u4e3a Onnx \u683c\u5f0f\u3002</p> </li> <li> <p>\u7136\u540e\u4f7f\u7528 <code>read_csv()</code> \u8bfb\u53d6\u4e00\u4e2a CSV \u6587\u4ef6\uff0c\u6309\u7167\u60a8\u5728\u4e4b\u524d\u8bfe\u7a0b\u4e2d\u7528\u7684\u76f8\u540c\u65b9\u5f0f\u5904\u7406\u60a8\u7684\u6570\u636e\uff1a</p> <pre><code>data = pd.read_csv('../data/cleaned_cuisines.csv')\ndata.head()\n</code></pre> </li> <li> <p>\u5220\u9664\u524d\u4e24\u5217\u65e0\u7528\u7684\u5217\uff0c\u5c06\u5176\u4f59\u7684\u6570\u636e\u4fdd\u5b58\u4e3a X\uff1a</p> <pre><code>X = data.iloc[:,2:]\nX.head()\n</code></pre> </li> <li> <p>\u4fdd\u5b58\u6807\u7b7e\u4e3a y\uff1a</p> <pre><code>y = data[['cuisine']]\ny.head()\n</code></pre> </li> </ol>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_3","title":"\u5f00\u59cb\u8bad\u7ec3","text":"<p>\u6211\u4eec\u5c06\u4f7f\u7528\u6709\u7740\u4e0d\u9519\u7cbe\u5ea6\u7684 SVC \u5e93\u3002</p> <ol> <li> <p>\u4ece Scikit-learn \u5bfc\u5165\u4e00\u4e9b\u5408\u9002\u7684\u5e93\uff1a</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report\n</code></pre> </li> <li> <p>\u62c6\u5206\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\uff1a</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n</code></pre> </li> <li> <p>\u50cf\u60a8\u5728\u4e4b\u524d\u8bfe\u7a0b\u4e2d\u6240\u505a\u7684\u4e00\u6837\uff0c\u6784\u5efa\u4e00\u4e2a SVC \u5206\u7c7b\u5668\u6a21\u578b\uff1a</p> <pre><code>model = SVC(kernel='linear', C=10, probability=True,random_state=0)\nmodel.fit(X_train,y_train.values.ravel())\n</code></pre> </li> <li> <p>\u73b0\u5728\uff0c\u8c03\u7528 <code>predict()</code> \u6d4b\u8bd5\u60a8\u7684\u6a21\u578b\uff1a</p> <pre><code>y_pred = model.predict(X_test)\n</code></pre> </li> <li> <p>\u6253\u5370\u5206\u7c7b\u62a5\u544a\u6765\u68c0\u67e5\u6a21\u578b\u8d28\u91cf\uff1a</p> <pre><code>print(classification_report(y_test,y_pred))\n</code></pre> <p>\u5982\u6211\u4eec\u4e4b\u524d\u770b\u89c1\u7684\uff0c\u7cbe\u5ea6\u633a\u597d\uff1a</p> <pre><code>                precision    recall  f1-score   support\n\n     chinese       0.72      0.69      0.70       257\n      indian       0.91      0.87      0.89       243\n    japanese       0.79      0.77      0.78       239\n      korean       0.83      0.79      0.81       236\n        thai       0.72      0.84      0.78       224\n\n    accuracy                           0.79      1199\n   macro avg       0.79      0.79      0.79      1199\nweighted avg       0.79      0.79      0.79      1199\n</code></pre> </li> </ol>"},{"location":"4-Classification/4-Applied/README.zh-CN/#onnx","title":"\u5c06\u60a8\u7684\u6a21\u578b\u8f6c\u6362\u5230 Onnx","text":"<p>\u8bf7\u786e\u4fdd\u4f7f\u7528\u6b63\u786e\u7684\u5f20\u91cf\u8fdb\u884c\u8f6c\u6362\u3002\u6570\u636e\u96c6\u5217\u51fa\u4e86 380 \u79cd\u539f\u6599\uff0c\u56e0\u6b64\u60a8\u9700\u8981\u5728 <code>FloatTensorType</code> \u4e2d\u6807\u8bb0\u8fd9\u4e2a\u6570\u5b57\uff1a</p> <ol> <li> <p>\u8bbe\u7f6e\u5f20\u91cf\u6570\u4e3a 380 \u6765\u8fdb\u884c\u8f6c\u6362\u3002</p> <pre><code>from skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\n\ninitial_type = [('float_input', FloatTensorType([None, 380]))]\noptions = {id(model): {'nocl': True, 'zipmap': False}}\n</code></pre> </li> <li> <p>\u521b\u5efa onx \u5e76\u4fdd\u5b58\u4e3a\u6587\u4ef6 model.onnx\uff1a</p> <pre><code>onx = convert_sklearn(model, initial_types=initial_type, options=options)\nwith open(\"./model.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n</code></pre> <p>\u6ce8\u610f\uff0c\u60a8\u53ef\u4ee5\u7ed9\u60a8\u7684\u8f6c\u6362\u811a\u672c\u4f20\u5165\u9009\u9879\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u4f20\u5165\u8bbe\u4e3a True \u7684 <code>nocl</code> \u53c2\u6570\u548c\u8bbe\u4e3a False \u7684 <code>zipmap</code> \u53c2\u6570\u3002\u7531\u4e8e\u8fd9\u662f\u4e00\u4e2a\u5206\u7c7b\u6a21\u578b\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9\u5220\u9664\u4ea7\u751f\u5b57\u5178\u5217\u8868\u7684 ZipMap\uff08\u4e0d\u5fc5\u8981\uff09\u3002<code>nocl</code> \u6307\u6a21\u578b\u4e2d\u5305\u542b\u7684\u7c7b\u522b\u4fe1\u606f\u3002\u901a\u8fc7\u5c06 <code>nocl</code> \u8bbe\u7f6e\u4e3a True \u6765\u51cf\u5c0f\u6a21\u578b\u7684\u5927\u5c0f\u3002</p> </li> </ol> <p>\u8fd0\u884c\u5b8c\u6574\u7684 notebook \u5de5\u7a0b\u6587\u4ef6\u73b0\u5728\u5c06\u4f1a\u6784\u5efa\u4e00\u4e2a Onnx \u6a21\u578b\u5e76\u4fdd\u5b58\u5230\u6b64\u6587\u4ef6\u5939\u4e2d\u3002</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_4","title":"\u67e5\u770b\u60a8\u7684\u6a21\u578b","text":"<p>\u5728 Visual Studio Code \u4e2d\uff0cOnnx \u6a21\u578b\u7684\u7ed3\u6784\u4e0d\u662f\u5f88\u6e05\u6670\u3002\u4f46\u6709\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u514d\u8d39\u8f6f\u4ef6\uff0c\u5f88\u591a\u7814\u7a76\u5458\u7528\u5b83\u505a\u6a21\u578b\u53ef\u89c6\u5316\u4ee5\u4fdd\u8bc1\u6a21\u578b\u88ab\u6b63\u786e\u6784\u5efa\u3002\u4e0b\u8f7d Netron \u7136\u540e\u6253\u5f00\u60a8\u7684 model.onnx \u6587\u4ef6\u3002\u60a8\u80fd\u770b\u5230\u60a8\u7684\u7b80\u5355\u6a21\u578b\u88ab\u53ef\u89c6\u5316\u4e86\uff0c\u5176\u4e2d\u5217\u4e3e\u6709\u4f20\u5165\u7684\u53c2\u6570 <code>380</code> \u548c\u5206\u7c7b\u5668\uff1a</p> <p></p> <p>Netron \u662f\u67e5\u770b\u60a8\u6a21\u578b\u7684\u6709\u7528\u5de5\u5177\u3002</p> <p>\u73b0\u5728\u60a8\u51c6\u5907\u597d\u4e86\u5728 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u7b80\u6d01\u7684\u6a21\u578b\u3002\u6211\u4eec\u6765\u6784\u5efa\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\uff0c\u5f53\u60a8\u67e5\u770b\u51b0\u7bb1\u65f6\u5b83\u4f1a\u6d3e\u4e0a\u7528\u573a\uff0c\u5e76\u8bd5\u56fe\u627e\u51fa\u60a8\u53ef\u4ee5\u4f7f\u7528\u54ea\u79cd\u5269\u4f59\u98df\u6750\u7ec4\u5408\u6765\u70f9\u996a\u7ed9\u5b9a\u7684\u83dc\u80b4\uff0c\u8fd9\u7531\u60a8\u7684\u6a21\u578b\u51b3\u5b9a\u3002</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#web_1","title":"\u6784\u5efa\u4e00\u4e2a\u63a8\u8350\u5668 Web \u5e94\u7528\u7a0b\u5e8f","text":"<p>\u60a8\u53ef\u4ee5\u5728 Web \u5e94\u7528\u7a0b\u5e8f\u4e2d\u76f4\u63a5\u4f7f\u7528\u60a8\u7684\u6a21\u578b\u3002\u8fd9\u79cd\u67b6\u6784\u5141\u8bb8\u60a8\u5728\u672c\u5730\u8fd0\u884c\uff0c\u5982\u679c\u9700\u8981\u7684\u8bdd\u751a\u81f3\u53ef\u4ee5\u79bb\u7ebf\u8fd0\u884c\u3002\u6211\u4eec\u4ece\u5728\u60a8\u4fdd\u5b58 <code>model.onnx</code> \u6587\u4ef6\u7684\u76f8\u540c\u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a <code>index.html</code> \u6587\u4ef6\u5f00\u59cb\u3002</p> <ol> <li> <p>\u5728 index.html \u6587\u4ef6\u4e2d\uff0c\u6dfb\u52a0\u4ee5\u4e0b\u6807\u7b7e\uff1a</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;header&gt;\n        &lt;title&gt;Cuisine Matcher&lt;/title&gt;\n    &lt;/header&gt;\n    &lt;body&gt;\n        ...\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </li> <li> <p>\u73b0\u5728\u5728 <code>body</code> \u6807\u7b7e\u5185\uff0c\u6dfb\u52a0\u4e00\u4e9b\u6807\u7b7e\u6765\u5c55\u793a\u4ee3\u8868\u4e00\u4e9b\u914d\u6599\u7684 checkbox \u5217\u8868\uff1a</p> <pre><code>&lt;h1&gt;Check your refrigerator. What can you create?&lt;/h1&gt;\n        &lt;div id=\"wrapper\"&gt;\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"4\" class=\"checkbox\"&gt;\n                &lt;label&gt;apple&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"247\" class=\"checkbox\"&gt;\n                &lt;label&gt;pear&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"77\" class=\"checkbox\"&gt;\n                &lt;label&gt;cherry&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"126\" class=\"checkbox\"&gt;\n                &lt;label&gt;fenugreek&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"302\" class=\"checkbox\"&gt;\n                &lt;label&gt;sake&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"327\" class=\"checkbox\"&gt;\n                &lt;label&gt;soy sauce&lt;/label&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"boxCont\"&gt;\n                &lt;input type=\"checkbox\" value=\"112\" class=\"checkbox\"&gt;\n                &lt;label&gt;cumin&lt;/label&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div style=\"padding-top:10px\"&gt;\n            &lt;button onClick=\"startInference()\"&gt;What kind of cuisine can you make?&lt;/button&gt;\n        &lt;/div&gt; \n</code></pre> <p>\u6ce8\u610f\uff0c\u6bcf\u4e2a checkbox \u90fd\u7ed9\u5b9a\u4e86\u4e00\u4e2a\u503c\uff0c\u5b83\u53cd\u6620\u4e86\u6839\u636e\u6570\u636e\u96c6\u53ef\u4ee5\u627e\u5230\u5bf9\u5e94\u914d\u6599\u7684\u7d22\u5f15\u3002\u4f8b\u5982\uff0c\u5728\u8fd9\u4e2a\u6309\u5b57\u6bcd\u987a\u5e8f\u6392\u5217\u7684\u5217\u8868\u4e2d\uff0cApple \u5728\u7b2c\u4e94\u5217\uff0c\u7531\u4e8e\u6211\u4eec\u4ece 0 \u5f00\u59cb\u8ba1\u6570\uff0c\u56e0\u6b64\u5b83\u7684\u503c\u662f 4 \u3002\u60a8\u53ef\u4ee5\u67e5\u9605\u914d\u6599\u8868\u683c\u6765\u67e5\u627e\u7ed9\u5b9a\u914d\u6599\u7684\u7d22\u5f15\u3002</p> <p>\u7ee7\u7eed\u60a8\u5728 index.htlm \u6587\u4ef6\u4e2d\u7684\u5de5\u4f5c\uff0c\u5728\u6700\u540e\u4e00\u4e2a\u95ed\u5408\u7684 <code>&lt;/div&gt;</code> \u540e\u6dfb\u52a0\u4e00\u4e2a\u811a\u672c\u4ee3\u7801\u5757\u53bb\u8c03\u7528\u6a21\u578b\u3002</p> </li> <li> <p>\u9996\u5148\uff0c\u5bfc\u5165 Onnx Runtime\uff1a</p> <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.09/dist/ort.min.js\"&gt;&lt;/script&gt; \n</code></pre> <p>Onnx Runtime \u7528\u4e8e\u5728\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\u8fd0\u884c Onnx \u6a21\u578b\uff0c\u5305\u62ec\u4f18\u5316\u548c\u4f7f\u7528\u7684 API\u3002</p> </li> <li> <p>\u4e00\u65e6 Runtime \u5c31\u4f4d\uff0c\u60a8\u53ef\u4ee5\u8c03\u7528\u5b83\uff1a</p> <pre><code>&lt;script&gt;\n    const ingredients = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n    const checks = [].slice.call(document.querySelectorAll('.checkbox'));\n\n    // \u4f7f\u7528 async context \u53bb\u8c03\u7528 onnxruntime \u65b9\u6cd5\n    function init() {\n\n        checks.forEach(function (checkbox, index) {\n            checkbox.onchange = function () {\n                if (this.checked) {\n                    var index = checkbox.value;\n\n                    if (index !== -1) {\n                        ingredients[index] = 1;\n                    }\n                    console.log(ingredients)\n                }\n                else {\n                    var index = checkbox.value;\n\n                    if (index !== -1) {\n                        ingredients[index] = 0;\n                    }\n                    console.log(ingredients)\n                }\n            }\n        })\n    }\n\n    function testCheckboxes() {\n            for (var i = 0; i &lt; checks.length; i++)\n                if (checks[i].type == \"checkbox\")\n                    if (checks[i].checked)\n                        return true;\n            return false;\n    }\n\n    async function startInference() {\n\n        let checked = testCheckboxes()\n\n        if (checked) {\n            try {\n                // \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 session \u5e76\u52a0\u8f7d\u6a21\u578b\u3002           \n                const session = await ort.InferenceSession.create('./model.onnx');\n\n                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);\n                const feeds = { float_input: input };\n\n                // \u4f20\u5165 feeds \u5e76\u8fd0\u884c\u3002\n\n                const results = await session.run(feeds);\n\n                // \u8bfb\u53d6\u7ed3\u679c\u3002\n                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')\n\n            } catch (e) {\n                console.log(`failed to inference ONNX model: ${e}.`);\n            }\n        }\n        else alert(\"Please check an ingredient\")\n    }\n    init();\n\n&lt;/script&gt;\n</code></pre> </li> </ol> <p>\u5728\u8fd9\u6bb5\u4ee3\u7801\u4e2d\uff0c\u53d1\u751f\u4e86\u8fd9\u4e9b\u4e8b\u60c5\uff1a</p> <ol> <li>\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a\u7531 380 \u4e2a\u53ef\u80fd\u503c\uff08 1 \u6216 0 \uff09\u7ec4\u6210\u7684\u6570\u7ec4\uff0c\u8fd9\u4e9b\u503c\u88ab\u53d1\u9001\u7ed9\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u662f\u5426\u9009\u4e2d\u4e86\u914d\u6599\u7684 checkbox\u3002</li> <li>\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a checkbox \u6570\u7ec4\u5e76\u5728\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u65f6\u8c03\u7528\u7684 <code>init</code> \u65b9\u6cd5\u4e2d\u786e\u5b9a\u5b83\u4eec\u662f\u5426\u88ab\u9009\u4e2d\u3002\u9009\u4e2d checkbox \u540e\uff0c<code>ingredients</code> \u6570\u7ec4\u4f1a\u53d1\u751f\u53d8\u5316\u6765\u53cd\u6620\u6240\u9009\u7684\u914d\u6599\u3002</li> <li>\u60a8\u521b\u5efa\u4e86\u4e00\u4e2a <code>testCheckboxes</code> \u65b9\u6cd5\u68c0\u67e5\u662f\u5426\u6709 checkbox \u88ab\u9009\u4e2d\u3002</li> <li>\u5f53\u6309\u94ae\u88ab\u70b9\u51fb\u5c06\u4f1a\u4f7f\u7528 <code>startInference</code> \u65b9\u6cd5\uff0c\u5982\u679c\u6709\u4efb\u4e00 checkbox \u88ab\u9009\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f1a\u5f00\u59cb\u63a8\u7406\u3002</li> <li>\u63a8\u7406\u8fc7\u7a0b\u5305\u62ec\uff1a</li> <li>\u8bbe\u7f6e\u5f02\u6b65\u52a0\u8f7d\u6a21\u578b</li> <li>\u521b\u5efa\u4e86\u4e00\u4e2a\u8981\u4f20\u5165\u6a21\u578b\u7684\u5f20\u91cf\u7ed3\u6784</li> <li>\u521b\u5efa <code>feeds</code>\uff0c\u5b83\u53cd\u6620\u4e86\u60a8\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u521b\u5efa\u7684 <code>float_input</code> \u8f93\u5165\uff08\u60a8\u53ef\u4ee5\u4f7f\u7528 Netron \u6765\u9a8c\u8bc1\u5177\u4f53\u540d\u79f0\uff09</li> <li>\u53d1\u9001 <code>feeds</code> \u5230\u6a21\u578b\u5e76\u7b49\u5f85\u8fd4\u56de\u7ed3\u679c</li> </ol>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_5","title":"\u6d4b\u8bd5\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u5728 Visual Studio Code \u4e2d\uff0c\u4ece\u60a8\u7684 index.html \u6587\u4ef6\u6240\u5728\u7684\u6587\u4ef6\u5939\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\u3002\u8bf7\u786e\u4fdd\u60a8\u5df2\u7ecf\u5168\u5c40\u5b89\u88c5\u4e86 http-server\uff0c\u6309\u63d0\u793a\u8f93\u5165 <code>http-server</code>\uff0c\u4e00\u4e2a localhost \u9875\u9762\u5c06\u4f1a\u6253\u5f00\uff0c\u60a8\u53ef\u4ee5\u6d4f\u89c8\u60a8\u7684 Web \u5e94\u7528\u7a0b\u5e8f\u3002\u68c0\u67e5\u4e00\u4e0b\u7ed9\u51fa\u4e0d\u540c\u7684\u914d\u6599\u4f1a\u6709\u4ec0\u4e48\u83dc\u54c1\u4f1a\u88ab\u63a8\u8350\uff1a</p> <p></p> <p>\u795d\u8d3a\u60a8\u5df2\u7ecf\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u51e0\u4e2a\u5b57\u6bb5\u7684 \u201c\u63a8\u8350\u201d Web \u5e94\u7528\u7a0b\u5e8f\u3002\u82b1\u70b9\u65f6\u95f4\u6765\u6784\u5efa\u8fd9\u4e2a\u7cfb\u7edf\u5427\uff01</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_6","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u60a8\u7684 Web \u5e94\u7528\u7a0b\u5e8f\u8fd8\u5f88\u5c0f\u5de7\uff0c\u6240\u4ee5\u7ee7\u7eed\u4f7f\u7528\u914d\u6599\u7d22\u5f15\u4e2d\u7684\u914d\u6599\u6570\u636e\u548c\u7d22\u5f15\u6570\u636e\u6765\u6784\u5efa\u5b83\u5427\u3002\u7528\u4ec0\u4e48\u6837\u7684\u53e3\u5473\u7ec4\u5408\u624d\u80fd\u521b\u9020\u51fa\u4e00\u9053\u7279\u5b9a\u7684\u6c11\u65cf\u83dc\u80b4\uff1f</p>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_7","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"4-Classification/4-Applied/README.zh-CN/#_8","title":"\u56de\u987e\u4e0e\u81ea\u5b66","text":"<p>\u867d\u7136\u672c\u8bfe\u7a0b\u53ea\u8ba8\u8bba\u4e86\u521b\u5efa\u4e00\u4e2a\u83dc\u54c1\u914d\u6599\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u7a0b\u5e8f\u9886\u57df\u6709\u975e\u5e38\u4e30\u5bcc\u7684\u793a\u4f8b\u3002\u9605\u8bfb\u4e86\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u662f\u5982\u4f55\u6784\u5efa\u7684\uff1a</p> <ul> <li>https://www.sciencedirect.com/topics/computer-science/recommendation-engine</li> <li>https://www.technologyreview.com/2014/08/25/171547/the-ultimate-challenge-for-recommendation-engines/</li> <li>https://www.technologyreview.com/2015/03/23/168831/everything-is-a-recommendation/</li> </ul>"},{"location":"4-Classification/4-Applied/README.zh-CN/#_9","title":"\u4f5c\u4e1a","text":"<p>\u6784\u5efa\u4e00\u4e2a\u63a8\u8350\u5668</p>"},{"location":"4-Classification/4-Applied/assignment/","title":"Build a recommender","text":""},{"location":"4-Classification/4-Applied/assignment/#instructions","title":"Instructions","text":"<p>Given your exercises in this lesson, you now know how to build JavaScript-based web app using Onnx Runtime and a converted Onnx model. Experiment with building a new recommender using data from these lessons or sourced elsewhere (give credit, please). You might create a pet recommender given various personality attributes, or a music genre recommender based on a person's mood. Be creative!</p>"},{"location":"4-Classification/4-Applied/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A web app and notebook are presented, both well documented and running One of those two is missing or flawed Both are either missing or flawed"},{"location":"4-Classification/4-Applied/assignment.zh-CN/","title":"\u6784\u5efa\u4e00\u4e2a\u63a8\u8350\u5668","text":""},{"location":"4-Classification/4-Applied/assignment.zh-CN/#_2","title":"\u8bf4\u660e","text":"<p>\u6839\u636e\u672c\u8bfe\u7a0b\u4e2d\u7684\u7ec3\u4e60\uff0c\u60a8\u73b0\u5728\u4e86\u89e3\u4e86\u5982\u4f55\u4f7f\u7528 Onnx Runtime \u548c\u8f6c\u6362\u540e\u7684 Onnx \u6a21\u578b\u6784\u5efa\u57fa\u4e8e JavaScript \u7684 Web \u5e94\u7528\u7a0b\u5e8f\u3002\u5c1d\u8bd5\u4f7f\u7528\u8bfe\u7a0b\u4e2d\u7684\u6570\u636e\u6216\u5176\u4ed6\u6765\u6e90\uff08\u8bf7\u6dfb\u52a0\u81f4\u8c22\uff09\u7684\u6570\u636e\u6765\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u63a8\u8350\u5668\u3002\u60a8\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u7684\u4e2a\u6027\u5c5e\u6027\u6784\u5efa\u4e00\u4e2a\u5ba0\u7269\u63a8\u8350\u5668\uff0c\u6216\u8005\u6839\u636e\u4eba\u7684\u60c5\u7eea\u521b\u5efa\u97f3\u4e50\u6d41\u6d3e\u63a8\u8350\u5668\u3002\u8981\u6709\u521b\u610f\uff01</p>"},{"location":"4-Classification/4-Applied/assignment.zh-CN/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u63d0\u4ea4\u4e86\u4e00\u4e2a Web \u5e94\u7528\u7a0b\u5e8f\u548c notebook \u5de5\u7a0b\u6587\u4ef6\uff0c\u4e24\u8005\u7684\u6ce8\u91ca\u8bf4\u660e\u5145\u5206\u5e76\u4e14\u80fd\u6b63\u5e38\u8fd0\u884c \u5176\u4e2d\u4e00\u4e2a\u6ca1\u63d0\u4ea4\u6216\u8005\u6709\u7f3a\u9677 \u4e24\u4e2a\u90fd\u6ca1\u63d0\u4ea4\u6216\u8005\u90fd\u6709\u7f3a\u9677"},{"location":"5-Clustering/","title":"Clustering models for machine learning","text":"<p>Clustering is a machine learning task where it looks to find objects that resemble one another and group these into groups called clusters.  What differs clustering from other approaches in machine learning, is that things happen automatically, in fact, it's fair to say it's the opposite of supervised learning. </p>"},{"location":"5-Clustering/#regional-topic-clustering-models-for-a-nigerian-audiences-musical-taste","title":"Regional topic: clustering models for a Nigerian audience's musical taste \ud83c\udfa7","text":"<p>Nigeria's diverse audience has diverse musical tastes. Using data scraped from Spotify (inspired by this article, let's look at some music popular in Nigeria. This dataset includes data about various songs' 'danceability' score, 'acousticness', loudness, 'speechiness', popularity and energy. It will be interesting to discover patterns in this data!</p> <p></p> <p>Photo by Marcela Laskoski on Unsplash</p> <p>In this series of lessons, you will discover new ways to analyze data using clustering techniques. Clustering is particularly useful when your dataset lacks labels. If it does have labels, then classification techniques such as those you learned in previous lessons might be more useful. But in cases where you are looking to group unlabelled data, clustering is a great way to discover patterns.</p> <p>There are useful low-code tools that can help you learn about working with clustering models. Try Azure ML for this task</p>"},{"location":"5-Clustering/#lessons","title":"Lessons","text":"<ol> <li>Introduction to clustering</li> <li>K-Means clustering</li> </ol>"},{"location":"5-Clustering/#credits","title":"Credits","text":"<p>These lessons were written with \ud83c\udfb6 by Jen Looper with helpful reviews by Rishit Dagli and Muhammad Sakib Khan Inan.</p> <p>The Nigerian Songs dataset was sourced from Kaggle as scraped from Spotify.</p> <p>Useful K-Means examples that aided in creating this lesson include this iris exploration, this introductory notebook, and this hypothetical NGO example.</p>"},{"location":"5-Clustering/README.zh-cn/","title":"\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u805a\u7c7b\u6a21\u578b","text":"<p>\u805a\u7c7b\uff08clustering\uff09\u662f\u4e00\u9879\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u7528\u4e8e\u5bfb\u627e\u7c7b\u4f3c\u5bf9\u8c61\u5e76\u5c06\u4ed6\u4eec\u5206\u6210\u4e0d\u540c\u7684\u7ec4\uff08\u8fd9\u4e9b\u7ec4\u79f0\u505a\u201c\u805a\u7c7b\u201d\uff08cluster\uff09\uff09\u3002\u805a\u7c7b\u4e0e\u5176\u5b83\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u805a\u7c7b\u662f\u81ea\u52a8\u8fdb\u884c\u7684\u3002\u4e8b\u5b9e\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u8bf4\u5b83\u662f\u76d1\u7763\u5b66\u4e60\u7684\u5bf9\u7acb\u9762\u3002</p>"},{"location":"5-Clustering/README.zh-cn/#_2","title":"\u672c\u8282\u4e3b\u9898: \u5c3c\u65e5\u5229\u4e9a\u89c2\u4f17\u97f3\u4e50\u54c1\u5473\u7684\u805a\u7c7b\u6a21\u578b\ud83c\udfa7","text":"<p>\u5c3c\u65e5\u5229\u4e9a\u591a\u6837\u5316\u7684\u89c2\u4f17\u6709\u7740\u591a\u6837\u5316\u7684\u97f3\u4e50\u54c1\u5473\u3002\u4f7f\u7528\u4ece Spotify \u4e0a\u6293\u53d6\u7684\u6570\u636e\uff08\u53d7\u5230\u672c\u6587\u7684\u542f\u53d1\uff09\uff0c\u8ba9\u6211\u4eec\u770b\u770b\u5c3c\u65e5\u5229\u4e9a\u6d41\u884c\u7684\u4e00\u4e9b\u97f3\u4e50\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u5305\u62ec\u5173\u4e8e\u5404\u79cd\u6b4c\u66f2\u7684\u821e\u8e48\u6027\u3001\u58f0\u5b66\u3001\u54cd\u5ea6\u3001\u8a00\u8bed\u3001\u6d41\u884c\u5ea6\u548c\u6d3b\u529b\u7684\u5206\u6570\u3002\u4ece\u8fd9\u4e9b\u6570\u636e\u4e2d\u53d1\u73b0\u4e00\u4e9b\u6a21\u5f0f\uff08pattern\uff09\u4f1a\u662f\u5f88\u6709\u8da3\u7684\u4e8b\u60c5!</p> <p></p> <p>Marcela Laskoski \u5728 Unsplash \u4e0a\u7684\u7167\u7247</p> <p>\u5728\u672c\u7cfb\u5217\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u5c06\u53d1\u73b0\u4f7f\u7528\u805a\u7c7b\u6280\u672f\u5206\u6790\u6570\u636e\u7684\u65b0\u65b9\u6cd5\u3002\u5f53\u6570\u636e\u96c6\u7f3a\u5c11\u6807\u7b7e\u7684\u65f6\u5019\uff0c\u805a\u7c7b\u7279\u522b\u6709\u7528\u3002\u5982\u679c\u5b83\u6709\u6807\u7b7e\uff0c\u90a3\u4e48\u5206\u7c7b\u6280\u672f(\u6bd4\u5982\u60a8\u5728\u524d\u9762\u7684\u8bfe\u7a0b\u4e2d\u6240\u5b66\u7684\u90a3\u4e9b)\u53ef\u80fd\u4f1a\u66f4\u6709\u7528\u3002\u4f46\u662f\u5982\u679c\u8981\u5bf9\u672a\u6807\u8bb0\u7684\u6570\u636e\u8fdb\u884c\u5206\u7ec4\uff0c\u805a\u7c7b\u662f\u53d1\u73b0\u6a21\u5f0f\u7684\u597d\u65b9\u6cd5\u3002</p> <p>\u8fd9\u91cc\u6709\u4e00\u4e9b\u6709\u7528\u7684\u4f4e\u4ee3\u7801\u5de5\u5177\u53ef\u4ee5\u5e2e\u52a9\u60a8\u4e86\u89e3\u5982\u4f55\u4f7f\u7528\u805a\u7c7b\u6a21\u578b\u3002\u5c1d\u8bd5 Azure ML for this task</p>"},{"location":"5-Clustering/README.zh-cn/#_3","title":"\u8bfe\u7a0b\u5b89\u6392","text":"<ol> <li>\u4ecb\u7ecd\u805a\u7c7b</li> <li>K-Means \u805a\u7c7b</li> </ol>"},{"location":"5-Clustering/README.zh-cn/#_4","title":"\u81f4\u8c22","text":"<p>\u8fd9\u4e9b\u8bfe\u7a0b\u7531 Jen Looper \u5728 \ud83c\udfb6 \u4e0a\u64b0\u5199\uff0c\u5e76\u7531 Rishit Dagli \u548c Muhammad Sakib Khan Inan \u8fdb\u884c\u4e86\u6709\u5e2e\u52a9\u7684\u8bc4\u5ba1\u3002</p> <p>\u5c3c\u65e5\u5229\u4e9a\u6b4c\u66f2\u6570\u636e\u96c6 \u6765\u81ea Kaggle \u6293\u53d6\u7684 Spotify \u6570\u636e\u3002</p> <p>\u4e00\u4e9b\u5e2e\u52a9\u521b\u9020\u4e86\u8fd9\u8282\u8bfe\u7a0b\u7684 K-Means \u4f8b\u5b50\u5305\u62ec:\u8679\u819c\u63a2\u7d22(iris exploration)\uff0c\u4ecb\u7ecd\u6027\u7684\u7b14\u8bb0(introductory notebook)\uff0c\u548c \u5047\u8bbe\u975e\u653f\u5e9c\u7ec4\u7ec7\u7684\u4f8b\u5b50(hypothetical NGO example)\u3002</p>"},{"location":"5-Clustering/1-Visualize/","title":"Introduction to clustering","text":"<p>Clustering is a type of Unsupervised Learning that presumes that a dataset is unlabelled or that its inputs are not matched with predefined outputs. It uses various algorithms to sort through unlabeled data and provide groupings according to patterns it discerns in the data. </p> <p></p> <p>\ud83c\udfa5 Click the image above for a video. While you're studying machine learning with clustering, enjoy some Nigerian Dance Hall tracks - this is a highly rated song from 2014 by PSquare.</p>"},{"location":"5-Clustering/1-Visualize/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"5-Clustering/1-Visualize/#introduction","title":"Introduction","text":"<p>Clustering is very useful for data exploration. Let's see if it can help discover trends and patterns in the way Nigerian audiences consume music.</p> <p>\u2705 Take a minute to think about the uses of clustering. In real life, clustering happens whenever you have a pile of laundry and need to sort out your family members' clothes \ud83e\udde6\ud83d\udc55\ud83d\udc56\ud83e\ude72. In data science, clustering happens when trying to analyze a user's preferences, or determine the characteristics of any unlabeled dataset. Clustering, in a way, helps make sense of chaos, like a sock drawer.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: MIT's John Guttag introduces clustering</p> <p>In a professional setting, clustering can be used to determine things like market segmentation, determining what age groups buy what items, for example. Another use would be anomaly detection, perhaps to detect fraud from a dataset of credit card transactions. Or you might use clustering to determine tumors in a batch of medical scans. </p> <p>\u2705 Think a minute about how you might have encountered clustering 'in the wild', in a banking, e-commerce, or business setting.</p> <p>\ud83c\udf93 Interestingly, cluster analysis originated in the fields of Anthropology and Psychology in the 1930s. Can you imagine how it might have been used?</p> <p>Alternately, you could use it for grouping search results - by shopping links, images, or reviews, for example. Clustering is useful when you have a large dataset that you want to reduce and on which you want to perform more granular analysis, so the technique can be used to learn about data before other models are constructed.</p> <p>\u2705 Once your data is organized in clusters, you assign it a cluster Id, and this technique can be useful when preserving a dataset's privacy; you can instead refer to a data point by its cluster id, rather than by more revealing identifiable data. Can you think of other reasons why you'd refer to a cluster Id rather than other elements of the cluster to identify it?</p> <p>Deepen your understanding of clustering techniques in this Learn module</p>"},{"location":"5-Clustering/1-Visualize/#getting-started-with-clustering","title":"Getting started with clustering","text":"<p>Scikit-learn offers a large array of methods to perform clustering. The type you choose will depend on your use case. According to the documentation, each method has various benefits. Here is a simplified table of the methods supported by Scikit-learn and their appropriate use cases:</p> Method name Use case K-Means general purpose, inductive Affinity propagation many, uneven clusters, inductive Mean-shift many, uneven clusters, inductive Spectral clustering few, even clusters, transductive Ward hierarchical clustering many, constrained clusters, transductive Agglomerative clustering many, constrained, non Euclidean distances, transductive DBSCAN non-flat geometry, uneven clusters, transductive OPTICS non-flat geometry, uneven clusters with variable density, transductive Gaussian mixtures flat geometry, inductive BIRCH large dataset with outliers, inductive <p>\ud83c\udf93 How we create clusters has a lot to do with how we gather up the data points into groups. Let's unpack some vocabulary:</p> <p>\ud83c\udf93 'Transductive' vs. 'inductive'</p> <p>Transductive inference is derived from observed training cases that map to specific test cases. Inductive inference is derived from training cases that map to general rules which are only then applied to test cases. </p> <p>An example: Imagine you have a dataset that is only partially  labelled. Some things are 'records', some 'cds', and some are blank. Your job is to provide labels for the blanks. If you choose an inductive approach, you'd train a model looking for 'records' and 'cds', and apply those labels to your unlabeled data. This approach will have trouble classifying things that are actually 'cassettes'. A transductive approach, on the other hand, handles this unknown data more effectively as it works to group similar items together and then applies a label to a group. In this case, clusters might reflect 'round musical things' and 'square musical things'. </p> <p>\ud83c\udf93 'Non-flat' vs. 'flat' geometry</p> <p>Derived from mathematical terminology, non-flat vs. flat geometry refers to the measure of distances between points by either 'flat' (Euclidean) or 'non-flat' (non-Euclidean) geometrical methods. </p> <p>'Flat' in this context refers to Euclidean geometry (parts of which are taught as 'plane' geometry), and non-flat refers to non-Euclidean geometry. What does geometry have to do with machine learning? Well, as two fields that are rooted in mathematics, there must be a common way to measure distances between points in clusters, and that can be done in a 'flat' or 'non-flat' way, depending on the nature of the data. Euclidean distances are measured as the length of a line segment between two points. Non-Euclidean distances are measured along a curve. If your data, visualized, seems to not exist on a plane, you might need to use a specialized algorithm to handle it.</p> <p> Infographic by Dasani Madipalli</p> <p>\ud83c\udf93 'Distances'</p> <p>Clusters are defined by their distance matrix, e.g. the distances between points. This distance can be measured in a few ways. Euclidean clusters are defined by the average of the point values, and contain a 'centroid' or center point. Distances are thus measured by the distance to that centroid. Non-Euclidean distances refer to 'clustroids', the point closest to other points. Clustroids in turn can be defined in various ways.</p> <p>\ud83c\udf93 'Constrained'</p> <p>Constrained Clustering introduces 'semi-supervised' learning into this unsupervised method. The relationships between points are flagged as 'cannot link' or 'must-link' so some rules are forced on the dataset.</p> <p>An example: If an algorithm is set free on a batch of unlabelled or semi-labelled data, the clusters it produces may be of poor quality. In the example above, the clusters might group 'round music things' and 'square music things' and 'triangular things' and 'cookies'. If given some constraints, or rules to follow (\"the item must be made of plastic\", \"the item needs to be able to produce music\") this can help 'constrain' the algorithm to make better choices.</p> <p>\ud83c\udf93 'Density'</p> <p>Data that is 'noisy' is considered to be 'dense'. The distances between points in each of its clusters may prove, on examination, to be more or less dense, or 'crowded' and thus this data needs to be analyzed with the appropriate clustering method. This article demonstrates the difference between using K-Means clustering vs. HDBSCAN algorithms to explore a noisy dataset with uneven cluster density.</p>"},{"location":"5-Clustering/1-Visualize/#clustering-algorithms","title":"Clustering algorithms","text":"<p>There are over 100 clustering algorithms, and their use depends on the nature of the data at hand. Let's discuss some of the major ones:</p> <ul> <li>Hierarchical clustering. If an object is classified by its proximity to a nearby object, rather than to one farther away, clusters are formed based on their members' distance to and from other objects. Scikit-learn's agglomerative clustering is hierarchical.</li> </ul> <p></p> <p>Infographic by Dasani Madipalli</p> <ul> <li>Centroid clustering. This popular algorithm requires the choice of 'k', or the number of clusters to form, after which the algorithm determines the center point of a cluster and gathers data around that point. K-means clustering is a popular version of centroid clustering. The center is determined by the nearest mean, thus the name. The squared distance from the cluster is minimized.</li> </ul> <p></p> <p>Infographic by Dasani Madipalli</p> <ul> <li> <p>Distribution-based clustering. Based in statistical modeling, distribution-based clustering centers on determining the probability that a data point belongs to a cluster, and assigning it accordingly. Gaussian mixture methods belong to this type.</p> </li> <li> <p>Density-based clustering. Data points are assigned to clusters based on their density, or their grouping around each other. Data points far from the group are considered outliers or noise. DBSCAN, Mean-shift and OPTICS belong to this type of clustering.</p> </li> <li> <p>Grid-based clustering. For multi-dimensional datasets, a grid is created and the data is divided amongst the grid's cells, thereby creating clusters.</p> </li> </ul>"},{"location":"5-Clustering/1-Visualize/#exercise-cluster-your-data","title":"Exercise - cluster your data","text":"<p>Clustering as a technique is greatly aided by proper visualization, so let's get started by visualizing our music data. This exercise will help us decide which of the methods of clustering we should most effectively use for the nature of this data.</p> <ol> <li> <p>Open the notebook.ipynb file in this folder.</p> </li> <li> <p>Import the <code>Seaborn</code> package for good data visualization.</p> <pre><code>!pip install seaborn\n</code></pre> </li> <li> <p>Append the song data from nigerian-songs.csv. Load up a dataframe with some data about the songs. Get ready to explore this data by importing the libraries and dumping out the data:</p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/nigerian-songs.csv\")\ndf.head()\n</code></pre> <p>Check the first few lines of data:</p> name album artist artist_top_genre release_date length popularity danceability acousticness energy instrumentalness liveness loudness speechiness tempo time_signature 0 Sparky Mandy &amp; The Jungle Cruel Santino alternative r&amp;b 2019 144000 48 0.666 0.851 0.42 0.534 0.11 -6.699 0.0829 133.015 5 1 shuga rush EVERYTHING YOU HEARD IS TRUE Odunsi (The Engine) afropop 2020 89488 30 0.71 0.0822 0.683 0.000169 0.101 -5.64 0.36 129.993 3 2 LITT! LITT! AYL\u00d8 indie r&amp;b 2018 207758 40 0.836 0.272 0.564 0.000537 0.11 -7.127 0.0424 130.005 4 3 Confident / Feeling Cool Enjoy Your Life Lady Donli nigerian pop 2019 175135 14 0.894 0.798 0.611 0.000187 0.0964 -4.961 0.113 111.087 4 4 wanted you rare. Odunsi (The Engine) afropop 2018 152049 25 0.702 0.116 0.833 0.91 0.348 -6.044 0.0447 105.115 4 </li> <li> <p>Get some information about the dataframe, calling <code>info()</code>:</p> <pre><code>df.info()\n</code></pre> </li> </ol> <p>The output looking like so:</p> <pre><code>```output\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 530 entries, 0 to 529\nData columns (total 16 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   name              530 non-null    object \n 1   album             530 non-null    object \n 2   artist            530 non-null    object \n 3   artist_top_genre  530 non-null    object \n 4   release_date      530 non-null    int64  \n 5   length            530 non-null    int64  \n 6   popularity        530 non-null    int64  \n 7   danceability      530 non-null    float64\n 8   acousticness      530 non-null    float64\n 9   energy            530 non-null    float64\n 10  instrumentalness  530 non-null    float64\n 11  liveness          530 non-null    float64\n 12  loudness          530 non-null    float64\n 13  speechiness       530 non-null    float64\n 14  tempo             530 non-null    float64\n 15  time_signature    530 non-null    int64  \ndtypes: float64(8), int64(4), object(4)\nmemory usage: 66.4+ KB\n```\n</code></pre> <ol> <li> <p>Double-check for null values, by calling <code>isnull()</code> and verifying the sum being 0:</p> <pre><code>df.isnull().sum()\n</code></pre> <p>Looking good:</p> <pre><code>name                0\nalbum               0\nartist              0\nartist_top_genre    0\nrelease_date        0\nlength              0\npopularity          0\ndanceability        0\nacousticness        0\nenergy              0\ninstrumentalness    0\nliveness            0\nloudness            0\nspeechiness         0\ntempo               0\ntime_signature      0\ndtype: int64\n</code></pre> </li> <li> <p>Describe the data:</p> <pre><code>df.describe()\n</code></pre> release_date length popularity danceability acousticness energy instrumentalness liveness loudness speechiness tempo time_signature count 530 530 530 530 530 530 530 530 530 530 530 530 mean 2015.390566 222298.1698 17.507547 0.741619 0.265412 0.760623 0.016305 0.147308 -4.953011 0.130748 116.487864 3.986792 std 3.131688 39696.82226 18.992212 0.117522 0.208342 0.148533 0.090321 0.123588 2.464186 0.092939 23.518601 0.333701 min 1998 89488 0 0.255 0.000665 0.111 0 0.0283 -19.362 0.0278 61.695 3 25% 2014 199305 0 0.681 0.089525 0.669 0 0.07565 -6.29875 0.0591 102.96125 4 50% 2016 218509 13 0.761 0.2205 0.7845 0.000004 0.1035 -4.5585 0.09795 112.7145 4 75% 2017 242098.5 31 0.8295 0.403 0.87575 0.000234 0.164 -3.331 0.177 125.03925 4 max 2020 511738 73 0.966 0.954 0.995 0.91 0.811 0.582 0.514 206.007 5 </li> </ol> <p>\ud83e\udd14 If we are working with clustering, an unsupervised method that does not require labeled data, why are we showing this data with labels? In the data exploration phase, they come in handy, but they are not necessary for the clustering algorithms to work. You could just as well remove the column headers and refer to the data by column number. </p> <p>Look at the general values of the data. Note that popularity can be '0', which show songs that have no ranking. Let's remove those shortly.</p> <ol> <li> <p>Use a barplot to find out the most popular genres:</p> <pre><code>import seaborn as sns\n\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top[:5].index,y=top[:5].values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> <p></p> </li> </ol> <p>\u2705 If you'd like to see more top values, change the top <code>[:5]</code> to a bigger value, or remove it to see all.</p> <p>Note, when the top genre is described as 'Missing', that means that Spotify did not classify it, so let's get rid of it.</p> <ol> <li> <p>Get rid of missing data by filtering it out</p> <pre><code>df = df[df['artist_top_genre'] != 'Missing']\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top.index,y=top.values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> <p>Now recheck the genres:</p> <p></p> </li> <li> <p>By far, the top three genres dominate this dataset. Let's concentrate on <code>afro dancehall</code>, <code>afropop</code>, and <code>nigerian pop</code>, additionally filter the dataset to remove anything with a 0 popularity value (meaning it was not classified with a popularity in the dataset and can be considered noise for our purposes):</p> <pre><code>df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]\ndf = df[(df['popularity'] &gt; 0)]\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top.index,y=top.values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> </li> <li> <p>Do a quick test to see if the data correlates in any particularly strong way:</p> <pre><code>corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n</code></pre> <p></p> <p>The only strong correlation is between <code>energy</code> and <code>loudness</code>, which is not too surprising, given that loud music is usually pretty energetic. Otherwise, the correlations are relatively weak. It will be interesting to see what a clustering algorithm can make of this data.</p> <p>\ud83c\udf93 Note that correlation does not imply causation! We have proof of correlation but no proof of causation. An amusing web site has some visuals that emphasize this point.</p> </li> </ol> <p>Is there any convergence in this dataset around a song's perceived popularity and danceability? A FacetGrid shows that there are concentric circles that line up, regardless of genre. Could it be that Nigerian tastes converge at a certain level of danceability for this genre?  </p> <p>\u2705 Try different datapoints (energy, loudness, speechiness) and more or different musical genres. What can you discover? Take a look at the <code>df.describe()</code> table to see the general spread of the data points.</p>"},{"location":"5-Clustering/1-Visualize/#exercise-data-distribution","title":"Exercise - data distribution","text":"<p>Are these three genres significantly different in the perception of their danceability, based on their popularity?</p> <ol> <li> <p>Examine our top three genres data distribution for popularity and danceability along a given x and y axis.</p> <pre><code>sns.set_theme(style=\"ticks\")\n\ng = sns.jointplot(\n    data=df,\n    x=\"popularity\", y=\"danceability\", hue=\"artist_top_genre\",\n    kind=\"kde\",\n)\n</code></pre> <p>You can discover concentric circles around a general point of convergence, showing the distribution of points.</p> <p>\ud83c\udf93 Note that this example uses a KDE (Kernel Density Estimate) graph that represents the data using a continuous probability density curve. This allows us to interpret data when working with multiple distributions.</p> <p>In general, the three genres align loosely in terms of their popularity and danceability. Determining clusters in this loosely-aligned data will be a challenge:</p> <p></p> </li> <li> <p>Create a scatter plot:</p> <pre><code>sns.FacetGrid(df, hue=\"artist_top_genre\", size=5) \\\n   .map(plt.scatter, \"popularity\", \"danceability\") \\\n   .add_legend()\n</code></pre> <p>A scatterplot of the same axes shows a similar pattern of convergence</p> <p></p> </li> </ol> <p>In general, for clustering, you can use scatterplots to show clusters of data, so mastering this type of visualization is very useful. In the next lesson, we will take this filtered data and use k-means clustering to discover groups in this data that see to overlap in interesting ways.</p>"},{"location":"5-Clustering/1-Visualize/#challenge","title":"\ud83d\ude80Challenge","text":"<p>In preparation for the next lesson, make a chart about the various clustering algorithms you might discover and use in a production environment. What kinds of problems is the clustering trying to address?</p>"},{"location":"5-Clustering/1-Visualize/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"5-Clustering/1-Visualize/#review-self-study","title":"Review &amp; Self Study","text":"<p>Before you apply clustering algorithms, as we have learned, it's a good idea to understand the nature of your dataset. Read more on this topic here</p> <p>This helpful article walks you through the different ways that various clustering algorithms behave, given different data shapes.</p>"},{"location":"5-Clustering/1-Visualize/#assignment","title":"Assignment","text":"<p>Research other visualizations for clustering</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/","title":"\u4ecb\u7ecd\u805a\u7c7b","text":"<p>\u805a\u7c7b\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u5b83\u5047\u5b9a\u6570\u636e\u96c6\u672a\u6807\u8bb0\u6216\u5176\u8f93\u5165\u4e0e\u9884\u5b9a\u4e49\u7684\u8f93\u51fa\u4e0d\u5339\u914d\u3002\u5b83\u4f7f\u7528\u5404\u79cd\u7b97\u6cd5\u5bf9\u672a\u6807\u8bb0\u7684\u6570\u636e\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u6839\u636e\u5b83\u5728\u6570\u636e\u4e2d\u8bc6\u522b\u7684\u6a21\u5f0f\u63d0\u4f9b\u5206\u7ec4\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u9762\u7684\u56fe\u7247\u89c2\u770b\u89c6\u9891\u3002\u5f53\u60a8\u901a\u8fc7\u805a\u7c7b\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u65f6\uff0c\u8bf7\u6b23\u8d4f\u4e00\u4e9b\u5c3c\u65e5\u5229\u4e9a\u821e\u5385\u66f2\u76ee - \u8fd9\u662f 2014 \u5e74 PSquare \u4e0a\u9ad8\u5ea6\u8bc4\u4ef7\u7684\u6b4c\u66f2\u3002</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u805a\u7c7b\u5bf9\u4e8e\u6570\u636e\u63a2\u7d22\u975e\u5e38\u6709\u7528\u3002\u8ba9\u6211\u4eec\u770b\u770b\u5b83\u662f\u5426\u6709\u52a9\u4e8e\u53d1\u73b0\u5c3c\u65e5\u5229\u4e9a\u89c2\u4f17\u6d88\u8d39\u97f3\u4e50\u7684\u8d8b\u52bf\u548c\u6a21\u5f0f\u3002</p> <p>\u2705\u82b1\u4e00\u70b9\u65f6\u95f4\u601d\u8003\u805a\u7c7b\u7684\u7528\u9014\u3002\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\uff0c\u6bcf\u5f53\u4f60\u6709\u4e00\u5806\u8863\u670d\u9700\u8981\u6574\u7406\u5bb6\u4eba\u7684\u8863\u670d\u65f6\uff0c\u5c31\u4f1a\u53d1\u751f\u805a\u7c7b\ud83e\udde6\ud83d\udc55\ud83d\udc56\ud83e\ude72. \u5728\u6570\u636e\u79d1\u5b66\u4e2d\uff0c\u805a\u7c7b\u7528\u4e8e\u5728\u5c1d\u8bd5\u5206\u6790\u7528\u6237\u7684\u504f\u597d\u6216\u786e\u5b9a\u4efb\u4f55\u672a\u6807\u8bb0\u6570\u636e\u96c6\u7684\u7279\u5f81\u3002\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\uff0c\u805a\u7c7b\u6709\u52a9\u4e8e\u7406\u89e3\u6742\u4e71\u7684\u72b6\u6001\uff0c\u5c31\u50cf\u662f\u4e00\u4e2a\u889c\u5b50\u62bd\u5c49\u3002</p> <p></p> <p>\ud83c\udfa5\u5355\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1a\u9ebb\u7701\u7406\u5de5\u5b66\u9662\u7684 John Guttag \u4ecb\u7ecd\u805a\u7c7b</p> <p>\u5728\u4e13\u4e1a\u73af\u5883\u4e2d\uff0c\u805a\u7c7b\u53ef\u7528\u4e8e\u786e\u5b9a\u8bf8\u5982\u5e02\u573a\u7ec6\u5206\u4e4b\u7c7b\u7684\u4e8b\u60c5\uff0c\u4f8b\u5982\u786e\u5b9a\u54ea\u4e9b\u5e74\u9f84\u7ec4\u8d2d\u4e70\u54ea\u4e9b\u5546\u54c1\u3002\u53e6\u4e00\u4e2a\u7528\u9014\u662f\u5f02\u5e38\u68c0\u6d4b\uff0c\u53ef\u80fd\u662f\u4ece\u4fe1\u7528\u5361\u4ea4\u6613\u6570\u636e\u96c6\u4e2d\u68c0\u6d4b\u6b3a\u8bc8\u3002\u6216\u8005\u60a8\u53ef\u4ee5\u4f7f\u7528\u805a\u7c7b\u6765\u786e\u5b9a\u4e00\u6279\u533b\u5b66\u626b\u63cf\u4e2d\u7684\u80bf\u7624\u3002</p> <p>\u2705 \u60f3\u4e00\u60f3\u60a8\u662f\u5982\u4f55\u5728\u94f6\u884c\u3001\u7535\u5b50\u5546\u52a1\u6216\u5546\u4e1a\u73af\u5883\u4e2d\u201c\u610f\u5916\u201d\u9047\u5230\u805a\u7c7b\u7684\u3002</p> <p>\ud83c\udf93\u6709\u8da3\u7684\u662f\uff0c\u805a\u7c7b\u5206\u6790\u8d77\u6e90\u4e8e 1930 \u5e74\u4ee3\u7684\u4eba\u7c7b\u5b66\u548c\u5fc3\u7406\u5b66\u9886\u57df\u3002\u4f60\u80fd\u60f3\u8c61\u5b83\u662f\u5982\u4f55\u88ab\u4f7f\u7528\u7684\u5417\uff1f</p> <p>\u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u5bf9\u641c\u7d22\u7ed3\u679c\u8fdb\u884c\u5206\u7ec4 - \u4f8b\u5982\uff0c\u901a\u8fc7\u8d2d\u7269\u94fe\u63a5\u3001\u56fe\u7247\u6216\u8bc4\u8bba\u3002\u5f53\u60a8\u6709\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\u60f3\u8981\u51cf\u5c11\u5e76\u4e14\u60f3\u8981\u5bf9\u5176\u6267\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u65f6\uff0c\u805a\u7c7b\u975e\u5e38\u6709\u7528\uff0c\u56e0\u6b64\u8be5\u6280\u672f\u53ef\u7528\u4e8e\u5728\u6784\u5efa\u5176\u4ed6\u6a21\u578b\u4e4b\u524d\u4e86\u89e3\u6570\u636e\u3002</p> <p>\u2705\u4e00\u65e6\u4f60\u7684\u6570\u636e\u88ab\u7ec4\u7ec7\u6210\u805a\u7c7b\uff0c\u4f60\u5c31\u4e3a\u5b83\u5206\u914d\u4e00\u4e2a\u805a\u7c7b ID\uff0c\u8fd9\u4e2a\u6280\u672f\u5728\u4fdd\u62a4\u6570\u636e\u96c6\u7684\u9690\u79c1\u65f6\u5f88\u6709\u7528\uff1b\u60a8\u53ef\u4ee5\u6539\u4e3a\u901a\u8fc7\u5176\u805a\u7c7b ID \u6765\u5f15\u7528\u6570\u636e\u70b9\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u66f4\u591a\u7684\u53ef\u660e\u663e\u533a\u5206\u7684\u6570\u636e\u3002\u60a8\u80fd\u60f3\u5230\u4e3a\u4ec0\u4e48\u8981\u5f15\u7528\u805a\u7c7b ID \u800c\u4e0d\u662f\u805a\u7c7b\u7684\u5176\u4ed6\u5143\u7d20\u6765\u8bc6\u522b\u5b83\u7684\u5176\u4ed6\u539f\u56e0\u5417\uff1f</p> <p>\u5728\u6b64\u5b66\u4e60\u6a21\u5757\u4e2d\u52a0\u6df1\u60a8\u5bf9\u805a\u7c7b\u6280\u672f\u7684\u7406\u89e3</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_4","title":"\u805a\u7c7b\u5165\u95e8","text":"<p>Scikit-learn \u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u65b9\u6cd5\u6765\u6267\u884c\u805a\u7c7b\u3002\u60a8\u9009\u62e9\u7684\u7c7b\u578b\u5c06\u53d6\u51b3\u4e8e\u60a8\u7684\u7528\u4f8b\u3002\u6839\u636e\u6587\u6863\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u4e0d\u540c\u7684\u597d\u5904\u3002\u4ee5\u4e0b\u662f Scikit-learn \u652f\u6301\u7684\u65b9\u6cd5\u53ca\u5176\u9002\u5f53\u7528\u4f8b\u7684\u7b80\u5316\u8868\uff1a</p> \u65b9\u6cd5\u540d\u79f0 \u7528\u4f8b K-Means \u901a\u7528\u76ee\u7684\uff0c\u5f52\u7eb3\u7684 Affinity propagation \u8bb8\u591a\uff0c\u4e0d\u5747\u5300\u7684\u805a\u7c7b\uff0c\u5f52\u7eb3\u7684 Mean-shift \u8bb8\u591a\uff0c\u4e0d\u5747\u5300\u7684\u805a\u7c7b\uff0c\u5f52\u7eb3\u7684 Spectral clustering \u5c11\u6570\uff0c\u751a\u81f3\u805a\u7c7b\uff0c\u8f6c\u5bfc\u7684 Ward hierarchical clustering \u8bb8\u591a\uff0c\u53d7\u7ea6\u675f\u7684\u805a\u7c7b\uff0c\u8f6c\u5bfc\u7684 Agglomerative clustering \u8bb8\u591a\uff0c\u53d7\u7ea6\u675f\u7684\uff0c\u975e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u8f6c\u5bfc\u7684 DBSCAN \u975e\u5e73\u9762\u51e0\u4f55\uff0c\u4e0d\u5747\u5300\u805a\u7c7b\uff0c\u8f6c\u5bfc\u7684 OPTICS \u4e0d\u5e73\u5766\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5177\u6709\u53ef\u53d8\u5bc6\u5ea6\u7684\u4e0d\u5747\u5300\u805a\u7c7b\uff0c\u8f6c\u5bfc\u7684 Gaussian mixtures \u5e73\u9762\u51e0\u4f55\uff0c\u5f52\u7eb3\u7684 BIRCH \u5177\u6709\u5f02\u5e38\u503c\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5f52\u7eb3\u7684 <p>\ud83c\udf93\u6211\u4eec\u5982\u4f55\u521b\u5efa\u805a\u7c7b\u4e0e\u6211\u4eec\u5982\u4f55\u5c06\u6570\u636e\u70b9\u6536\u96c6\u5230\u7ec4\u4e2d\u6709\u5f88\u5927\u5173\u7cfb\u3002\u8ba9\u6211\u4eec\u5206\u6790\u4e00\u4e9b\u8bcd\u6c47\uff1a</p> <p>\ud83c\udf93 \u201c\u8f6c\u5bfc\u201d\u4e0e\u201c\u5f52\u7eb3\u201d</p> <p>\u8f6c\u5bfc\u63a8\u7406\u6e90\u81ea\u89c2\u5bdf\u5230\u7684\u6620\u5c04\u5230\u7279\u5b9a\u6d4b\u8bd5\u7528\u4f8b\u7684\u8bad\u7ec3\u7528\u4f8b\u3002\u5f52\u7eb3\u63a8\u7406\u6e90\u81ea\u6620\u5c04\u5230\u4e00\u822c\u89c4\u5219\u7684\u8bad\u7ec3\u6848\u4f8b\uff0c\u7136\u540e\u624d\u5e94\u7528\u4e8e\u6d4b\u8bd5\u6848\u4f8b\u3002</p> <p>\u793a\u4f8b\uff1a\u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u4ec5\u90e8\u5206\u6807\u8bb0\u7684\u6570\u636e\u96c6\u3002\u6709\u4e9b\u4e1c\u897f\u662f\u201c\u8bb0\u5f55\u201d\uff0c\u6709\u4e9b\u662f\u201cCD\u201d\uff0c\u6709\u4e9b\u662f\u7a7a\u767d\u7684\u3002\u60a8\u7684\u5de5\u4f5c\u662f\u4e3a\u7a7a\u767d\u63d0\u4f9b\u6807\u7b7e\u3002\u5982\u679c\u60a8\u9009\u62e9\u5f52\u7eb3\u65b9\u6cd5\uff0c\u60a8\u5c06\u8bad\u7ec3\u4e00\u4e2a\u5bfb\u627e\u201c\u8bb0\u5f55\u201d\u548c\u201cCD\u201d\u7684\u6a21\u578b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6807\u7b7e\u5e94\u7528\u4e8e\u672a\u6807\u8bb0\u7684\u6570\u636e\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u96be\u4ee5\u5bf9\u5b9e\u9645\u4e0a\u662f\u201c\u76d2\u5f0f\u78c1\u5e26\u201d\u7684\u4e1c\u897f\u8fdb\u884c\u5206\u7c7b\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8f6c\u5bfc\u65b9\u6cd5\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u8fd9\u4e9b\u672a\u77e5\u6570\u636e\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5c06\u76f8\u4f3c\u7684\u9879\u76ee\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u7136\u540e\u5c06\u6807\u7b7e\u5e94\u7528\u4e8e\u4e00\u4e2a\u7ec4\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u805a\u7c7b\u53ef\u80fd\u53cd\u6620\u201c\u5706\u5f62\u97f3\u4e50\u4e8b\u7269\u201d\u548c\u201c\u65b9\u5f62\u97f3\u4e50\u4e8b\u7269\u201d\u3002</p> <p>\ud83c\udf93 \u201c\u975e\u5e73\u9762\u201d\u4e0e\u201c\u5e73\u9762\u201d\u51e0\u4f55</p> <p>\u6e90\u81ea\u6570\u5b66\u672f\u8bed\uff0c\u975e\u5e73\u9762\u4e0e\u5e73\u9762\u51e0\u4f55\u662f\u6307\u901a\u8fc7\u201c\u5e73\u9762\u201d\uff08\u6b27\u51e0\u91cc\u5fb7\uff09\u6216\u201c\u975e\u5e73\u9762\u201d\uff08\u975e\u6b27\u51e0\u91cc\u5f97\uff09\u51e0\u4f55\u65b9\u6cd5\u6d4b\u91cf\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002</p> <p>\u5728\u6b64\u4e0a\u4e0b\u6587\u4e2d\uff0c\u201c\u5e73\u9762\u201d\u662f\u6307\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff08\u5176\u4e2d\u4e00\u90e8\u5206\u88ab\u6559\u5bfc\u4e3a\u201c\u5e73\u9762\u201d\u51e0\u4f55\uff09\uff0c\u800c\u975e\u5e73\u9762\u662f\u6307\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u3002\u51e0\u4f55\u4e0e\u673a\u5668\u5b66\u4e60\u6709\u4ec0\u4e48\u5173\u7cfb\uff1f\u597d\u5427\uff0c\u4f5c\u4e3a\u690d\u6839\u4e8e\u6570\u5b66\u7684\u4e24\u4e2a\u9886\u57df\uff0c\u5fc5\u987b\u6709\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u805a\u7c7b\u4e2d\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u201c\u5e73\u5766\u201d(flat)\u6216\u201c\u975e\u5e73\u5766\u201d(non-flat)\u7684\u65b9\u5f0f\u5b8c\u6210\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6570\u636e\u7684\u6027\u8d28. \u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u6d4b\u91cf\u4e3a\u4e24\u70b9\u4e4b\u95f4\u7ebf\u6bb5\u7684\u957f\u5ea6\u3002\u975e\u6b27\u8ddd\u79bb\u662f\u6cbf\u66f2\u7ebf\u6d4b\u91cf\u7684\u3002\u5982\u679c\u60a8\u7684\u53ef\u89c6\u5316\u6570\u636e\u4f3c\u4e4e\u4e0d\u5b58\u5728\u4e8e\u5e73\u9762\u4e0a\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4f7f\u7528\u4e13\u95e8\u7684\u7b97\u6cd5\u6765\u5904\u7406\u5b83\u3002</p> <p> Dasani Madipalli \u4f5c\u56fe</p> <p>\ud83c\udf93 '\u8ddd\u79bb'</p> <p>\u805a\u7c7b\u7531\u5b83\u4eec\u7684\u8ddd\u79bb\u77e9\u9635\u5b9a\u4e49\uff0c\u4f8b\u5982\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u8fd9\u4e2a\u8ddd\u79bb\u53ef\u4ee5\u901a\u8fc7\u51e0\u79cd\u65b9\u5f0f\u6765\u6d4b\u91cf\u3002\u6b27\u51e0\u91cc\u5f97\u805a\u7c7b\u7531\u70b9\u503c\u7684\u5e73\u5747\u503c\u5b9a\u4e49\uff0c\u5e76\u5305\u542b\u201c\u8d28\u5fc3\u201d\u6216\u4e2d\u5fc3\u70b9\u3002\u56e0\u6b64\uff0c\u8ddd\u79bb\u662f\u901a\u8fc7\u5230\u8be5\u8d28\u5fc3\u7684\u8ddd\u79bb\u6765\u6d4b\u91cf\u7684\u3002\u975e\u6b27\u5f0f\u8ddd\u79bb\u6307\u7684\u662f\u201c\u805a\u7c7b\u4e2d\u5fc3\u201d\uff0c\u5373\u79bb\u5176\u4ed6\u70b9\u6700\u8fd1\u7684\u70b9\u3002\u805a\u7c7b\u4e2d\u5fc3\u53c8\u53ef\u4ee5\u7528\u5404\u79cd\u65b9\u5f0f\u5b9a\u4e49\u3002</p> <p>\ud83c\udf93 '\u7ea6\u675f'</p> <p>\u7ea6\u675f\u805a\u7c7b\u5c06\u201c\u534a\u76d1\u7763\u201d\u5b66\u4e60\u5f15\u5165\u5230\u8fd9\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2d\u3002\u70b9\u4e4b\u95f4\u7684\u5173\u7cfb\u88ab\u6807\u8bb0\u4e3a\u201c\u65e0\u6cd5\u94fe\u63a5\u201d\u6216\u201c\u5fc5\u987b\u94fe\u63a5\u201d\uff0c\u56e0\u6b64\u5bf9\u6570\u636e\u96c6\u5f3a\u52a0\u4e86\u4e00\u4e9b\u89c4\u5219\u3002</p> <p>\u4e00\u4e2a\u4f8b\u5b50\uff1a\u5982\u679c\u4e00\u4e2a\u7b97\u6cd5\u5728\u4e00\u6279\u672a\u6807\u8bb0\u6216\u534a\u6807\u8bb0\u7684\u6570\u636e\u4e0a\u4e0d\u53d7\u7ea6\u675f\uff0c\u5b83\u4ea7\u751f\u7684\u805a\u7c7b\u8d28\u91cf\u53ef\u80fd\u5f88\u5dee\u3002\u5728\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u805a\u7c7b\u53ef\u80fd\u5c06\u201c\u5706\u5f62\u97f3\u4e50\u4e8b\u7269\u201d\u548c\u201c\u65b9\u5f62\u97f3\u4e50\u4e8b\u7269\u201d\u4ee5\u53ca\u201c\u4e09\u89d2\u5f62\u4e8b\u7269\u201d\u548c\u201c\u997c\u5e72\u201d\u5206\u7ec4\u3002\u5982\u679c\u7ed9\u51fa\u4e00\u4e9b\u7ea6\u675f\u6216\u8981\u9075\u5faa\u7684\u89c4\u5219\uff08\u201c\u7269\u54c1\u5fc5\u987b\u7531\u5851\u6599\u5236\u6210\u201d\u3001\u201c\u7269\u54c1\u9700\u8981\u80fd\u591f\u4ea7\u751f\u97f3\u4e50\u201d\uff09\uff0c\u8fd9\u53ef\u4ee5\u5e2e\u52a9\u201c\u7ea6\u675f\u201d\u7b97\u6cd5\u505a\u51fa\u66f4\u597d\u7684\u9009\u62e9\u3002</p> <p>\ud83c\udf93 '\u5bc6\u5ea6'</p> <p>\u201c\u5608\u6742\u201d\u7684\u6570\u636e\u88ab\u8ba4\u4e3a\u662f\u201c\u5bc6\u96c6\u7684\u201d\u3002\u5728\u68c0\u67e5\u65f6\uff0c\u6bcf\u4e2a\u805a\u7c7b\u4e2d\u7684\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u53ef\u80fd\u6216\u591a\u6216\u5c11\u5730\u5bc6\u96c6\u6216\u201c\u62e5\u6324\u201d\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528\u9002\u5f53\u7684\u805a\u7c7b\u65b9\u6cd5\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u3002\u672c\u6587\u6f14\u793a\u4e86\u4f7f\u7528 K-Means \u805a\u7c7b\u4e0e HDBSCAN \u7b97\u6cd5\u63a2\u7d22\u5177\u6709\u4e0d\u5747\u5300\u805a\u7c7b\u5bc6\u5ea6\u7684\u5608\u6742\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u533a\u522b\u3002</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_5","title":"\u805a\u7c7b\u7b97\u6cd5","text":"<p>\u6709\u8d85\u8fc7 100 \u79cd\u805a\u7c7b\u7b97\u6cd5\uff0c\u5b83\u4eec\u7684\u4f7f\u7528\u53d6\u51b3\u4e8e\u624b\u5934\u6570\u636e\u7684\u6027\u8d28\u3002\u8ba9\u6211\u4eec\u8ba8\u8bba\u4e00\u4e9b\u4e3b\u8981\u7684\uff1a</p> <ul> <li>\u5c42\u6b21\u805a\u7c7b\u3002\u5982\u679c\u4e00\u4e2a\u5bf9\u8c61\u662f\u6839\u636e\u5176\u4e0e\u9644\u8fd1\u5bf9\u8c61\u7684\u63a5\u8fd1\u7a0b\u5ea6\u800c\u4e0d\u662f\u8f83\u8fdc\u5bf9\u8c61\u6765\u5206\u7c7b\u7684\uff0c\u5219\u805a\u7c7b\u662f\u6839\u636e\u5176\u6210\u5458\u4e0e\u5176\u4ed6\u5bf9\u8c61\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u5f62\u6210\u7684\u3002Scikit-learn \u7684\u51dd\u805a\u805a\u7c7b\u662f\u5206\u5c42\u7684\u3002</li> </ul> <p></p> <p>Dasani Madipalli \u4f5c\u56fe</p> <ul> <li>\u8d28\u5fc3\u805a\u7c7b\u3002\u8fd9\u79cd\u6d41\u884c\u7684\u7b97\u6cd5\u9700\u8981\u9009\u62e9\u201ck\u201d\u6216\u8981\u5f62\u6210\u7684\u805a\u7c7b\u6570\u91cf\uff0c\u7136\u540e\u7b97\u6cd5\u786e\u5b9a\u805a\u7c7b\u7684\u4e2d\u5fc3\u70b9\u5e76\u56f4\u7ed5\u8be5\u70b9\u6536\u96c6\u6570\u636e\u3002K-means \u805a\u7c7b\u662f\u8d28\u5fc3\u805a\u7c7b\u7684\u6d41\u884c\u7248\u672c\u3002\u4e2d\u5fc3\u7531\u6700\u8fd1\u7684\u5e73\u5747\u503c\u786e\u5b9a\uff0c\u56e0\u6b64\u53eb\u505a\u8d28\u5fc3\u3002\u4e0e\u805a\u7c7b\u7684\u5e73\u65b9\u8ddd\u79bb\u88ab\u6700\u5c0f\u5316\u3002</li> </ul> <p></p> <p>Dasani Madipalli \u4f5c\u56fe</p> <ul> <li> <p>\u57fa\u4e8e\u5206\u5e03\u7684\u805a\u7c7b\u3002\u57fa\u4e8e\u7edf\u8ba1\u5efa\u6a21\uff0c\u57fa\u4e8e\u5206\u5e03\u7684\u805a\u7c7b\u4e2d\u5fc3\u786e\u5b9a\u4e00\u4e2a\u6570\u636e\u70b9\u5c5e\u4e8e\u4e00\u4e2a\u805a\u7c7b\u7684\u6982\u7387\uff0c\u5e76\u76f8\u5e94\u5730\u5206\u914d\u5b83\u3002\u9ad8\u65af\u6df7\u5408\u65b9\u6cd5\u5c5e\u4e8e\u8fd9\u79cd\u7c7b\u578b\u3002</p> </li> <li> <p>\u57fa\u4e8e\u5bc6\u5ea6\u7684\u805a\u7c7b\u3002\u6570\u636e\u70b9\u6839\u636e\u5b83\u4eec\u7684\u5bc6\u5ea6\u6216\u5b83\u4eec\u5f7c\u6b64\u7684\u5206\u7ec4\u5206\u914d\u7ed9\u805a\u7c7b\u3002\u8fdc\u79bb\u8be5\u7ec4\u7684\u6570\u636e\u70b9\u88ab\u89c6\u4e3a\u5f02\u5e38\u503c\u6216\u566a\u58f0\u3002DBSCAN\u3001Mean-shift \u548c OPTICS \u5c5e\u4e8e\u6b64\u7c7b\u805a\u7c7b\u3002</p> </li> <li> <p>\u57fa\u4e8e\u7f51\u683c\u7684\u805a\u7c7b\u3002\u5bf9\u4e8e\u591a\u7ef4\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e00\u4e2a\u7f51\u683c\u5e76\u5c06\u6570\u636e\u5212\u5206\u5230\u7f51\u683c\u7684\u5355\u5143\u683c\u4e2d\uff0c\u4ece\u800c\u521b\u5efa\u805a\u7c7b\u3002</p> </li> </ul>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u5bf9\u4f60\u7684\u6570\u636e\u8fdb\u884c\u805a\u7c7b","text":"<p>\u9002\u5f53\u7684\u53ef\u89c6\u5316\u5bf9\u805a\u7c7b\u4f5c\u4e3a\u4e00\u79cd\u6280\u672f\u6709\u5f88\u5927\u5e2e\u52a9\uff0c\u6240\u4ee5\u8ba9\u6211\u4eec\u4ece\u53ef\u89c6\u5316\u6211\u4eec\u7684\u97f3\u4e50\u6570\u636e\u5f00\u59cb\u3002\u8fd9\u4e2a\u7ec3\u4e60\u5c06\u5e2e\u52a9\u6211\u4eec\u51b3\u5b9a\u6211\u4eec\u5e94\u8be5\u6700\u6709\u6548\u5730\u4f7f\u7528\u54ea\u79cd\u805a\u7c7b\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u7684\u6027\u8d28\u3002</p> <ol> <li> <p>\u6253\u5f00\u6b64\u6587\u4ef6\u5939\u4e2d\u7684 notebook.ipynb \u6587\u4ef6\u3002</p> </li> <li> <p>\u5bfc\u5165 <code>Seaborn</code> \u5305\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u6570\u636e\u53ef\u89c6\u5316\u3002</p> <pre><code>!pip install seaborn\n</code></pre> </li> <li> <p>\u9644\u52a0\u6765\u81ea nigerian-songs.csv \u7684\u6b4c\u66f2\u6570\u636e\u3002\u52a0\u8f7d\u5305\u542b\u6709\u5173\u6b4c\u66f2\u7684\u4e00\u4e9b\u6570\u636e\u7684\u6570\u636e\u5e27\u3002\u51c6\u5907\u597d\u901a\u8fc7\u5bfc\u5165\u5e93\u548c\u8f6c\u50a8\u6570\u636e\u6765\u63a2\u7d22\u8fd9\u4e9b\u6570\u636e\uff1a</p> <pre><code>import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/nigerian-songs.csv\")\ndf.head()\n</code></pre> <p>\u68c0\u67e5\u524d\u51e0\u884c\u6570\u636e\uff1a</p> name album artist artist_top_genre release_date length popularity danceability acousticness energy instrumentalness liveness loudness speechiness tempo time_signature 0 Sparky Mandy &amp; The Jungle Cruel Santino alternative r&amp;b 2019 144000 48 0.666 0.851 0.42 0.534 0.11 -6.699 0.0829 133.015 5 1 shuga rush EVERYTHING YOU HEARD IS TRUE Odunsi (The Engine) afropop 2020 89488 30 0.71 0.0822 0.683 0.000169 0.101 -5.64 0.36 129.993 3 2 LITT! LITT! AYL\u00d8 indie r&amp;b 2018 207758 40 0.836 0.272 0.564 0.000537 0.11 -7.127 0.0424 130.005 4 3 Confident / Feeling Cool Enjoy Your Life Lady Donli nigerian pop 2019 175135 14 0.894 0.798 0.611 0.000187 0.0964 -4.961 0.113 111.087 4 4 wanted you rare. Odunsi (The Engine) afropop 2018 152049 25 0.702 0.116 0.833 0.91 0.348 -6.044 0.0447 105.115 4 </li> <li> <p>\u83b7\u53d6\u6709\u5173\u6570\u636e\u5e27\u7684\u4e00\u4e9b\u4fe1\u606f\uff0c\u8c03\u7528 <code>info()</code>\uff1a</p> <pre><code>df.info()\n</code></pre> </li> </ol> <p>\u8f93\u51fa\u770b\u8d77\u6765\u50cf\u8fd9\u6837\uff1a</p> <pre><code>```output\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 530 entries, 0 to 529\nData columns (total 16 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   name              530 non-null    object \n 1   album             530 non-null    object \n 2   artist            530 non-null    object \n 3   artist_top_genre  530 non-null    object \n 4   release_date      530 non-null    int64  \n 5   length            530 non-null    int64  \n 6   popularity        530 non-null    int64  \n 7   danceability      530 non-null    float64\n 8   acousticness      530 non-null    float64\n 9   energy            530 non-null    float64\n 10  instrumentalness  530 non-null    float64\n 11  liveness          530 non-null    float64\n 12  loudness          530 non-null    float64\n 13  speechiness       530 non-null    float64\n 14  tempo             530 non-null    float64\n 15  time_signature    530 non-null    int64  \ndtypes: float64(8), int64(4), object(4)\nmemory usage: 66.4+ KB\n```\n</code></pre> <ol> <li> <p>\u901a\u8fc7\u8c03\u7528 <code>isnull()</code> \u548c\u9a8c\u8bc1\u603b\u548c\u4e3a 0 \u6765\u4ed4\u7ec6\u68c0\u67e5\u7a7a\u503c\uff1a</p> <pre><code>df.isnull().sum()\n</code></pre> <p>\u770b\u8d77\u6765\u4e0d\u9519\uff1a</p> <pre><code>name                0\nalbum               0\nartist              0\nartist_top_genre    0\nrelease_date        0\nlength              0\npopularity          0\ndanceability        0\nacousticness        0\nenergy              0\ninstrumentalness    0\nliveness            0\nloudness            0\nspeechiness         0\ntempo               0\ntime_signature      0\ndtype: int64\n</code></pre> </li> <li> <p>\u63cf\u8ff0\u6570\u636e\uff1a</p> <pre><code>df.describe()\n</code></pre> release_date length popularity danceability acousticness energy instrumentalness liveness loudness speechiness tempo time_signature count 530 530 530 530 530 530 530 530 530 530 530 530 mean 2015.390566 222298.1698 17.507547 0.741619 0.265412 0.760623 0.016305 0.147308 -4.953011 0.130748 116.487864 3.986792 std 3.131688 39696.82226 18.992212 0.117522 0.208342 0.148533 0.090321 0.123588 2.464186 0.092939 23.518601 0.333701 min 1998 89488 0 0.255 0.000665 0.111 0 0.0283 -19.362 0.0278 61.695 3 25% 2014 199305 0 0.681 0.089525 0.669 0 0.07565 -6.29875 0.0591 102.96125 4 50% 2016 218509 13 0.761 0.2205 0.7845 0.000004 0.1035 -4.5585 0.09795 112.7145 4 75% 2017 242098.5 31 0.8295 0.403 0.87575 0.000234 0.164 -3.331 0.177 125.03925 4 max 2020 511738 73 0.966 0.954 0.995 0.91 0.811 0.582 0.514 206.007 5 </li> </ol> <p>\ud83e\udd14\u5982\u679c\u6211\u4eec\u6b63\u5728\u4f7f\u7528\u805a\u7c7b\uff0c\u4e00\u79cd\u4e0d\u9700\u8981\u6807\u8bb0\u6570\u636e\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4e3a\u4ec0\u4e48\u6211\u4eec\u7528\u6807\u7b7e\u663e\u793a\u8fd9\u4e9b\u6570\u636e\uff1f\u5728\u6570\u636e\u63a2\u7d22\u9636\u6bb5\uff0c\u5b83\u4eec\u6d3e\u4e0a\u7528\u573a\uff0c\u4f46\u5b83\u4eec\u4e0d\u662f\u805a\u7c7b\u7b97\u6cd5\u5de5\u4f5c\u6240\u5fc5\u9700\u7684\u3002\u60a8\u4e5f\u53ef\u4ee5\u5220\u9664\u5217\u6807\u9898\u5e76\u6309\u5217\u53f7\u5f15\u7528\u6570\u636e\u3002</p> <p>\u67e5\u770b\u6570\u636e\u7684\u666e\u904d\u503c\u3002\u8bf7\u6ce8\u610f\uff0c\u6d41\u884c\u5ea6\u53ef\u4ee5\u662f\u201c0\u201d\uff0c\u8868\u793a\u6ca1\u6709\u6392\u540d\u7684\u6b4c\u66f2\u3002\u8ba9\u6211\u4eec\u5c3d\u5feb\u5220\u9664\u5b83\u4eec\u3002</p> <ol> <li> <p>\u4f7f\u7528\u6761\u5f62\u56fe\u627e\u51fa\u6700\u53d7\u6b22\u8fce\u7684\u7c7b\u578b\uff1a</p> <pre><code>import seaborn as sns\n\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top[:5].index,y=top[:5].values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> <p></p> </li> </ol> <p>\u2705\u5982\u679c\u60a8\u60f3\u67e5\u770b\u66f4\u591a\u9876\u90e8\u503c\uff0c\u8bf7\u5c06\u9876\u90e8\u66f4\u6539<code>[:5]</code>\u4e3a\u66f4\u5927\u7684\u503c\uff0c\u6216\u5c06\u5176\u5220\u9664\u4ee5\u67e5\u770b\u5168\u90e8\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u5f53\u9876\u7ea7\u6d41\u6d3e\u88ab\u63cf\u8ff0\u4e3a\u201c\u7f3a\u5931\u201d\u65f6\uff0c\u8fd9\u610f\u5473\u7740 Spotify \u6ca1\u6709\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\uff0c\u6240\u4ee5\u8ba9\u6211\u4eec\u907f\u514d\u5b83\u3002</p> <ol> <li> <p>\u901a\u8fc7\u8fc7\u6ee4\u6389\u4e22\u5931\u7684\u6570\u636e\u6765\u907f\u514d</p> <pre><code>df = df[df['artist_top_genre'] != 'Missing']\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top.index,y=top.values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> <p>\u73b0\u5728\u91cd\u65b0\u68c0\u67e5 genres\uff1a</p> <p></p> </li> <li> <p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u524d\u4e09\u5927\u6d41\u6d3e\u4e3b\u5bfc\u4e86\u8fd9\u4e2a\u6570\u636e\u96c6\u3002\u8ba9\u6211\u4eec\u4e13\u6ce8\u4e8e <code>afro dancehall</code>\uff0c<code>afropop</code> \u548c <code>nigerian pop</code>\uff0c\u53e6\u5916\u8fc7\u6ee4\u6570\u636e\u96c6\u4ee5\u5220\u9664\u4efb\u4f55\u5177\u6709 0 \u6d41\u884c\u5ea6\u503c\u7684\u5185\u5bb9\uff08\u8fd9\u610f\u5473\u7740\u5b83\u5728\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u88ab\u5f52\u7c7b\u4e3a\u6d41\u884c\u5ea6\u5e76\u4e14\u53ef\u4ee5\u88ab\u89c6\u4e3a\u6211\u4eec\u7684\u76ee\u7684\u7684\u566a\u97f3\uff09\uff1a</p> <pre><code>df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]\ndf = df[(df['popularity'] &gt; 0)]\ntop = df['artist_top_genre'].value_counts()\nplt.figure(figsize=(10,7))\nsns.barplot(x=top.index,y=top.values)\nplt.xticks(rotation=45)\nplt.title('Top genres',color = 'blue')\n</code></pre> </li> <li> <p>\u505a\u4e00\u4e2a\u5feb\u901f\u6d4b\u8bd5\uff0c\u770b\u770b\u6570\u636e\u662f\u5426\u4ee5\u4efb\u4f55\u7279\u522b\u5f3a\u7684\u65b9\u5f0f\u76f8\u5173\uff1a</p> <pre><code>corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)\n</code></pre> <p></p> <p>\u552f\u4e00\u5f3a\u76f8\u5173\u6027\u662f <code>energy</code> \u548c\u4e4b\u95f4 <code>loudness</code>\uff0c\u8fd9\u5e76\u4e0d\u5947\u602a\uff0c\u56e0\u4e3a\u5608\u6742\u7684\u97f3\u4e50\u901a\u5e38\u975e\u5e38\u6709\u6d3b\u529b\u3002\u5426\u5219\uff0c\u76f8\u5173\u6027\u76f8\u5bf9\u8f83\u5f31\u3002\u770b\u770b\u805a\u7c7b\u7b97\u6cd5\u53ef\u4ee5\u5982\u4f55\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u4f1a\u5f88\u6709\u8da3\u3002</p> <p>\ud83c\udf93\u8bf7\u6ce8\u610f\uff0c\u76f8\u5173\u6027\u5e76\u4e0d\u610f\u5473\u7740\u56e0\u679c\u5173\u7cfb\uff01\u6211\u4eec\u6709\u76f8\u5173\u6027\u7684\u8bc1\u636e\uff0c\u4f46\u6ca1\u6709\u56e0\u679c\u5173\u7cfb\u7684\u8bc1\u636e\u3002\u4e00\u4e2a\u6709\u8da3\u7684\u7f51\u7ad9\u6709\u4e00\u4e9b\u5f3a\u8c03\u8fd9\u4e00\u70b9\u7684\u89c6\u89c9\u6548\u679c\u3002</p> </li> </ol> <p>\u8fd9\u4e2a\u6570\u636e\u96c6\u662f\u5426\u56f4\u7ed5\u6b4c\u66f2\u7684\u6d41\u884c\u5ea6\u548c\u53ef\u821e\u6027\u6709\u4efb\u4f55\u6536\u655b\uff1fFacetGrid \u663e\u793a\u65e0\u8bba\u6d41\u6d3e\u5982\u4f55\uff0c\u90fd\u6709\u540c\u5fc3\u5706\u6392\u5217\u3002\u5bf9\u4e8e\u8fd9\u79cd\u7c7b\u578b\uff0c\u5c3c\u65e5\u5229\u4e9a\u4eba\u7684\u53e3\u5473\u662f\u5426\u4f1a\u5728\u67d0\u79cd\u7a0b\u5ea6\u7684\u53ef\u821e\u6027\u4e0a\u8d8b\u4e8e\u4e00\u81f4\uff1f</p> <p>\u2705 \u5c1d\u8bd5\u4e0d\u540c\u7684\u6570\u636e\u70b9\uff08\u80fd\u91cf\u3001\u54cd\u5ea6\u3001\u8bed\u97f3\uff09\u548c\u66f4\u591a\u6216\u4e0d\u540c\u7684\u97f3\u4e50\u7c7b\u578b\u3002\u4f60\u80fd\u53d1\u73b0\u4ec0\u4e48\uff1f\u67e5\u770b <code>df.describe()</code> \u8868\u683c\u4ee5\u4e86\u89e3\u6570\u636e\u70b9\u7684\u4e00\u822c\u5206\u5e03\u3002</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u6570\u636e\u5206\u5e03","text":"<p>\u8fd9\u4e09\u79cd\u6d41\u6d3e\u662f\u5426\u56e0\u5176\u53d7\u6b22\u8fce\u7a0b\u5ea6\u800c\u5bf9\u5176\u53ef\u821e\u6027\u7684\u770b\u6cd5\u6709\u663e\u7740\u5dee\u5f02\uff1f</p> <ol> <li> <p>\u68c0\u67e5\u6211\u4eec\u6cbf\u7ed9\u5b9a x \u548c y \u8f74\u7684\u6d41\u884c\u5ea6\u548c\u53ef\u821e\u6027\u7684\u524d\u4e09\u79cd\u7c7b\u578b\u6570\u636e\u5206\u5e03\u3002</p> <pre><code>sns.set_theme(style=\"ticks\")\n\ng = sns.jointplot(\n    data=df,\n    x=\"popularity\", y=\"danceability\", hue=\"artist_top_genre\",\n    kind=\"kde\",\n)\n</code></pre> <p>\u60a8\u53ef\u4ee5\u53d1\u73b0\u56f4\u7ed5\u4e00\u822c\u6536\u655b\u70b9\u7684\u540c\u5fc3\u5706\uff0c\u663e\u793a\u70b9\u7684\u5206\u5e03\u3002</p> <p>\ud83c\udf93\u8bf7\u6ce8\u610f\uff0c\u6b64\u793a\u4f8b\u4f7f\u7528 KDE\uff08\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff09\u56fe\uff0c\u8be5\u56fe\u4f7f\u7528\u8fde\u7eed\u6982\u7387\u5bc6\u5ea6\u66f2\u7ebf\u8868\u793a\u6570\u636e\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u5728\u5904\u7406\u591a\u4e2a\u5206\u5e03\u65f6\u89e3\u91ca\u6570\u636e\u3002</p> <p>\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e09\u79cd\u6d41\u6d3e\u5728\u6d41\u884c\u5ea6\u548c\u53ef\u821e\u6027\u65b9\u9762\u677e\u6563\u5730\u5bf9\u9f50\u3002\u5728\u8fd9\u79cd\u677e\u6563\u5bf9\u9f50\u7684\u6570\u636e\u4e2d\u786e\u5b9a\u805a\u7c7b\u5c06\u662f\u4e00\u4e2a\u6311\u6218\uff1a</p> <p></p> </li> <li> <p>\u521b\u5efa\u6563\u70b9\u56fe\uff1a</p> <pre><code>sns.FacetGrid(df, hue=\"artist_top_genre\", size=5) \\\n   .map(plt.scatter, \"popularity\", \"danceability\") \\\n   .add_legend()\n</code></pre> <p>\u76f8\u540c\u8f74\u7684\u6563\u70b9\u56fe\u663e\u793a\u4e86\u7c7b\u4f3c\u7684\u6536\u655b\u6a21\u5f0f</p> <p></p> </li> </ol> <p>\u4e00\u822c\u6765\u8bf4\uff0c\u5bf9\u4e8e\u805a\u7c7b\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u6563\u70b9\u56fe\u6765\u5c55\u793a\u6570\u636e\u7684\u805a\u7c7b\uff0c\u6240\u4ee5\u638c\u63e1\u8fd9\u79cd\u7c7b\u578b\u7684\u53ef\u89c6\u5316\u662f\u975e\u5e38\u6709\u7528\u7684\u3002\u5728\u4e0b\u4e00\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u5e76\u4f7f\u7528 k-means \u805a\u7c7b\u6765\u53d1\u73b0\u8fd9\u4e9b\u6570\u636e\u4e2d\u4ee5\u6709\u8da3\u65b9\u5f0f\u91cd\u53e0\u7684\u7ec4\u3002</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_6","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u4e3a\u4e0b\u4e00\u8bfe\u505a\u51c6\u5907\uff0c\u5236\u4f5c\u4e00\u5f20\u56fe\u8868\uff0c\u8bf4\u660e\u60a8\u53ef\u80fd\u4f1a\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u53d1\u73b0\u548c\u4f7f\u7528\u7684\u5404\u79cd\u805a\u7c7b\u7b97\u6cd5\u3002</p> <p>\u805a\u7c7b\u8bd5\u56fe\u89e3\u51b3\u4ec0\u4e48\u6837\u7684\u95ee\u9898\uff1f</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_7","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_8","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u5728\u5e94\u7528\u805a\u7c7b\u7b97\u6cd5\u4e4b\u524d\uff0c\u6b63\u5982\u6211\u4eec\u6240\u4e86\u89e3\u7684\uff0c\u4e86\u89e3\u6570\u636e\u96c6\u7684\u6027\u8d28\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\u3002\u5728\u6b64\u5904\u9605\u8bfb\u6709\u5173\u6b64\u4e3b\u9898\u7684\u66f4\u591a\u4fe1\u606f</p> <p>\u8fd9\u7bc7\u6709\u7528\u7684\u6587\u7ae0\u5c06\u5f15\u5bfc\u60a8\u4e86\u89e3\u5404\u79cd\u805a\u7c7b\u7b97\u6cd5\u5728\u7ed9\u5b9a\u4e0d\u540c\u6570\u636e\u5f62\u72b6\u7684\u60c5\u51b5\u4e0b\u7684\u4e0d\u540c\u884c\u4e3a\u65b9\u5f0f\u3002</p>"},{"location":"5-Clustering/1-Visualize/README.zh-cn/#_9","title":"\u4f5c\u4e1a","text":"<p>\u7814\u7a76\u7528\u4e8e\u805a\u7c7b\u7684\u5176\u4ed6\u53ef\u89c6\u5316</p>"},{"location":"5-Clustering/1-Visualize/assignment/","title":"Research other visualizations for clustering","text":""},{"location":"5-Clustering/1-Visualize/assignment/#instructions","title":"Instructions","text":"<p>In this lesson, you have worked with some visualization techniques to get a grasp on plotting your data in preparation for clustering it. Scatterplots, in particular are useful for finding groups of objects. Research different ways and different libraries to create scatterplots and document your work in a notebook. You can use the data from this lesson, other lessons, or data you source yourself (please credit its source, however, in your notebook). Plot some data using scatterplots and explain what you discover.</p>"},{"location":"5-Clustering/1-Visualize/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with five well-documented scatterplots A notebook is presented with fewer than five scatterplots and it is less well documented An incomplete notebook is presented"},{"location":"5-Clustering/1-Visualize/assignment.zh-cn/","title":"\u7814\u7a76\u7528\u4e8e\u805a\u7c7b\u7684\u5176\u4ed6\u53ef\u89c6\u5316","text":""},{"location":"5-Clustering/1-Visualize/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u672c\u8282\u8bfe\u4e2d\uff0c\u60a8\u4f7f\u7528\u4e86\u4e00\u4e9b\u53ef\u89c6\u5316\u6280\u672f\u6765\u638c\u63e1\u7ed8\u5236\u6570\u636e\u56fe\uff0c\u4e3a\u805a\u7c7b\u6570\u636e\u505a\u51c6\u5907\u3002\u6563\u70b9\u56fe\u5728\u5bfb\u627e\u4e00\u7ec4\u5bf9\u8c61\u65f6\u5c24\u5176\u6709\u7528\u3002\u7814\u7a76\u4e0d\u540c\u7684\u65b9\u6cd5\u548c\u4e0d\u540c\u7684\u5e93\u6765\u521b\u5efa\u6563\u70b9\u56fe\uff0c\u5e76\u5728 notebook \u4e0a\u8bb0\u5f55\u4f60\u7684\u5de5\u4f5c\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u8fd9\u8282\u8bfe\u7684\u6570\u636e\uff0c\u5176\u4ed6\u8bfe\u7684\u6570\u636e\uff0c\u6216\u8005\u4f60\u81ea\u5df1\u7684\u6570\u636e\uff08\u4f46\u662f\uff0c\u8bf7\u628a\u5b83\u7684\u6765\u6e90\u8bb0\u5728\u4f60\u7684 notebook \u4e0a\uff09\u3002\u7528\u6563\u70b9\u56fe\u7ed8\u5236\u4e00\u4e9b\u6570\u636e\uff0c\u5e76\u89e3\u91ca\u4f60\u7684\u53d1\u73b0\u3002</p>"},{"location":"5-Clustering/1-Visualize/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u89c4\u5219","text":"\u8bc4\u5224\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b notebook \u4e0a\u6709\u4e94\u4e2a\u8be6\u7ec6\u6587\u6863\u7684\u6563\u70b9\u56fe notebook \u4e0a\u7684\u6563\u70b9\u56fe\u5c11\u4e8e 5 \u4e2a\uff0c\u800c\u4e14\u6587\u6863\u5199\u5f97\u4e0d\u592a\u8be6\u7ec6 \u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684 notebook"},{"location":"5-Clustering/1-Visualize/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"5-Clustering/2-K-Means/","title":"K-Means clustering","text":""},{"location":"5-Clustering/2-K-Means/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>In this lesson, you will learn how to create clusters using Scikit-learn and the Nigerian music dataset you imported earlier. We will cover the basics of K-Means for Clustering. Keep in mind that, as you learned in the earlier lesson, there are many ways to work with clusters and the method you use depends on your data. We will try K-Means as it's the most common clustering technique. Let's get started!</p> <p>Terms you will learn about:</p> <ul> <li>Silhouette scoring</li> <li>Elbow method</li> <li>Inertia</li> <li>Variance</li> </ul>"},{"location":"5-Clustering/2-K-Means/#introduction","title":"Introduction","text":"<p>K-Means Clustering is a method derived from the domain of signal processing. It is used to divide and partition groups of data into 'k' clusters using a series of observations. Each observation works to group a given datapoint closest to its nearest 'mean', or the center point of a cluster.</p> <p>The clusters can be visualized as Voronoi diagrams, which include a point (or 'seed') and its corresponding region. </p> <p></p> <p>infographic by Jen Looper</p> <p>The K-Means clustering process executes in a three-step process:</p> <ol> <li>The algorithm selects k-number of center points by sampling from the dataset. After this, it loops:<ol> <li>It assigns each sample to the nearest centroid.</li> <li>It creates new centroids by taking the mean value of all of the samples assigned to the previous centroids.</li> <li>Then, it calculates the difference between the new and old centroids and repeats until the centroids are stabilized.</li> </ol> </li> </ol> <p>One drawback of using K-Means includes the fact that you will need to establish 'k', that is the number of centroids. Fortunately the  'elbow method' helps to estimate a good starting value for 'k'. You'll try it in a minute.</p>"},{"location":"5-Clustering/2-K-Means/#prerequisite","title":"Prerequisite","text":"<p>You will work in this lesson's notebook.ipynb file that includes the data import and preliminary cleaning you did in the last lesson.</p>"},{"location":"5-Clustering/2-K-Means/#exercise-preparation","title":"Exercise - preparation","text":"<p>Start by taking another look at the songs data.</p> <ol> <li> <p>Create a boxplot, calling <code>boxplot()</code> for each column:</p> <pre><code>plt.figure(figsize=(20,20), dpi=200)\n\nplt.subplot(4,3,1)\nsns.boxplot(x = 'popularity', data = df)\n\nplt.subplot(4,3,2)\nsns.boxplot(x = 'acousticness', data = df)\n\nplt.subplot(4,3,3)\nsns.boxplot(x = 'energy', data = df)\n\nplt.subplot(4,3,4)\nsns.boxplot(x = 'instrumentalness', data = df)\n\nplt.subplot(4,3,5)\nsns.boxplot(x = 'liveness', data = df)\n\nplt.subplot(4,3,6)\nsns.boxplot(x = 'loudness', data = df)\n\nplt.subplot(4,3,7)\nsns.boxplot(x = 'speechiness', data = df)\n\nplt.subplot(4,3,8)\nsns.boxplot(x = 'tempo', data = df)\n\nplt.subplot(4,3,9)\nsns.boxplot(x = 'time_signature', data = df)\n\nplt.subplot(4,3,10)\nsns.boxplot(x = 'danceability', data = df)\n\nplt.subplot(4,3,11)\nsns.boxplot(x = 'length', data = df)\n\nplt.subplot(4,3,12)\nsns.boxplot(x = 'release_date', data = df)\n</code></pre> <p>This data is a little noisy: by observing each column as a boxplot, you can see outliers.</p> <p></p> </li> </ol> <p>You could go through the dataset and remove these outliers, but that would make the data pretty minimal.</p> <ol> <li> <p>For now, choose which columns you will use for your clustering exercise. Pick ones with similar ranges and encode the <code>artist_top_genre</code> column as numeric data:</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nX = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]\n\ny = df['artist_top_genre']\n\nX['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])\n\ny = le.transform(y)\n</code></pre> </li> <li> <p>Now you need to pick how many clusters to target. You know there are 3 song genres that we carved out of the dataset, so let's try 3:</p> <pre><code>from sklearn.cluster import KMeans\n\nnclusters = 3 \nseed = 0\n\nkm = KMeans(n_clusters=nclusters, random_state=seed)\nkm.fit(X)\n\n# Predict the cluster for each data point\n\ny_cluster_kmeans = km.predict(X)\ny_cluster_kmeans\n</code></pre> </li> </ol> <p>You see an array printed out with predicted clusters (0, 1,or 2) for each row of the dataframe.</p> <ol> <li> <p>Use this array to calculate a 'silhouette score':</p> <pre><code>from sklearn import metrics\nscore = metrics.silhouette_score(X, y_cluster_kmeans)\nscore\n</code></pre> </li> </ol>"},{"location":"5-Clustering/2-K-Means/#silhouette-score","title":"Silhouette score","text":"<p>Look for a silhouette score closer to 1. This score varies from -1 to 1, and if the score is 1, the cluster is dense and well-separated from other clusters. A value near 0 represents overlapping clusters with samples very close to the decision boundary of the neighboring clusters. (Source)</p> <p>Our score is .53, so right in the middle. This indicates that our data is not particularly well-suited to this type of clustering, but let's continue.</p>"},{"location":"5-Clustering/2-K-Means/#exercise-build-a-model","title":"Exercise - build a model","text":"<ol> <li> <p>Import <code>KMeans</code> and start the clustering process.</p> <pre><code>from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n</code></pre> <p>There are a few parts here that warrant explaining.</p> <p>\ud83c\udf93 range: These are the iterations of the clustering process</p> <p>\ud83c\udf93 random_state: \"Determines random number generation for centroid initialization.\" Source</p> <p>\ud83c\udf93 WCSS: \"within-cluster sums of squares\" measures the squared average distance of all the points within a cluster to the cluster centroid. Source. </p> <p>\ud83c\udf93 Inertia: K-Means algorithms attempt to choose centroids to minimize 'inertia', \"a measure of how internally coherent clusters are.\" Source. The value is appended to the wcss variable on each iteration.</p> <p>\ud83c\udf93 k-means++: In Scikit-learn you can use the 'k-means++' optimization, which \"initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization.</p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/#elbow-method","title":"Elbow method","text":"<p>Previously, you surmised that, because you have targeted 3 song genres, you should choose 3 clusters. But is that the case?</p> <ol> <li> <p>Use the 'elbow method' to make sure.</p> <pre><code>plt.figure(figsize=(10,5))\nsns.lineplot(range(1, 11), wcss,marker='o',color='red')\nplt.title('Elbow')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n</code></pre> <p>Use the <code>wcss</code> variable that you built in the previous step to create a chart showing where the 'bend' in the elbow is, which indicates the optimum number of clusters. Maybe it is 3!</p> <p></p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/#exercise-display-the-clusters","title":"Exercise - display the clusters","text":"<ol> <li> <p>Try the process again, this time setting three clusters, and display the clusters as a scatterplot:</p> <pre><code>from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 3)\nkmeans.fit(X)\nlabels = kmeans.predict(X)\nplt.scatter(df['popularity'],df['danceability'],c = labels)\nplt.xlabel('popularity')\nplt.ylabel('danceability')\nplt.show()\n</code></pre> </li> <li> <p>Check the model's accuracy:</p> <pre><code>labels = kmeans.labels_\n\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n\nprint('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))\n</code></pre> <p>This model's accuracy is not very good, and the shape of the clusters gives you a hint why. </p> <p></p> <p>This data is too imbalanced, too little correlated and there is too much variance between the column values to cluster well. In fact, the clusters that form are probably heavily influenced or skewed by the three genre categories we defined above. That was a learning process!</p> <p>In Scikit-learn's documentation, you can see that a model like this one, with clusters not very well demarcated, has a 'variance' problem:</p> <p></p> <p>Infographic from Scikit-learn</p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/#variance","title":"Variance","text":"<p>Variance is defined as \"the average of the squared differences from the Mean\" (Source). In the context of this clustering problem, it refers to data that the numbers of our dataset tend to diverge a bit too much from the mean. </p> <p>\u2705 This is a great moment to think about all the ways you could correct this issue. Tweak the data a bit more? Use different columns? Use a different algorithm? Hint: Try scaling your data to normalize it and test other columns.</p> <p>Try this 'variance calculator' to understand the concept a bit more.</p>"},{"location":"5-Clustering/2-K-Means/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Spend some time with this notebook, tweaking parameters. Can you improve the accuracy of the model by cleaning  the data more (removing outliers, for example)? You can use weights to give more weight to given data samples. What else can you do to create better clusters?</p> <p>Hint: Try to scale your data. There's commented code in the notebook that adds standard scaling to make the data columns resemble each other more closely in terms of range. You'll find that while the silhouette score goes down, the 'kink' in the elbow graph smooths out. This is because leaving the data unscaled allows data with less variance to carry more weight. Read a bit more on this problem here.</p>"},{"location":"5-Clustering/2-K-Means/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"5-Clustering/2-K-Means/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take a look at a K-Means Simulator such as this one. You can use this tool to visualize sample data points and determine its centroids. You can edit the data's randomness, numbers of clusters and numbers of centroids. Does this help you get an idea of how the data can be grouped?</p> <p>Also, take a look at this handout on K-Means from Stanford.</p>"},{"location":"5-Clustering/2-K-Means/#assignment","title":"Assignment","text":"<p>Try different clustering methods</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/","title":"K-Means \u805a\u7c7b","text":"<p>\ud83c\udfa5 \u5355\u51fb\u4e0a\u56fe\u89c2\u770b\u89c6\u9891\uff1aAndrew Ng \u89e3\u91ca\u805a\u7c7b</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528 Scikit-learn \u548c\u60a8\u4e4b\u524d\u5bfc\u5165\u7684\u5c3c\u65e5\u5229\u4e9a\u97f3\u4e50\u6570\u636e\u96c6\u521b\u5efa\u805a\u7c7b\u3002\u6211\u4eec\u5c06\u4ecb\u7ecd K-Means \u805a\u7c7b \u7684\u57fa\u7840\u77e5\u8bc6\u3002\u8bf7\u8bb0\u4f4f\uff0c\u6b63\u5982\u60a8\u5728\u4e0a\u4e00\u8bfe\u4e2d\u5b66\u5230\u7684\uff0c\u4f7f\u7528\u805a\u7c7b\u7684\u65b9\u6cd5\u6709\u5f88\u591a\u79cd\uff0c\u60a8\u4f7f\u7528\u7684\u65b9\u6cd5\u53d6\u51b3\u4e8e\u60a8\u7684\u6570\u636e\u3002\u6211\u4eec\u5c06\u5c1d\u8bd5 K-Means\uff0c\u56e0\u4e3a\u5b83\u662f\u6700\u5e38\u89c1\u7684\u805a\u7c7b\u6280\u672f\u3002\u8ba9\u6211\u4eec\u5f00\u59cb\u5427\uff01</p> <p>\u60a8\u5c06\u4e86\u89e3\u7684\u672f\u8bed\uff1a</p> <ul> <li>\u8f6e\u5ed3\u6253\u5206</li> <li>\u624b\u8098\u65b9\u6cd5</li> <li>\u60ef\u6027</li> <li>\u65b9\u5dee</li> </ul>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_2","title":"\u4ecb\u7ecd","text":"<p>K-Means Clustering \u662f\u4e00\u79cd\u6e90\u81ea\u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684\u65b9\u6cd5\u3002\u5b83\u7528\u4e8e\u4f7f\u7528\u4e00\u7cfb\u5217\u89c2\u5bdf\u5c06\u6570\u636e\u7ec4\u5212\u5206\u548c\u5212\u5206\u4e3a\u201ck\u201d\u4e2a\u805a\u7c7b\u3002\u6bcf\u4e2a\u89c2\u5bdf\u90fd\u7528\u4e8e\u5bf9\u6700\u63a5\u8fd1\u5176\u6700\u8fd1\u201c\u5e73\u5747\u503c\u201d\u6216\u805a\u7c7b\u4e2d\u5fc3\u70b9\u7684\u7ed9\u5b9a\u6570\u636e\u70b9\u8fdb\u884c\u5206\u7ec4\u3002</p> <p>\u805a\u7c7b\u53ef\u4ee5\u53ef\u89c6\u5316\u4e3a Voronoi \u56fe\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u70b9\uff08\u6216\u201c\u79cd\u5b50\u201d\uff09\u53ca\u5176\u76f8\u5e94\u7684\u533a\u57df\u3002</p> <p></p> <p>Jen Looper\u4f5c\u56fe</p> <p>K-Means \u805a\u7c7b\u8fc7\u7a0b\u5206\u4e09\u6b65\u6267\u884c\uff1a</p> <ol> <li>\u8be5\u7b97\u6cd5\u901a\u8fc7\u4ece\u6570\u636e\u96c6\u4e2d\u91c7\u6837\u6765\u9009\u62e9 k \u4e2a\u4e2d\u5fc3\u70b9\u3002\u5728\u6b64\u4e4b\u540e\uff0c\u5b83\u5faa\u73af\uff1a</li> <li>\u5b83\u5c06\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u5230\u6700\u8fd1\u7684\u8d28\u5fc3\u3002</li> <li>\u5b83\u901a\u8fc7\u53d6\u5206\u914d\u7ed9\u5148\u524d\u8d28\u5fc3\u7684\u6240\u6709\u6837\u672c\u7684\u5e73\u5747\u503c\u6765\u521b\u5efa\u65b0\u8d28\u5fc3\u3002</li> <li>\u7136\u540e\uff0c\u5b83\u8ba1\u7b97\u65b0\u65e7\u8d28\u5fc3\u4e4b\u95f4\u7684\u5dee\u5f02\u5e76\u91cd\u590d\u76f4\u5230\u8d28\u5fc3\u7a33\u5b9a\u3002</li> </ol> <p>\u4f7f\u7528 K-Means \u7684\u4e00\u4e2a\u7f3a\u70b9\u5305\u62ec\u60a8\u9700\u8981\u5efa\u7acb\u201ck\u201d\uff0c\u5373\u8d28\u5fc3\u7684\u6570\u91cf\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u201c\u8098\u90e8\u6cd5\u5219\u201d\u6709\u52a9\u4e8e\u4f30\u8ba1\u201ck\u201d\u7684\u826f\u597d\u8d77\u59cb\u503c\u3002\u8bd5\u4e00\u4e0b\u5427\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_3","title":"\u524d\u7f6e\u6761\u4ef6","text":"<p>\u60a8\u5c06\u4f7f\u7528\u672c\u8bfe\u7684 notebook.ipynb \u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u60a8\u5728\u4e0a\u4e00\u8bfe\u4e2d\u6240\u505a\u7684\u6570\u636e\u5bfc\u5165\u548c\u521d\u6b65\u6e05\u7406\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u51c6\u5907","text":"<p>\u9996\u5148\u518d\u770b\u770b\u6b4c\u66f2\u6570\u636e\u3002</p> <ol> <li> <p>\u521b\u5efa\u4e00\u4e2a\u7bb1\u7ebf\u56fe\uff0c<code>boxplot()</code> \u4e3a\u6bcf\u4e00\u5217\u8c03\u7528\uff1a</p> <pre><code>plt.figure(figsize=(20,20), dpi=200)\n\nplt.subplot(4,3,1)\nsns.boxplot(x = 'popularity', data = df)\n\nplt.subplot(4,3,2)\nsns.boxplot(x = 'acousticness', data = df)\n\nplt.subplot(4,3,3)\nsns.boxplot(x = 'energy', data = df)\n\nplt.subplot(4,3,4)\nsns.boxplot(x = 'instrumentalness', data = df)\n\nplt.subplot(4,3,5)\nsns.boxplot(x = 'liveness', data = df)\n\nplt.subplot(4,3,6)\nsns.boxplot(x = 'loudness', data = df)\n\nplt.subplot(4,3,7)\nsns.boxplot(x = 'speechiness', data = df)\n\nplt.subplot(4,3,8)\nsns.boxplot(x = 'tempo', data = df)\n\nplt.subplot(4,3,9)\nsns.boxplot(x = 'time_signature', data = df)\n\nplt.subplot(4,3,10)\nsns.boxplot(x = 'danceability', data = df)\n\nplt.subplot(4,3,11)\nsns.boxplot(x = 'length', data = df)\n\nplt.subplot(4,3,12)\nsns.boxplot(x = 'release_date', data = df)\n</code></pre> <p>\u8fd9\u4e2a\u6570\u636e\u6709\u70b9\u5608\u6742\uff1a\u901a\u8fc7\u89c2\u5bdf\u6bcf\u4e00\u5217\u4f5c\u4e3a\u7bb1\u7ebf\u56fe\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u5f02\u5e38\u503c\u3002</p> <p></p> </li> </ol> <p>\u60a8\u53ef\u4ee5\u6d4f\u89c8\u6570\u636e\u96c6\u5e76\u5220\u9664\u8fd9\u4e9b\u5f02\u5e38\u503c\uff0c\u4f46\u8fd9\u4f1a\u4f7f\u6570\u636e\u975e\u5e38\u5c11\u3002</p> <ol> <li> <p>\u73b0\u5728\uff0c\u9009\u62e9\u60a8\u5c06\u7528\u4e8e\u805a\u7c7b\u7ec3\u4e60\u7684\u5217\u3002\u9009\u62e9\u5177\u6709\u76f8\u4f3c\u8303\u56f4\u7684\u90a3\u4e9b\u5e76\u5c06 <code>artist_top_genre</code> \u5217\u7f16\u7801\u4e3a\u6570\u5b57\u7c7b\u578b\u7684\u6570\u636e\uff1a</p> <pre><code>from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nX = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]\n\ny = df['artist_top_genre']\n\nX['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])\n\ny = le.transform(y)\n</code></pre> </li> <li> <p>\u73b0\u5728\u60a8\u9700\u8981\u9009\u62e9\u8981\u5b9a\u4f4d\u7684\u805a\u7c7b\u6570\u91cf\u3002\u60a8\u77e5\u9053\u6211\u4eec\u4ece\u6570\u636e\u96c6\u4e2d\u6316\u6398\u51fa 3 \u79cd\u6b4c\u66f2\u6d41\u6d3e\uff0c\u6240\u4ee5\u8ba9\u6211\u4eec\u5c1d\u8bd5 3 \u79cd\uff1a</p> <pre><code>from sklearn.cluster import KMeans\n\nnclusters = 3 \nseed = 0\n\nkm = KMeans(n_clusters=nclusters, random_state=seed)\nkm.fit(X)\n\n# Predict the cluster for each data point\n\ny_cluster_kmeans = km.predict(X)\ny_cluster_kmeans\n</code></pre> </li> </ol> <p>\u60a8\u4f1a\u770b\u5230\u6253\u5370\u51fa\u7684\u6570\u7ec4\uff0c\u5176\u4e2d\u5305\u542b\u6570\u636e\u5e27\u6bcf\u4e00\u884c\u7684\u9884\u6d4b\u805a\u7c7b\uff080\u30011 \u6216 2\uff09\u3002</p> <ol> <li> <p>\u4f7f\u7528\u6b64\u6570\u7ec4\u8ba1\u7b97\u201c\u8f6e\u5ed3\u5206\u6570\u201d\uff1a</p> <pre><code>from sklearn import metrics\nscore = metrics.silhouette_score(X, y_cluster_kmeans)\nscore\n</code></pre> </li> </ol>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_4","title":"\u8f6e\u5ed3\u5206\u6570","text":"<p>\u5bfb\u627e\u63a5\u8fd1 1 \u7684\u8f6e\u5ed3\u5206\u6570\u3002\u8be5\u5206\u6570\u4ece -1 \u5230 1 \u4e0d\u7b49\uff0c\u5982\u679c\u5206\u6570\u4e3a 1\uff0c\u5219\u8be5\u805a\u7c7b\u5bc6\u96c6\u4e14\u4e0e\u5176\u4ed6\u805a\u7c7b\u5206\u79bb\u826f\u597d\u3002\u63a5\u8fd1 0 \u7684\u503c\u8868\u793a\u91cd\u53e0\u805a\u7c7b\uff0c\u6837\u672c\u975e\u5e38\u63a5\u8fd1\u76f8\u90bb\u805a\u7c7b\u7684\u51b3\u7b56\u8fb9\u754c\u3002\u6765\u6e90\u3002</p> <p>\u6211\u4eec\u7684\u5206\u6570\u662f 0.53\uff0c\u6240\u4ee5\u6b63\u597d\u5728\u4e2d\u95f4\u3002\u8fd9\u8868\u660e\u6211\u4eec\u7684\u6570\u636e\u4e0d\u662f\u7279\u522b\u9002\u5408\u8fd9\u79cd\u7c7b\u578b\u7684\u805a\u7c7b\uff0c\u4f46\u8ba9\u6211\u4eec\u7ee7\u7eed\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u5efa\u7acb\u6a21\u578b","text":"<ol> <li> <p>\u5bfc\u5165 <code>KMeans</code> \u5e76\u542f\u52a8\u805a\u7c7b\u8fc7\u7a0b\u3002</p> <pre><code>from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n</code></pre> <p>\u8fd9\u91cc\u6709\u51e0\u4e2a\u90e8\u5206\u9700\u8981\u89e3\u91ca\u3002</p> <p>\ud83c\udf93 range\uff1a\u8fd9\u4e9b\u662f\u805a\u7c7b\u8fc7\u7a0b\u7684\u8fed\u4ee3</p> <p>\ud83c\udf93 random_state\uff1a\u201c\u786e\u5b9a\u8d28\u5fc3\u521d\u59cb\u5316\u7684\u968f\u673a\u6570\u751f\u6210\u3002\u201d \u6765\u6e90</p> <p>\ud83c\udf93 WCSS\uff1a\u201c\u805a\u7c7b\u5185\u5e73\u65b9\u548c\u201d\u6d4b\u91cf\u805a\u7c7b\u5185\u6240\u6709\u70b9\u5230\u805a\u7c7b\u8d28\u5fc3\u7684\u5e73\u65b9\u5e73\u5747\u8ddd\u79bb\u3002\u6765\u6e90\u3002</p> <p>\ud83c\udf93 Inertia\uff1aK-Means \u7b97\u6cd5\u5c1d\u8bd5\u9009\u62e9\u8d28\u5fc3\u4ee5\u6700\u5c0f\u5316\u201c\u60ef\u6027\u201d\uff0c\u201c\u60ef\u6027\u662f\u8861\u91cf\u5185\u90e8\u76f8\u5e72\u7a0b\u5ea6\u7684\u4e00\u79cd\u65b9\u6cd5\u201d\u3002\u6765\u6e90\u3002\u8be5\u503c\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u9644\u52a0\u5230 wcss \u53d8\u91cf\u3002</p> <p>\ud83c\udf93 k-means++\uff1a\u5728 Scikit-learn \u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u201ck-means++\u201d\u4f18\u5316\uff0c\u5b83\u201c\u5c06\u8d28\u5fc3\u521d\u59cb\u5316\u4e3a\uff08\u901a\u5e38\uff09\u5f7c\u6b64\u8fdc\u79bb\uff0c\u5bfc\u81f4\u53ef\u80fd\u6bd4\u968f\u673a\u521d\u59cb\u5316\u66f4\u597d\u7684\u7ed3\u679c\u3002</p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_5","title":"\u624b\u8098\u65b9\u6cd5","text":"<p>\u4e4b\u524d\uff0c\u60a8\u63a8\u6d4b\uff0c\u56e0\u4e3a\u60a8\u5df2\u7ecf\u5b9a\u4f4d\u4e86 3 \u4e2a\u6b4c\u66f2 genre\uff0c\u6240\u4ee5\u60a8\u5e94\u8be5\u9009\u62e9 3 \u4e2a\u805a\u7c7b\u3002\u4f46\u771f\u7684\u662f\u8fd9\u6837\u5417\uff1f</p> <ol> <li> <p>\u4f7f\u7528\u624b\u8098\u65b9\u6cd5\u6765\u786e\u8ba4\u3002</p> <pre><code>plt.figure(figsize=(10,5))\nsns.lineplot(range(1, 11), wcss,marker='o',color='red')\nplt.title('Elbow')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n</code></pre> <p>\u4f7f\u7528 <code>wcss</code> \u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u6784\u5efa\u7684\u53d8\u91cf\u521b\u5efa\u4e00\u4e2a\u56fe\u8868\uff0c\u663e\u793a\u8098\u90e8\u201c\u5f2f\u66f2\u201d\u7684\u4f4d\u7f6e\uff0c\u8fd9\u8868\u793a\u6700\u4f73\u805a\u7c7b\u6570\u3002\u4e5f\u8bb8\u662f 3\uff01</p> <p></p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#-_2","title":"\u7ec3\u4e60 - \u663e\u793a\u805a\u7c7b","text":"<ol> <li> <p>\u518d\u6b21\u5c1d\u8bd5\u8be5\u8fc7\u7a0b\uff0c\u8fd9\u6b21\u8bbe\u7f6e\u4e09\u4e2a\u805a\u7c7b\uff0c\u5e76\u5c06\u805a\u7c7b\u663e\u793a\u4e3a\u6563\u70b9\u56fe\uff1a</p> <pre><code>from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 3)\nkmeans.fit(X)\nlabels = kmeans.predict(X)\nplt.scatter(df['popularity'],df['danceability'],c = labels)\nplt.xlabel('popularity')\nplt.ylabel('danceability')\nplt.show()\n</code></pre> </li> <li> <p>\u68c0\u67e5\u6a21\u578b\u7684\u51c6\u786e\u6027\uff1a</p> <pre><code>labels = kmeans.labels_\n\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n\nprint('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))\n</code></pre> <p>\u8fd9\u4e2a\u6a21\u578b\u7684\u51c6\u786e\u6027\u4e0d\u662f\u5f88\u597d\uff0c\u805a\u7c7b\u7684\u5f62\u72b6\u7ed9\u4e86\u4f60\u4e00\u4e2a\u63d0\u793a\u3002</p> <p></p> <p>\u8fd9\u4e9b\u6570\u636e\u592a\u4e0d\u5e73\u8861\uff0c\u76f8\u5173\u6027\u592a\u4f4e\uff0c\u5217\u503c\u4e4b\u95f4\u7684\u5dee\u5f02\u592a\u5927\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u805a\u7c7b\u3002\u4e8b\u5b9e\u4e0a\uff0c\u5f62\u6210\u7684\u805a\u7c7b\u53ef\u80fd\u53d7\u5230\u6211\u4eec\u4e0a\u9762\u5b9a\u4e49\u7684\u4e09\u4e2a\u7c7b\u578b\u7c7b\u522b\u7684\u4e25\u91cd\u5f71\u54cd\u6216\u626d\u66f2\u3002\u90a3\u662f\u4e00\u4e2a\u5b66\u4e60\u7684\u8fc7\u7a0b\uff01</p> <p>\u5728 Scikit-learn \u7684\u6587\u6863\u4e2d\uff0c\u4f60\u53ef\u4ee5\u770b\u5230\u50cf\u8fd9\u6837\u7684\u6a21\u578b\uff0c\u805a\u7c7b\u5212\u5206\u4e0d\u662f\u5f88\u597d\uff0c\u6709\u4e00\u4e2a\u201c\u65b9\u5dee\u201d\u95ee\u9898\uff1a</p> <p></p> <p>\u56fe\u6765\u81ea Scikit-learn</p> </li> </ol>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_6","title":"\u65b9\u5dee","text":"<p>\u65b9\u5dee\u88ab\u5b9a\u4e49\u4e3a\u201c\u6765\u81ea\u5747\u503c\u7684\u5e73\u65b9\u5dee\u7684\u5e73\u5747\u503c\u201d\u6e90\u3002\u5728\u8fd9\u4e2a\u805a\u7c7b\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5b83\u6307\u7684\u662f\u6211\u4eec\u6570\u636e\u96c6\u7684\u6570\u91cf\u5f80\u5f80\u4e0e\u5e73\u5747\u503c\u76f8\u5dee\u592a\u591a\u7684\u6570\u636e\u3002</p> <p>\u2705\u8fd9\u662f\u8003\u8651\u53ef\u4ee5\u7ea0\u6b63\u6b64\u95ee\u9898\u7684\u6240\u6709\u65b9\u6cd5\u7684\u597d\u65f6\u673a\u3002\u7a0d\u5fae\u8c03\u6574\u4e00\u4e0b\u6570\u636e\uff1f\u4f7f\u7528\u4e0d\u540c\u7684\u5217\uff1f\u4f7f\u7528\u4e0d\u540c\u7684\u7b97\u6cd5\uff1f\u63d0\u793a\uff1a\u5c1d\u8bd5\u7f29\u653e\u6570\u636e\u4ee5\u5bf9\u5176\u8fdb\u884c\u6807\u51c6\u5316\u5e76\u6d4b\u8bd5\u5176\u4ed6\u5217\u3002</p> <p>\u8bd5\u8bd5\u8fd9\u4e2a\u201c\u65b9\u5dee\u8ba1\u7b97\u5668\u201d\u6765\u66f4\u591a\u5730\u7406\u89e3\u8fd9\u4e2a\u6982\u5ff5\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_7","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u82b1\u4e00\u4e9b\u65f6\u95f4\u5728\u8fd9\u4e2a\u7b14\u8bb0\u672c\u4e0a\uff0c\u8c03\u6574\u53c2\u6570\u3002\u60a8\u80fd\u5426\u901a\u8fc7\u66f4\u591a\u5730\u6e05\u7406\u6570\u636e\uff08\u4f8b\u5982\uff0c\u53bb\u9664\u5f02\u5e38\u503c\uff09\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\uff1f\u60a8\u53ef\u4ee5\u4f7f\u7528\u6743\u91cd\u4e3a\u7ed9\u5b9a\u7684\u6570\u636e\u6837\u672c\u8d4b\u4e88\u66f4\u591a\u6743\u91cd\u3002\u4f60\u8fd8\u80fd\u505a\u4e9b\u4ec0\u4e48\u6765\u521b\u5efa\u66f4\u597d\u7684\u805a\u7c7b\uff1f</p> <p>\u63d0\u793a\uff1a\u5c1d\u8bd5\u7f29\u653e\u60a8\u7684\u6570\u636e\u3002\u7b14\u8bb0\u672c\u4e2d\u7684\u6ce8\u91ca\u4ee3\u7801\u6dfb\u52a0\u4e86\u6807\u51c6\u7f29\u653e\uff0c\u4f7f\u6570\u636e\u5217\u5728\u8303\u56f4\u65b9\u9762\u66f4\u52a0\u76f8\u4f3c\u3002\u60a8\u4f1a\u53d1\u73b0\uff0c\u5f53\u8f6e\u5ed3\u5206\u6570\u4e0b\u964d\u65f6\uff0c\u8098\u90e8\u56fe\u4e2d\u7684\u201c\u626d\u7ed3\u201d\u53d8\u5f97\u5e73\u6ed1\u3002\u8fd9\u662f\u56e0\u4e3a\u4e0d\u7f29\u653e\u6570\u636e\u53ef\u4ee5\u8ba9\u65b9\u5dee\u8f83\u5c0f\u7684\u6570\u636e\u627f\u8f7d\u66f4\u591a\u7684\u6743\u91cd\u3002\u5728\u8fd9\u91cc\u9605\u8bfb\u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u7684\u4fe1\u606f\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_8","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_9","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u770b\u770b\u50cf\u8fd9\u6837\u7684 K-Means \u6a21\u62df\u5668\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u6b64\u5de5\u5177\u6765\u53ef\u89c6\u5316\u6837\u672c\u6570\u636e\u70b9\u5e76\u786e\u5b9a\u5176\u8d28\u5fc3\u3002\u60a8\u53ef\u4ee5\u7f16\u8f91\u6570\u636e\u7684\u968f\u673a\u6027\u3001\u805a\u7c7b\u6570\u548c\u8d28\u5fc3\u6570\u3002\u8fd9\u662f\u5426\u6709\u52a9\u4e8e\u60a8\u4e86\u89e3\u5982\u4f55\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7ec4\uff1f</p> <p>\u53e6\u5916\uff0c\u770b\u770b\u65af\u5766\u798f\u5927\u5b66\u7684 K-Means \u8bb2\u4e49\u3002</p>"},{"location":"5-Clustering/2-K-Means/README.zh-cn/#_10","title":"\u4f5c\u4e1a","text":"<p>\u5c1d\u8bd5\u4e0d\u540c\u7684\u805a\u7c7b\u65b9\u6cd5</p>"},{"location":"5-Clustering/2-K-Means/assignment/","title":"Try different clustering methods","text":""},{"location":"5-Clustering/2-K-Means/assignment/#instructions","title":"Instructions","text":"<p>In this lesson you learned about K-Means clustering. Sometimes K-Means is not appropriate for your data. Create a notebook using data either from these lessons or from somewhere else (credit your source) and show a different clustering method NOT using K-Means. What did you learn? </p>"},{"location":"5-Clustering/2-K-Means/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with a well-documented clustering model A notebook is presented without good documentation and/or incomplete Incomplete work is submitted"},{"location":"5-Clustering/2-K-Means/assignment.zh-cn/","title":"\u5c1d\u8bd5\u4e0d\u540c\u7684\u805a\u7c7b\u65b9\u6cd5","text":""},{"location":"5-Clustering/2-K-Means/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u60a8\u5b66\u4e60\u4e86 K-Means \u805a\u7c7b\u3002\u6709\u65f6 K-Means \u4e0d\u9002\u5408\u60a8\u7684\u6570\u636e\u3002\u4f7f\u7528\u6765\u81ea\u8fd9\u4e9b\u8bfe\u7a0b\u6216\u5176\u4ed6\u5730\u65b9\u7684\u6570\u636e\uff08\u5f52\u529f\u4e8e\u60a8\u7684\u6765\u6e90\uff09\u521b\u5efanotebook\uff0c\u5e76\u5c55\u793a\u4e0d\u4f7f\u7528 K-Means \u7684\u4e0d\u540c\u805a\u7c7b\u65b9\u6cd5\u3002\u4f60\u5b66\u5230\u4e86\u4ec0\u4e48\uff1f</p>"},{"location":"5-Clustering/2-K-Means/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u89c4\u5219","text":"\u8bc4\u5224\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u4e00\u4e2a\u5177\u6709\u826f\u597d\u6587\u6863\u8bb0\u5f55\u7684\u805a\u7c7b\u6a21\u578b\u7684notebook \u4e00\u4e2a\u6ca1\u6709\u8be6\u7ec6\u6587\u6863\u6216\u4e0d\u5b8c\u6574\u7684notebook \u63d0\u4ea4\u4e86\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u5de5\u4f5c"},{"location":"5-Clustering/2-K-Means/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"6-NLP/","title":"Getting started with natural language processing","text":"<p>Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. It is a component of artificial intelligence (AI). NLP has existed for more than 50 years and has roots in the field of linguistics. The whole field is directed at helping machines understand and process the human language. This can then be used to perform tasks like spell check or machine translation. It has a variety of real-world applications in a number of fields, including medical research, search engines and business intelligence.</p>"},{"location":"6-NLP/#regional-topic-european-languages-and-literature-and-romantic-hotels-of-europe","title":"Regional topic: European languages and literature and romantic hotels of Europe \u2764\ufe0f","text":"<p>In this section of the curriculum, you will be introduced to one of the most widespread uses of machine learning: natural language processing (NLP). Derived from computational linguistics, this category of artificial intelligence is the bridge between humans and machines via voice or textual communication.</p> <p>In these lessons we'll learn the basics of NLP by building small conversational bots to learn how machine learning aids in making these conversations more and more 'smart'. You'll travel back in time, chatting with Elizabeth Bennett and Mr. Darcy from Jane Austen's classic novel, Pride and Prejudice, published in 1813. Then, you'll further your knowledge by learning about sentiment analysis via hotel reviews in Europe.</p> <p></p> <p>Photo by Elaine Howlin on Unsplash</p>"},{"location":"6-NLP/#lessons","title":"Lessons","text":"<ol> <li>Introduction to natural language processing</li> <li>Common NLP tasks and techniques</li> <li>Translation and sentiment analysis with machine learning</li> <li>Preparing your data</li> <li>NLTK for Sentiment Analysis</li> </ol>"},{"location":"6-NLP/#credits","title":"Credits","text":"<p>These natural language processing lessons were written with \u2615 by Stephen Howell</p>"},{"location":"6-NLP/README.zh-cn/","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8","text":"<p>\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u662f\u4eba\u5de5\u667a\u80fd\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u4e3b\u8981\u7814\u7a76\u5982\u4f55\u8ba9\u673a\u5668\u7406\u89e3\u548c\u5904\u7406\u4eba\u7c7b\u8bed\u8a00\uff0c\u5e76\u7528\u5b83\u6765\u6267\u884c\u62fc\u5199\u68c0\u67e5\u6216\u673a\u5668\u7ffb\u8bd1\u7b49\u4efb\u52a1\u3002</p>"},{"location":"6-NLP/README.zh-cn/#_2","title":"\u672c\u8282\u4e3b\u9898\uff1a\u6b27\u6d32\u8bed\u8a00\u6587\u5b66\u548c\u6b27\u6d32\u6d6a\u6f2b\u9152\u5e97 \u2764\ufe0f","text":"<p>\u5728\u8fd9\u90e8\u5206\u8bfe\u7a0b\u4e2d\uff0c\u60a8\u5c06\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u6700\u5e7f\u6cdb\u7684\u7528\u9014\u4e4b\u4e00\uff1a\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP)\u3002\u6e90\u81ea\u8ba1\u7b97\u8bed\u8a00\u5b66\uff0c\u8fd9\u4e00\u7c7b\u4eba\u5de5\u667a\u80fd\u4f1a\u901a\u8fc7\u8bed\u97f3\u6216\u6587\u672c\u4e0e\u4eba\u7c7b\u4ea4\u6d41\uff0c\u5efa\u7acb\u8fde\u63a5\u4eba\u4e0e\u673a\u5668\u7684\u6865\u6881\u3002</p> <p>\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u901a\u8fc7\u6784\u5efa\u5c0f\u578b\u5bf9\u8bdd\u673a\u5668\u4eba\u6765\u5b66\u4e60 NLP \u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u4ee5\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u662f\u5982\u4f55\u4f7f\u8fd9\u4e2a\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u201c\u667a\u80fd\u201d\u3002\u60a8\u5c06\u7a7f\u8d8a\u56de 1813 \u5e74\uff0c\u4e0e\u7b80\u00b7\u5965\u65af\u6c40\u7684\u7ecf\u5178\u5c0f\u8bf4 \u50b2\u6162\u4e0e\u504f\u89c1 \u4e2d\u7684 Elizabeth Bennett \u548c Mr. Darcy \u804a\u5929\uff08\u8be5\u5c0f\u8bf4\u4e8e 1813 \u5e74\u51fa\u7248\uff09\u3002\u7136\u540e\uff0c\u60a8\u5c06\u901a\u8fc7\u6b27\u6d32\u7684\u9152\u5e97\u8bc4\u8bba\u6765\u8fdb\u4e00\u6b65\u5b66\u4e60\u60c5\u611f\u5206\u6790\u3002</p> <p></p> <p>\u7531 Elaine Howlin \u62cd\u6444\uff0c \u6765\u81ea Unsplash</p>"},{"location":"6-NLP/README.zh-cn/#_3","title":"\u8bfe\u7a0b","text":"<ol> <li>\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b80\u4ecb</li> <li>NLP \u5e38\u89c1\u4efb\u52a1\u4e0e\u6280\u5de7</li> <li>\u673a\u5668\u5b66\u4e60\u7ffb\u8bd1\u548c\u60c5\u611f\u5206\u6790</li> <li>\u51c6\u5907\u6570\u636e</li> <li>\u7528\u4e8e\u60c5\u611f\u5206\u6790\u7684\u5de5\u5177\uff1aNLTK</li> </ol>"},{"location":"6-NLP/README.zh-cn/#_4","title":"\u4f5c\u8005","text":"<p>\u8fd9\u4e9b\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u7531 Stephen Howell \u7528 \u2615 \u7f16\u5199</p>"},{"location":"6-NLP/1-Introduction-to-NLP/","title":"Introduction to natural language processing","text":"<p>This lesson covers a brief history and important concepts of natural language processing, a subfield of computational linguistics.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"6-NLP/1-Introduction-to-NLP/#introduction","title":"Introduction","text":"<p>NLP, as it is commonly known, is one of the best-known areas where machine learning has been applied and used in production software.</p> <p>\u2705 Can you think of software that you use every day that probably has some NLP embedded? What about your word processing programs or mobile apps that you use regularly?</p> <p>You will learn about:</p> <ul> <li>The idea of languages. How languages developed and what the major areas of study have been.</li> <li>Definition and concepts. You will also learn definitions and concepts about how computers process text, including parsing, grammar, and identifying nouns and verbs. There are some coding tasks in this lesson, and several important concepts are introduced that you will learn to code later on in the next lessons.</li> </ul>"},{"location":"6-NLP/1-Introduction-to-NLP/#computational-linguistics","title":"Computational linguistics","text":"<p>Computational linguistics is an area of research and development over many decades that studies how computers can work with, and even understand, translate, and communicate with languages. Natural language processing (NLP) is a related field focused on how computers can process 'natural', or human, languages.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#example-phone-dictation","title":"Example - phone dictation","text":"<p>If you have ever dictated to your phone instead of typing or asked a virtual assistant a question, your speech was converted into a text form and then processed or parsed from the language you spoke. The detected keywords were then processed into a format that the phone or assistant could understand and act on.</p> <p></p> <p>Real linguistic comprehension is hard! Image by Jen Looper</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#how-is-this-technology-made-possible","title":"How is this technology made possible?","text":"<p>This is possible because someone wrote a computer program to do this. A few decades ago, some science fiction writers predicted that people would mostly speak to their computers, and the computers would always understand exactly what they meant. Sadly, it turned out to be a harder problem that many imagined, and while it is a much better understood problem today, there are significant challenges in achieving 'perfect' natural language processing when it comes to understanding the meaning of a sentence. This is a particularly hard problem when it comes to understanding humour or detecting emotions such as sarcasm in a sentence.</p> <p>At this point, you may be remembering school classes where the teacher covered the parts of grammar in a sentence. In some countries, students are taught grammar and linguistics as a dedicated subject, but in many, these topics are included as part of learning a language: either your first language in primary school (learning to read and write) and perhaps a second language in post-primary, or high school. Don't  worry if you are not an expert at differentiating nouns from verbs or adverbs from adjectives!</p> <p>If you struggle with the difference between the simple present and present progressive, you are not alone. This is a challenging thing for many people, even native speakers of a language. The good news is that computers are really good at applying formal rules, and you will learn to write code that can parse a sentence as well as a human. The greater challenge you will examine later is understanding the meaning, and sentiment, of a sentence.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#prerequisites","title":"Prerequisites","text":"<p>For this lesson, the main prerequisite is being able to read and understand the language of this lesson. There are no math problems or equations to solve. While the original author wrote this lesson in English, it is also translated into other languages, so you could be reading a translation. There are examples where a number of different languages are used (to compare the different grammar rules of different languages). These are not translated, but the explanatory text is, so the meaning should be clear.</p> <p>For the coding tasks, you will use Python and the examples are using Python 3.8.</p> <p>In this section, you will need, and use:</p> <ul> <li>Python 3 comprehension.  Programming language comprehension in Python 3, this lesson uses input, loops, file reading, arrays.</li> <li>Visual Studio Code + extension. We will use Visual Studio Code and its Python extension. You can also use a Python IDE of your choice.</li> <li>TextBlob. TextBlob is a simplified text processing library for Python. Follow the instructions on the TextBlob site to install it on your system (install the corpora as well, as shown below):</li> </ul> <pre><code>pip install -U textblob\npython -m textblob.download_corpora\n</code></pre> <p>\ud83d\udca1 Tip: You can run Python directly in VS Code environments. Check the docs for more information.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#talking-to-machines","title":"Talking to machines","text":"<p>The history of trying to make computers understand human language goes back decades, and one of the earliest scientists to consider natural language processing was Alan Turing.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#the-turing-test","title":"The 'Turing test'","text":"<p>When Turing was researching artificial intelligence in the 1950's, he considered if a conversational test could be given to a human and computer (via typed correspondence) where the human in the conversation was not sure if they were conversing with another human or a computer.</p> <p>If, after a certain length of conversation, the human could not determine that the answers were from a computer or not, then could the computer be said to be thinking?</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#the-inspiration-the-imitation-game","title":"The inspiration - 'the imitation game'","text":"<p>The idea for this came from a party game called The Imitation Game where an interrogator is alone in a room and tasked with determining which of two people (in another room) are male and female respectively. The interrogator can send notes, and must try to think of questions where the written answers reveal the gender of the mystery person. Of course, the players in the other room are trying to trick the interrogator by answering questions in such as way as to mislead or confuse the interrogator, whilst also giving the appearance of answering honestly.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#developing-eliza","title":"Developing Eliza","text":"<p>In the 1960's an MIT scientist called Joseph Weizenbaum developed Eliza, a computer 'therapist' that would ask the human questions and give the appearance of understanding their answers. However, while Eliza could parse a sentence and identify certain grammatical constructs and keywords so as to give a reasonable answer, it could not be said to understand the sentence. If Eliza was presented with a sentence following the format \"I am sad\" it might rearrange and substitute words in the sentence to form the response \"How long have you been sad\". </p> <p>This gave the impression that Eliza understood the statement and was asking a follow-on question, whereas in reality, it was changing the tense and adding some words. If Eliza could not identify a keyword that it had a response for, it would instead give a random response that could be applicable to many different statements. Eliza could be easily tricked, for instance if a user wrote \"You are a bicycle\" it might respond with \"How long have I been a bicycle?\", instead of a more reasoned response.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video about original ELIZA program</p> <p>Note: You can read the original description of Eliza published in 1966 if you have an ACM account. Alternately, read about Eliza on wikipedia</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#exercise-coding-a-basic-conversational-bot","title":"Exercise - coding a basic conversational bot","text":"<p>A conversational bot, like Eliza, is a program that elicits user input and seems to understand and respond intelligently. Unlike Eliza, our bot will not have several rules giving it the appearance of having an intelligent conversation. Instead, our bot will have one ability only, to keep the conversation going with random responses that might work in almost any trivial conversation.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#the-plan","title":"The plan","text":"<p>Your steps when building a conversational bot:</p> <ol> <li>Print instructions advising the user how to interact with the bot</li> <li>Start a loop</li> <li>Accept user input</li> <li>If user has asked to exit, then exit</li> <li>Process user input and determine response (in this case, the response is a random choice from a list of possible generic responses)</li> <li>Print response</li> <li>loop back to step 2</li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/#building-the-bot","title":"Building the bot","text":"<p>Let's create the bot next. We'll start by defining some phrases.</p> <ol> <li> <p>Create this bot yourself in Python with the following random responses:</p> <pre><code>random_responses = [\"That is quite interesting, please tell me more.\",\n                    \"I see. Do go on.\",\n                    \"Why do you say that?\",\n                    \"Funny weather we've been having, isn't it?\",\n                    \"Let's change the subject.\",\n                    \"Did you catch the game last night?\"]\n</code></pre> <p>Here is some sample output to guide you (user input is on the lines starting with <code>&gt;</code>):</p> <pre><code>Hello, I am Marvin, the simple robot.\nYou can end this conversation at any time by typing 'bye'\nAfter typing each answer, press 'enter'\nHow are you today?\n&gt; I am good thanks\nThat is quite interesting, please tell me more.\n&gt; today I went for a walk     \nDid you catch the game last night?\n&gt; I did, but my team lost\nFunny weather we've been having, isn't it?\n&gt; yes but I hope next week is better\nLet's change the subject.\n&gt; ok, lets talk about music\nWhy do you say that?\n&gt; because I like music!\nWhy do you say that?\n&gt; bye\nIt was nice talking to you, goodbye!\n</code></pre> <p>One possible solution to the task is here</p> <p>\u2705 Stop and consider</p> <ol> <li>Do you think the random responses would 'trick' someone into thinking that the bot actually understood them?</li> <li>What features would the bot need to be more effective?</li> <li>If a bot could really 'understand' the meaning of a sentence, would it need to 'remember' the meaning of previous sentences in a conversation too?</li> </ol> </li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Choose one of the \"stop and consider\" elements above and either try to implement them in code or write a solution on paper using pseudocode.</p> <p>In the next lesson, you'll learn about a number of other approaches to parsing natural language and machine learning.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"6-NLP/1-Introduction-to-NLP/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take a look at the references below as further reading opportunities.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/#references","title":"References","text":"<ol> <li>Schubert, Lenhart, \"Computational Linguistics\", The Stanford Encyclopedia of Philosophy (Spring 2020 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/spr2020/entries/computational-linguistics/.</li> <li>Princeton University \"About WordNet.\" WordNet. Princeton University. 2010. </li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/#assignment","title":"Assignment","text":"<p>Search for a bot</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4ecb\u7ecd","text":"<p>\u8fd9\u8282\u8bfe\u8bb2\u89e3\u4e86 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u7684\u7b80\u8981\u5386\u53f2\u548c\u91cd\u8981\u6982\u5ff5\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u8ba1\u7b97\u8bed\u8a00\u5b66\u7684\u4e00\u4e2a\u5b50\u9886\u57df\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_2","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u4f17\u6240\u5468\u77e5\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08Natural Language Processing, NLP\uff09\u662f\u673a\u5668\u5b66\u4e60\u5728\u751f\u4ea7\u8f6f\u4ef6\u4e2d\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u9886\u57df\u4e4b\u4e00\u3002</p> <p>\u2705 \u4f60\u80fd\u60f3\u5230\u54ea\u4e9b\u4f60\u65e5\u5e38\u751f\u6d3b\u4e2d\u4f7f\u7528\u7684\u8f6f\u4ef6\u53ef\u80fd\u5d4c\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5462\uff1f\u6216\u8005\uff0c\u4f60\u7ecf\u5e38\u4f7f\u7528\u7684\u6587\u5b57\u5904\u7406\u7a0b\u5e8f\u6216\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\u662f\u5426\u5d4c\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5462\uff1f</p> <p>\u4f60\u5c06\u4f1a\u5b66\u4e60\u5230\uff1a</p> <ul> <li>\u4ec0\u4e48\u662f\u300c\u8bed\u8a00\u300d\u3002\u8bed\u8a00\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ee5\u53ca\u76f8\u5173\u7814\u7a76\u7684\u4e3b\u8981\u9886\u57df\u3002</li> <li>\u5b9a\u4e49\u548c\u6982\u5ff5\u3002\u4f60\u8fd8\u5c06\u4e86\u89e3\u5173\u4e8e\u8ba1\u7b97\u673a\u6587\u672c\u5904\u7406\u7684\u6982\u5ff5\u3002\u5305\u62ec\u89e3\u6790\uff08parsing\uff09\u3001\u8bed\u6cd5\uff08grammar\uff09\u4ee5\u53ca\u8bc6\u522b\u540d\u8bcd\u4e0e\u52a8\u8bcd\u3002\u8fd9\u8282\u8bfe\u4e2d\u6709\u4e00\u4e9b\u7f16\u7a0b\u4efb\u52a1\uff1b\u8fd8\u6709\u4e00\u4e9b\u91cd\u8981\u6982\u5ff5\u5c06\u5728\u4ee5\u540e\u7684\u8bfe\u7a0b\u4e2d\u88ab\u5f15\u5165\uff0c\u5c4a\u65f6\u4f60\u4e5f\u4f1a\u7ec3\u4e60\u901a\u8fc7\u7f16\u7a0b\u5b9e\u73b0\u5176\u5b83\u6982\u5ff5\u3002</li> </ul>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_4","title":"\u8ba1\u7b97\u8bed\u8a00\u5b66","text":"<p>\u8ba1\u7b97\u8bed\u8a00\u5b66 (Computational Linguistics) \u662f\u4e00\u4e2a\u7ecf\u8fc7\u51e0\u5341\u5e74\u7814\u7a76\u548c\u53d1\u5c55\u7684\u9886\u57df\uff0c\u5b83\u7814\u7a76\u5982\u4f55\u8ba9\u8ba1\u7b97\u673a\u80fd\u4f7f\u7528\u3001\u7406\u89e3\u3001\u7ffb\u8bd1\u8bed\u8a00\u5e76\u4f7f\u7528\u8bed\u8a00\u4ea4\u6d41\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8ba1\u7b97\u673a\u5982\u4f55\u5904\u7406\u300c\u81ea\u7136\u7684\u300d\uff08\u6216\u8005\u8bf4\uff0c\u4eba\u7c7b\u7684\uff09\u8bed\u8a00\u7684\u76f8\u5173\u9886\u57df\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_5","title":"\u4e3e\u4f8b\uff1a\u7535\u8bdd\u53f7\u7801\u8bc6\u522b","text":"<p>\u5982\u679c\u4f60\u66fe\u7ecf\u5728\u624b\u673a\u4e0a\u4f7f\u7528\u8bed\u97f3\u8f93\u5165\u66ff\u4ee3\u952e\u76d8\u8f93\u5165\uff0c\u6216\u8005\u4f7f\u7528\u8fc7\u865a\u62df\u8bed\u97f3\u52a9\u624b\uff0c\u90a3\u4e48\u4f60\u7684\u8bed\u97f3\u5c06\u88ab\u8f6c\u5f55\uff08\u6216\u8005\u53eb\u89e3\u6790\uff09\u4e3a\u6587\u672c\u5f62\u5f0f\u540e\u8fdb\u884c\u5904\u7406\u3002\u88ab\u68c0\u6d4b\u5230\u7684\u5173\u952e\u5b57\u6700\u540e\u5c06\u88ab\u5904\u7406\u6210\u624b\u673a\u6216\u8bed\u97f3\u52a9\u624b\u53ef\u4ee5\u7406\u89e3\u5e76\u53ef\u4ee5\u4f9d\u6b64\u505a\u51fa\u884c\u4e3a\u7684\u683c\u5f0f\u3002</p> <p></p> <p>\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u8bed\u8a00\u7406\u89e3\u5f88\u96be\uff01\u56fe\u6e90\uff1aJen Looper</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_6","title":"\u8fd9\u9879\u6280\u672f\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff1f","text":"<p>\u6211\u4eec\u4e4b\u6240\u4ee5\u53ef\u80fd\u5b8c\u6210\u8fd9\u6837\u7684\u4efb\u52a1\uff0c\u662f\u56e0\u4e3a\u6709\u4eba\u7f16\u5199\u4e86\u4e00\u4e2a\u8ba1\u7b97\u673a\u7a0b\u5e8f\u6765\u5b9e\u73b0\u5b83\u3002\u51e0\u5341\u5e74\u524d\uff0c\u4e00\u4e9b\u79d1\u5e7b\u4f5c\u5bb6\u9884\u6d4b\uff0c\u5728\u672a\u6765\uff0c\u4eba\u7c7b\u5f88\u5927\u53ef\u80fd\u4f1a\u80fd\u591f\u4ed6\u4eec\u7684\u7535\u8111\u5bf9\u8bdd\uff0c\u800c\u7535\u8111\u603b\u662f\u80fd\u51c6\u786e\u5730\u7406\u89e3\u4eba\u7c7b\u7684\u610f\u601d\u3002\u53ef\u60dc\u7684\u662f\uff0c\u4e8b\u5b9e\u8bc1\u660e\u8fd9\u4e2a\u95ee\u9898\u7684\u89e3\u51b3\u6bd4\u6211\u4eec\u60f3\u8c61\u7684\u66f4\u56f0\u96be\u3002\u867d\u7136\u4eca\u5929\u8fd9\u4e2a\u95ee\u9898\u5df2\u7ecf\u88ab\u521d\u6b65\u89e3\u51b3\uff0c\u4f46\u5728\u7406\u89e3\u53e5\u5b50\u7684\u542b\u4e49\u65f6\uff0c\u8981\u5b9e\u73b0 \u201c\u5b8c\u7f8e\u201d \u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218 \u2014\u2014 \u7406\u89e3\u5e7d\u9ed8\u6216\u662f\u68c0\u6d4b\u611f\u60c5\uff08\u6bd4\u5982\u8bbd\u523a\uff09\u5bf9\u4e8e\u8ba1\u7b97\u673a\u6765\u8bf4\u5c24\u5176\u56f0\u96be\u3002</p> <p>\u73b0\u5728\uff0c\u4f60\u53ef\u80fd\u4f1a\u60f3\u8d77\u8bfe\u5802\u4e0a\u8001\u5e08\u8bb2\u89e3\u7684\u8bed\u6cd5\u3002\u5728\u67d0\u4e9b\u56fd\u5bb6/\u5730\u533a\uff0c\u8bed\u6cd5\u548c\u8bed\u8a00\u5b66\u77e5\u8bc6\u662f\u5b66\u751f\u7684\u4e13\u9898\u8bfe\u5185\u5bb9\u3002\u4f46\u5728\u53e6\u4e00\u4e9b\u56fd\u5bb6/\u5730\u533a\uff0c\u4e0d\u7ba1\u662f\u4ece\u5c0f\u5b66\u4e60\u7684\u7b2c\u4e00\u8bed\u8a00\uff08\u5b66\u4e60\u9605\u8bfb\u548c\u5199\u4f5c\uff09\uff0c\u8fd8\u662f\u4e4b\u540e\u5b66\u4e60\u7684\u7b2c\u4e8c\u8bed\u8a00\u4e2d\uff0c\u8bed\u6cd5\u53ca\u8bed\u8a00\u5b66\u77e5\u8bc6\u90fd\u662f\u4f5c\u4e3a\u8bed\u8a00\u7684\u4e00\u90e8\u5206\u6559\u5b66\u7684\u3002\u6240\u4ee5\uff0c\u5982\u679c\u4f60\u4e0d\u80fd\u5f88\u597d\u5730\u533a\u5206\u540d\u8bcd\u4e0e\u52a8\u8bcd\u6216\u8005\u533a\u5206\u526f\u8bcd\u4e0e\u5f62\u5bb9\u8bcd\uff0c\u8bf7\u4e0d\u8981\u62c5\u5fc3\uff01</p> <p>\u4f60\u8fd8\u4e3a\u96be\u4ee5\u533a\u5206\u4e00\u822c\u73b0\u5728\u65f6\u4e0e\u73b0\u5728\u8fdb\u884c\u65f6\u800c\u70e6\u607c\u5417\uff1f\u6ca1\u5173\u7cfb\u7684\uff0c\u5373\u4f7f\u662f\u5bf9\u4ee5\u8fd9\u95e8\u8bed\u8a00\u4e3a\u6bcd\u8bed\u7684\u4eba\u5728\u5185\u7684\u5927\u591a\u6570\u4eba\u6765\u8bf4\uff0c\u533a\u5206\u5b83\u4eec\u90fd\u5f88\u6709\u6311\u6218\u6027\u3002\u4f46\u662f\uff0c\u8ba1\u7b97\u673a\u975e\u5e38\u5584\u4e8e\u5e94\u7528\u6807\u51c6\u7684\u89c4\u5219\uff0c\u4f60\u5c06\u5b66\u4f1a\u7f16\u5199\u53ef\u4ee5\u50cf\u4eba\u4e00\u6837\u201c\u89e3\u6790\u201d\u53e5\u5b50\u7684\u4ee3\u7801\u3002\u7a0d\u540e\u4f60\u5c06\u9762\u5bf9\u7684\u66f4\u5927\u6311\u6218\u662f\u7406\u89e3\u53e5\u5b50\u7684\u8bed\u4e49\u548c\u60c5\u7eea\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_7","title":"\u524d\u63d0","text":"<p>\u672c\u8282\u6559\u7a0b\u7684\u4e3b\u8981\u5148\u51b3\u6761\u4ef6\u662f\u80fd\u591f\u9605\u8bfb\u548c\u7406\u89e3\u672c\u8282\u6559\u7a0b\u7684\u8bed\u8a00\u3002\u672c\u8282\u4e2d\u6ca1\u6709\u6570\u5b66\u95ee\u9898\u6216\u65b9\u7a0b\u9700\u8981\u89e3\u51b3\u3002\u867d\u7136\u539f\u4f5c\u8005\u7528\u82f1\u6587\u5199\u4e86\u8fd9\u6559\u7a0b\uff0c\u4f46\u5b83\u4e5f\u88ab\u7ffb\u8bd1\u6210\u5176\u4ed6\u8bed\u8a00\uff0c\u6240\u4ee5\u4f60\u53ef\u80fd\u5728\u9605\u8bfb\u7ffb\u8bd1\u5185\u5bb9\u3002\u8fd9\u8282\u8bfe\u7684\u793a\u4f8b\u4e2d\u6d89\u53ca\u5230\u5f88\u591a\u8bed\u8a00\u79cd\u7c7b\uff08\u4ee5\u6bd4\u8f83\u4e0d\u540c\u8bed\u8a00\u7684\u4e0d\u540c\u8bed\u6cd5\u89c4\u5219\uff09\u3002\u8fd9\u4e9b\u662f\u672a\u7ffb\u8bd1\u7684\uff0c\u4f46\u5bf9\u5b83\u4eec\u7684\u89e3\u91ca\u662f\u7ffb\u8bd1\u8fc7\u7684\uff0c\u6240\u4ee5\u4f60\u5e94\u8be5\u80fd\u7406\u89e3\u5b83\u5728\u8bb2\u4ec0\u4e48\u3002</p> <p>\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0c\u4f60\u5c06\u4f1a\u4f7f\u7528 Python \u8bed\u8a00\uff0c\u793a\u4f8b\u4f7f\u7528\u7684\u662f Python 3.8 \u7248\u672c\u3002</p> <p>\u5728\u672c\u8282\u4e2d\u4f60\u5c06\u9700\u8981\u5e76\u4f7f\u7528\u5982\u4e0b\u6280\u80fd\uff1a</p> <ul> <li>Python 3\u3002\u4f60\u9700\u8981\u80fd\u591f\u7406\u89e3\u5e76\u4f7f\u7528 Python 3. \u672c\u8bfe\u5c06\u4f1a\u4f7f\u7528\u8f93\u5165\u3001\u5faa\u73af\u3001\u6587\u4ef6\u8bfb\u53d6\u3001\u6570\u7ec4\u529f\u80fd\u3002</li> <li>Visual Studio Code + \u6269\u5c55\u3002 \u6211\u4eec\u5c06\u4f7f\u7528 Visual Studio Code \u53ca\u5176 Python \u6269\u5c55\u3002\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4f60\u559c\u6b22\u7684 Python IDE\u3002</li> <li>TextBlob\u3002TextBlob \u662f\u4e00\u4e2a\u7cbe\u7b80\u7684 Python \u6587\u672c\u5904\u7406\u5e93\u3002\u8bf7\u6309\u7167 TextBlob \u7f51\u7ad9\u4e0a\u7684\u8bf4\u660e\uff0c\u5728\u60a8\u7684\u7cfb\u7edf\u4e0a\u5b89\u88c5\u5b83\uff08\u4e5f\u9700\u8981\u5b89\u88c5\u8bed\u6599\u5e93\uff0c\u5b89\u88c5\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff09\uff1a</li> <li><pre><code>pip install -U textblob\npython -m textblob.download_corpora\n</code></pre></li> </ul> <p>\ud83d\udca1 \u63d0\u793a\uff1a\u4f60\u53ef\u4ee5\u5728 VS Code \u73af\u5883\u4e2d\u76f4\u63a5\u8fd0\u884c Python\u3002 \u70b9\u51fb \u6587\u6863 \u67e5\u770b\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_8","title":"\u4e0e\u673a\u5668\u5bf9\u8bdd","text":"<p>\u8bd5\u56fe\u8ba9\u8ba1\u7b97\u673a\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u7684\u5c1d\u8bd5\u6700\u65e9\u53ef\u4ee5\u8ffd\u6eaf\u5230\u51e0\u5341\u5e74\u524d\u3002Alan Turing \u662f\u6700\u65e9\u7814\u7a76\u81ea\u7136\u8bed\u8a00\u5904\u7406\u95ee\u9898\u7684\u79d1\u5b66\u5bb6\u4e4b\u4e00\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_9","title":"\u56fe\u7075\u6d4b\u8bd5","text":"<p>\u5f53\u56fe\u7075\u5728 1950 \u5e74\u4ee3\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u65f6\uff0c\u4ed6\u60f3\u51fa\u4e86\u8fd9\u4e2a\u601d\u7ef4\u5b9e\u9a8c\uff1a\u8ba9\u4eba\u7c7b\u548c\u8ba1\u7b97\u673a\u901a\u8fc7\u6253\u5b57\u7684\u65b9\u5f0f\u6765\u4ea4\u8c08\uff0c\u5176\u4e2d\u4eba\u7c7b\u5e76\u4e0d\u77e5\u9053\u5bf9\u65b9\u662f\u4eba\u7c7b\u8fd8\u662f\u8ba1\u7b97\u673a\u3002</p> <p>\u5982\u679c\u7ecf\u8fc7\u4e00\u5b9a\u65f6\u95f4\u7684\u4ea4\u8c08\uff0c\u4eba\u7c7b\u65e0\u6cd5\u786e\u5b9a\u5bf9\u65b9\u662f\u5426\u662f\u8ba1\u7b97\u673a\uff0c\u90a3\u4e48\u662f\u5426\u53ef\u4ee5\u8ba4\u4e3a\u8ba1\u7b97\u673a\u6b63\u5728\u201c\u601d\u8003\u201d\uff1f</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#-","title":"\u7075\u611f - \u201c\u6a21\u4eff\u6e38\u620f\u201d","text":"<p>\u8fd9\u4e2a\u60f3\u6cd5\u6765\u81ea\u4e00\u4e2a\u540d\u4e3a \u6a21\u4eff\u6e38\u620f \u7684\u6d3e\u5bf9\u6e38\u620f\uff0c\u5176\u4e2d\u4e00\u540d\u5ba1\u8baf\u8005\u72ec\u81ea\u4e00\u4eba\u5728\u4e00\u4e2a\u623f\u95f4\u91cc\uff0c\u8d1f\u8d23\u786e\u5b9a\u5728\u53e6\u4e00\u4e2a\u623f\u95f4\u91cc\u7684\u4e24\u4eba\u7684\u6027\u522b\uff08\u7537\u6027\u6216\u5973\u6027\uff09\u3002\u5ba1\u8baf\u8005\u53ef\u4ee5\u4f20\u9012\u7b14\u8bb0\uff0c\u5e76\u4e14\u9700\u8981\u60f3\u51fa\u80fd\u591f\u63ed\u793a\u795e\u79d8\u4eba\u6027\u522b\u7684\u95ee\u9898\u3002\u5f53\u7136\uff0c\u53e6\u4e00\u4e2a\u623f\u95f4\u7684\u73a9\u5bb6\u4e5f\u53ef\u4ee5\u901a\u8fc7\u56de\u7b54\u95ee\u9898\u7684\u65b9\u5f0f\u6765\u6b3a\u9a97\u5ba1\u8baf\u8005\uff0c\u4f8b\u5982\u7528\u770b\u4f3c\u771f\u8bda\u7684\u65b9\u5f0f\u8bef\u5bfc\u6216\u8ff7\u60d1\u5ba1\u8baf\u8005\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#eliza","title":"Eliza \u7684\u7814\u53d1","text":"<p>\u5728 1960 \u5e74\u4ee3\u7684\u9ebb\u7701\u7406\u5de5\u5b66\u9662\uff0c\u4e00\u4f4d\u540d\u53eb Joseph Weizenbaum \u7684\u79d1\u5b66\u5bb6\u5f00\u53d1\u4e86 Eliza\u3002Eliza \u662f\u4e00\u4f4d\u8ba1\u7b97\u673a\u201c\u6cbb\u7597\u5e08\u201d\uff0c\u5b83\u53ef\u4ee5\u5411\u4eba\u7c7b\u63d0\u51fa\u95ee\u9898\u5e76\u8ba9\u4eba\u7c7b\u89c9\u5f97\u5b83\u80fd\u7406\u89e3\u4eba\u7c7b\u7684\u56de\u7b54\u3002\u7136\u800c\uff0c\u867d\u7136 Eliza \u53ef\u4ee5\u89e3\u6790\u53e5\u5b50\u5e76\u8bc6\u522b\u67d0\u4e9b\u8bed\u6cd5\u7ed3\u6784\u548c\u5173\u952e\u5b57\u4ee5\u7ed9\u51fa\u5408\u7406\u7684\u7b54\u6848\uff0c\u4f46\u4e0d\u80fd\u8bf4\u5b83\u7406\u89e3\u4e86\u53e5\u5b50\u3002\u5982\u679c Eliza \u770b\u5230\u7684\u53e5\u5b50\u683c\u5f0f\u4e3a\u201cI am sad\u201d\uff08\u6211\u5f88 \u96be\u8fc7\uff09\uff0c\u5b83\u53ef\u80fd\u4f1a\u91cd\u65b0\u6392\u5217\u5e76\u66ff\u6362\u53e5\u5b50\u4e2d\u7684\u5355\u8bcd\uff0c\u56de\u7b54 \u201cHow long have you been sad\"\uff08\u4f60\u5df2\u7ecf \u96be\u8fc7 \u591a\u4e45\u4e86\uff09\u3002</p> <p>\u770b\u8d77\u6765\u50cf\u662f Eliza \u7406\u89e3\u4e86\u8fd9\u53e5\u8bdd\uff0c\u8fd8\u5728\u8be2\u95ee\u5173\u4e8e\u8fd9\u53e5\u8bdd\u7684\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u4e0a\uff0c\u5b83\u53ea\u662f\u5728\u6539\u53d8\u65f6\u6001\u548c\u6dfb\u52a0\u8bcd\u8bed\u3002\u5982\u679c Eliza \u6ca1\u6709\u5728\u56de\u7b54\u4e2d\u53d1\u73b0\u5b83\u77e5\u9053\u5982\u4f55\u54cd\u5e94\u7684\u8bcd\u6c47\uff0c\u5b83\u4f1a\u7ed9\u51fa\u4e00\u4e2a\u968f\u673a\u54cd\u5e94\uff0c\u8be5\u54cd\u5e94\u53ef\u4ee5\u9002\u7528\u4e8e\u8bb8\u591a\u4e0d\u540c\u7684\u8bed\u53e5\u3002 Eliza \u5f88\u5bb9\u6613\u88ab\u6b3a\u9a97\uff0c\u4f8b\u5982\uff0c\u5982\u679c\u7528\u6237\u5199\u4e86 \"You are a bicycle\"\uff08\u4f60\u662f \u4e2a \u81ea\u884c\u8f66\uff09\uff0c\u5b83\u53ef\u80fd\u4f1a\u56de\u590d \"How long have I been a bicycle?\"\uff08\u6211\u5df2\u7ecf\u662f \u4e00\u4e2a \u81ea\u884c\u8f66 \u591a\u4e45\u4e86?\uff09\uff0c\u800c\u4e0d\u662f\u66f4\u5408\u7406\u7684\u56de\u7b54\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u65b9\u7684\u56fe\u7247\u67e5\u770b\u5173\u4e8e Eliza \u539f\u578b\u7684\u89c6\u9891</p> <p>\u65c1\u6ce8\uff1a\u5982\u679c\u4f60\u62e5\u6709 ACM \u8d26\u6237\uff0c\u4f60\u53ef\u4ee5\u9605\u8bfb 1996 \u5e74\u53d1\u8868\u7684 Eliza \u7684\u539f\u59cb\u4ecb\u7ecd\u3002\u6216\u8005\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u4e0a\u9605\u8bfb\u6709\u5173 Eliza \u7684\u4fe1\u606f\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#-_1","title":"\u7ec3\u4e60 - \u7f16\u7a0b\u5b9e\u73b0\u4e00\u4e2a\u57fa\u7840\u7684\u5bf9\u8bdd\u673a\u5668\u4eba","text":"<p>\u50cf Eliza \u4e00\u6837\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u662f\u4e00\u4e2a\u770b\u8d77\u6765\u53ef\u4ee5\u667a\u80fd\u5730\u7406\u89e3\u548c\u54cd\u5e94\u7528\u6237\u8f93\u5165\u7684\u7a0b\u5e8f\u3002\u4e0e Eliza \u4e0d\u540c\u7684\u662f\uff0c\u6211\u4eec\u7684\u673a\u5668\u4eba\u4e0d\u4f1a\u7528\u89c4\u5219\u8ba9\u5b83\u770b\u8d77\u6765\u50cf\u662f\u5728\u8fdb\u884c\u667a\u80fd\u5bf9\u8bdd\u3002\u6211\u4eec\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5c06\u53ea\u6709\u4e00\u79cd\u80fd\u529b\uff1a\u5b83\u53ea\u4f1a\u901a\u8fc7\u57fa\u672c\u4e0a\u53ef\u4ee5\u7cca\u5f04\u6240\u6709\u666e\u901a\u5bf9\u8bdd\u7684\u53e5\u5b50\u6765\u968f\u673a\u56de\u7b54\uff0c\u4f7f\u5f97\u8c08\u8bdd\u80fd\u591f\u7ee7\u7eed\u8fdb\u884c\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_10","title":"\u8ba1\u5212","text":"<p>\u642d\u5efa\u804a\u5929\u673a\u5668\u4eba\u7684\u6b65\u9aa4</p> <ol> <li>\u6253\u5370\u7528\u6237\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u4f7f\u7528\u8bf4\u660e</li> <li>\u5f00\u542f\u5faa\u73af</li> <li>\u83b7\u53d6\u7528\u6237\u8f93\u5165</li> <li>\u5982\u679c\u7528\u6237\u8981\u6c42\u9000\u51fa\uff0c\u5c31\u9000\u51fa</li> <li>\u5904\u7406\u7528\u6237\u8f93\u5165\u5e76\u9009\u62e9\u4e00\u4e2a\u56de\u7b54\uff08\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4ece\u56de\u7b54\u5217\u8868\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u56de\u7b54\uff09</li> <li>\u6253\u5370\u56de\u7b54</li> <li>\u91cd\u590d\u6b65\u9aa4 2</li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_11","title":"\u6784\u5efa\u804a\u5929\u673a\u5668\u4eba","text":"<p>\u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u5efa\u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba\u3002\u6211\u4eec\u5c06\u4ece\u5b9a\u4e49\u4e00\u4e9b\u77ed\u8bed\u5f00\u59cb\u3002</p> <ol> <li> <p>\u4f7f\u7528\u4ee5\u4e0b\u968f\u673a\u7684\u56de\u590d\uff08<code>random_responses</code>\uff09\u5728 Python \u4e2d\u81ea\u5df1\u521b\u5efa\u6b64\u673a\u5668\u4eba\uff1a</p> <pre><code>random_responses = [\"That is quite interesting, please tell me more.\",\n                    \"I see. Do go on.\",\n                    \"Why do you say that?\",\n                    \"Funny weather we've been having, isn't it?\",\n                    \"Let's change the subject.\",\n                    \"Did you catch the game last night?\"]\n</code></pre> <p>\u7a0b\u5e8f\u8fd0\u884c\u770b\u8d77\u6765\u5e94\u8be5\u662f\u8fd9\u6837\uff1a\uff08\u7528\u6237\u8f93\u5165\u4f4d\u4e8e\u4ee5 <code>&gt;</code> \u5f00\u5934\u7684\u884c\u4e0a\uff09</p> <pre><code>Hello, I am Marvin, the simple robot.\nYou can end this conversation at any time by typing 'bye'\nAfter typing each answer, press 'enter'\nHow are you today?\n&gt; I am good thanks\nThat is quite interesting, please tell me more.\n&gt; today I went for a walk     \nDid you catch the game last night?\n&gt; I did, but my team lost\nFunny weather we've been having, isn't it?\n&gt; yes but I hope next week is better\nLet's change the subject.\n&gt; ok, lets talk about music\nWhy do you say that?\n&gt; because I like music!\nWhy do you say that?\n&gt; bye\nIt was nice talking to you, goodbye!\n</code></pre> <p>\u793a\u4f8b\u7a0b\u5e8f\u5728\u8fd9\u91cc\u3002\u8fd9\u53ea\u662f\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u2705 \u505c\u4e0b\u6765\uff0c\u601d\u8003\u4e00\u4e0b</p> <ol> <li>\u4f60\u8ba4\u4e3a\u8fd9\u4e9b\u968f\u673a\u54cd\u5e94\u80fd\u591f\u201c\u6b3a\u9a97\u201d\u4eba\u7c7b\uff0c\u4f7f\u4eba\u7c7b\u8ba4\u4e3a\u673a\u5668\u4eba\u5b9e\u9645\u4e0a\u7406\u89e3\u4e86\u4ed6\u4eec\u7684\u610f\u601d\u5417\uff1f</li> <li>\u673a\u5668\u4eba\u9700\u8981\u54ea\u4e9b\u529f\u80fd\u624d\u80fd\u66f4\u6709\u6548\u7684\u56de\u5e94\uff1f</li> <li>\u5982\u679c\u673a\u5668\u4eba\u771f\u7684\u53ef\u4ee5\u201c\u7406\u89e3\u201d\u4e00\u4e2a\u53e5\u5b50\u7684\u610f\u601d\uff0c\u5b83\u662f\u5426\u4e5f\u9700\u8981\u201c\u8bb0\u4f4f\u201d\u524d\u9762\u53e5\u5b50\u7684\u610f\u601d\uff1f</li> </ol> </li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_12","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u5728\u4e0a\u9762\u7684\u300c\u505c\u4e0b\u6765\uff0c\u601d\u8003\u4e00\u4e0b\u300d\u677f\u5757\u4e2d\u9009\u62e9\u4e00\u4e2a\u95ee\u9898\uff0c\u5c1d\u8bd5\u7f16\u7a0b\u5b9e\u73b0\u5b83\u4eec\uff0c\u6216\u4f7f\u7528\u4f2a\u4ee3\u7801\u5728\u7eb8\u4e0a\u7f16\u5199\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u5728\u4e0b\u4e00\u8bfe\u4e2d\uff0c\u60a8\u5c06\u4e86\u89e3\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u548c\u673a\u5668\u5b66\u4e60\u7684\u8bb8\u591a\u5176\u4ed6\u65b9\u6cd5\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_13","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_14","title":"\u590d\u4e60\u4e0e\u81ea\u5b66","text":"<p>\u770b\u770b\u4e0b\u9762\u7684\u53c2\u8003\u8d44\u6599\u4f5c\u4e3a\u8fdb\u4e00\u6b65\u7684\u53c2\u8003\u9605\u8bfb\u3002</p>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_15","title":"\u53c2\u8003","text":"<ol> <li>Schubert, Lenhart, \"Computational Linguistics\", The Stanford Encyclopedia of Philosophy (Spring 2020 Edition), Edward N. Zalta (ed.), URL = https://plato.stanford.edu/archives/spr2020/entries/computational-linguistics/.</li> <li>Princeton University \"About WordNet.\" WordNet. Princeton University. 2010. </li> </ol>"},{"location":"6-NLP/1-Introduction-to-NLP/README.zh-cn/#_16","title":"\u4efb\u52a1","text":"<p>\u67e5\u627e\u4e00\u4e2a\u673a\u5668\u4eba</p>"},{"location":"6-NLP/1-Introduction-to-NLP/assignment/","title":"Search for a bot","text":""},{"location":"6-NLP/1-Introduction-to-NLP/assignment/#instructions","title":"Instructions","text":"<p>Bots are everywhere. Your assignment: find one and adopt it! You can find them on web sites, in banking applications, and on the phone, for example when you call financial services companies for advice or account information. Analyze the bot and see if you can confuse it. If you can confuse the bot, why do you think that happened? Write a short paper about your experience.</p>"},{"location":"6-NLP/1-Introduction-to-NLP/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A full  page paper is written, explaining the presumed bot architecture and outlining your experience with it A paper is incomplete or not well researched No paper is submitted"},{"location":"6-NLP/2-Tasks/","title":"Common natural language processing tasks and techniques","text":"<p>For most natural language processing tasks, the text to be processed, must be broken down, examined, and the results stored or cross referenced with rules and data sets. These tasks, allows the programmer to derive the meaning or intent or only the frequency of terms and words in a text.</p>"},{"location":"6-NLP/2-Tasks/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Let's discover common techniques used in processing text. Combined with machine learning, these techniques help you to analyse large amounts of text efficiently. Before applying ML to these tasks, however, let's understand the problems encountered by an NLP specialist.</p>"},{"location":"6-NLP/2-Tasks/#tasks-common-to-nlp","title":"Tasks common to NLP","text":"<p>There are different ways to analyse a text you are working on. There are tasks you can perform and through these tasks you are able to gauge an understanding of the text and draw conclusions. You usually carry out these tasks in a sequence.</p>"},{"location":"6-NLP/2-Tasks/#tokenization","title":"Tokenization","text":"<p>Probably the first thing most NLP algorithms have to do is to split the text into tokens, or words. While this sounds simple, having to account for punctuation and different languages' word and sentence delimiters can make it tricky. You might have to use various methods to determine demarcations.</p> <p></p> <p>Tokenizing a sentence from Pride and Prejudice. Infographic by Jen Looper</p>"},{"location":"6-NLP/2-Tasks/#embeddings","title":"Embeddings","text":"<p>Word embeddings are a way to convert your text data numerically. Embeddings are done in a way so that words with a similar meaning or words used together cluster together.</p> <p></p> <p>\"I have the highest respect for your nerves, they are my old friends.\" - Word embeddings for a sentence in Pride and Prejudice. Infographic by Jen Looper</p> <p>\u2705 Try this interesting tool to experiment with word embeddings. Clicking on one word shows clusters of similar words: 'toy' clusters with 'disney', 'lego', 'playstation', and 'console'.</p>"},{"location":"6-NLP/2-Tasks/#parsing-part-of-speech-tagging","title":"Parsing &amp; Part-of-speech Tagging","text":"<p>Every word that has been tokenized can be tagged as a part of speech - a noun, verb, or adjective. The sentence <code>the quick red fox jumped over the lazy brown dog</code> might be POS tagged as fox = noun, jumped = verb.</p> <p></p> <p>Parsing a sentence from Pride and Prejudice. Infographic by Jen Looper</p> <p>Parsing is recognizing what words are related to each other in a sentence - for instance <code>the quick red fox jumped</code> is an adjective-noun-verb sequence that is separate from the <code>lazy brown dog</code> sequence.  </p>"},{"location":"6-NLP/2-Tasks/#word-and-phrase-frequencies","title":"Word and Phrase Frequencies","text":"<p>A useful procedure when analyzing a large body of text is to build a dictionary of every word or phrase of interest and how often it appears. The phrase <code>the quick red fox jumped over the lazy brown dog</code> has a word frequency of 2 for the.</p> <p>Let's look at an example text where we count the frequency of words. Rudyard Kipling's poem The Winners contains the following verse:</p> <pre><code>What the moral? Who rides may read.\nWhen the night is thick and the tracks are blind\nA friend at a pinch is a friend, indeed,\nBut a fool to wait for the laggard behind.\nDown to Gehenna or up to the Throne,\nHe travels the fastest who travels alone.\n</code></pre> <p>As phrase frequencies can be case insensitive or case sensitive as required, the phrase <code>a friend</code> has a frequency of 2 and <code>the</code> has a frequency of 6, and <code>travels</code> is 2.</p>"},{"location":"6-NLP/2-Tasks/#n-grams","title":"N-grams","text":"<p>A text can be split into sequences of words of a set length, a single word (unigram), two words (bigrams), three words (trigrams) or any number of words (n-grams).</p> <p>For instance <code>the quick red fox jumped over the lazy brown dog</code> with a n-gram score of 2 produces the following n-grams:</p> <ol> <li>the quick </li> <li>quick red </li> <li>red fox</li> <li>fox jumped </li> <li>jumped over </li> <li>over the </li> <li>the lazy </li> <li>lazy brown </li> <li>brown dog</li> </ol> <p>It might be easier to visualize it as a sliding box over the sentence. Here it is for n-grams of 3 words, the n-gram is in bold in each sentence:</p> <ol> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> <li>the quick red fox jumped over the lazy brown dog</li> </ol> <p></p> <p>N-gram value of 3: Infographic by Jen Looper</p>"},{"location":"6-NLP/2-Tasks/#noun-phrase-extraction","title":"Noun phrase Extraction","text":"<p>In most sentences, there is a noun that is the subject, or object of the sentence. In English, it is often identifiable as having 'a' or 'an' or 'the' preceding it. Identifying the subject or object of a sentence by 'extracting the noun phrase' is a common task in NLP when attempting to understand the meaning of a sentence.</p> <p>\u2705 In the sentence \"I cannot fix on the hour, or the spot, or the look or the words, which laid the foundation. It is too long ago. I was in the middle before I knew that I had begun.\", can you identify the noun phrases?</p> <p>In the sentence <code>the quick red fox jumped over the lazy brown dog</code> there are 2 noun phrases: quick red fox and lazy brown dog.</p>"},{"location":"6-NLP/2-Tasks/#sentiment-analysis","title":"Sentiment analysis","text":"<p>A sentence or text can be analysed for sentiment, or how positive or negative it is. Sentiment is measured in polarity and objectivity/subjectivity. Polarity is measured from -1.0 to 1.0 (negative to positive) and 0.0 to 1.0 (most objective to most subjective).</p> <p>\u2705 Later you'll learn that there are different ways to determine sentiment using machine learning, but one way is to have a list of words and phrases that are categorized as positive or negative by a human expert and apply that model to text to calculate a polarity score. Can you see how this would work in some circumstances and less well in others?</p>"},{"location":"6-NLP/2-Tasks/#inflection","title":"Inflection","text":"<p>Inflection enables you to take a word and get the singular or plural of the word.</p>"},{"location":"6-NLP/2-Tasks/#lemmatization","title":"Lemmatization","text":"<p>A lemma is the root or headword for a set of words, for instance flew, flies, flying have a lemma of the verb fly.</p> <p>There are also useful databases available for the NLP researcher, notably:</p>"},{"location":"6-NLP/2-Tasks/#wordnet","title":"WordNet","text":"<p>WordNet is a database of words, synonyms, antonyms and many other details for every word in many different languages. It is incredibly useful when attempting to build translations, spell checkers, or language tools of any type.</p>"},{"location":"6-NLP/2-Tasks/#nlp-libraries","title":"NLP Libraries","text":"<p>Luckily, you don't have to build all of these techniques yourself, as there are excellent Python libraries available that make it much more accessible to developers who aren't specialized in natural language processing or machine learning. The next lessons include more examples of these, but here you will learn some useful examples to help you with the next task.</p>"},{"location":"6-NLP/2-Tasks/#exercise-using-textblob-library","title":"Exercise - using <code>TextBlob</code> library","text":"<p>Let's use a library called TextBlob as it contains helpful APIs for tackling these types of tasks. TextBlob \"stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\" It has a considerable amount of ML embedded in its API.</p> <p>Note: A useful Quick Start guide is available for TextBlob that is recommended for experienced Python developers </p> <p>When attempting to identify noun phrases, TextBlob offers several options of extractors to find noun phrases. </p> <ol> <li> <p>Take a look at <code>ConllExtractor</code>.</p> <pre><code>from textblob import TextBlob\nfrom textblob.np_extractors import ConllExtractor\n# import and create a Conll extractor to use later \nextractor = ConllExtractor()\n\n# later when you need a noun phrase extractor:\nuser_input = input(\"&gt; \")\nuser_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified\nnp = user_input_blob.noun_phrases                                    \n</code></pre> <p>What's going on here? ConllExtractor is \"A noun phrase extractor that uses chunk parsing trained with the ConLL-2000 training corpus.\" ConLL-2000 refers to the 2000 Conference on Computational Natural Language Learning. Each year the conference hosted a workshop to tackle a thorny NLP problem, and in 2000 it was noun chunking. A model was trained on the Wall Street Journal, with \"sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens)\". You can look at the procedures used here and the results.</p> </li> </ol>"},{"location":"6-NLP/2-Tasks/#challenge-improving-your-bot-with-nlp","title":"Challenge - improving your bot with NLP","text":"<p>In the previous lesson you built a very simple Q&amp;A bot. Now, you'll make Marvin a bit more sympathetic by analyzing your input for sentiment and printing out a response to match the sentiment. You'll also need to identify a <code>noun_phrase</code> and ask about it.</p> <p>Your steps when building a better conversational bot:</p> <ol> <li>Print instructions advising the user how to interact with the bot</li> <li>Start loop </li> <li>Accept user input</li> <li>If user has asked to exit, then exit</li> <li>Process user input and determine appropriate sentiment response</li> <li>If a noun phrase is detected in the sentiment, pluralize it and ask for more input on that topic</li> <li>Print response</li> <li>loop back to step 2</li> </ol> <p>Here is the code snippet to determine sentiment using TextBlob. Note there are only four gradients of sentiment response (you could have more if you like):</p> <pre><code>if user_input_blob.polarity &lt;= -0.5:\n  response = \"Oh dear, that sounds bad. \"\nelif user_input_blob.polarity &lt;= 0:\n  response = \"Hmm, that's not great. \"\nelif user_input_blob.polarity &lt;= 0.5:\n  response = \"Well, that sounds positive. \"\nelif user_input_blob.polarity &lt;= 1:\n  response = \"Wow, that sounds great. \"\n</code></pre> <p>Here is some sample output to guide you (user input is on the lines with starting with &gt;):</p> <pre><code>Hello, I am Marvin, the friendly robot.\nYou can end this conversation at any time by typing 'bye'\nAfter typing each answer, press 'enter'\nHow are you today?\n&gt; I am ok\nWell, that sounds positive. Can you tell me more?\n&gt; I went for a walk and saw a lovely cat\nWell, that sounds positive. Can you tell me more about lovely cats?\n&gt; cats are the best. But I also have a cool dog\nWow, that sounds great. Can you tell me more about cool dogs?\n&gt; I have an old hounddog but he is sick\nHmm, that's not great. Can you tell me more about old hounddogs?\n&gt; bye\nIt was nice talking to you, goodbye!\n</code></pre> <p>One possible solution to the task is here</p> <p>\u2705 Knowledge Check</p> <ol> <li>Do you think the sympathetic responses would 'trick' someone into thinking that the bot actually understood them?</li> <li>Does identifying the noun phrase make the bot more 'believable'?</li> <li>Why would extracting a 'noun phrase' from a sentence a useful thing to do?</li> </ol> <p>Implement the bot in the prior knowledge check and test it on a friend. Can it trick them? Can you make your bot more 'believable?'</p>"},{"location":"6-NLP/2-Tasks/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Take a task in the prior knowledge check and try to implement it. Test the bot on a friend. Can it trick them? Can you make your bot more 'believable?'</p>"},{"location":"6-NLP/2-Tasks/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"6-NLP/2-Tasks/#review-self-study","title":"Review &amp; Self Study","text":"<p>In the next few lessons you will learn more about sentiment analysis. Research this interesting technique in articles such as these on KDNuggets</p>"},{"location":"6-NLP/2-Tasks/#assignment","title":"Assignment","text":"<p>Make a bot talk back</p>"},{"location":"6-NLP/2-Tasks/assignment/","title":"Make a Bot talk back","text":""},{"location":"6-NLP/2-Tasks/assignment/#instructions","title":"Instructions","text":"<p>In the past few lessons, you programmed a basic bot with whom to chat. This bot gives random answers until you say 'bye'. Can you make the answers a little less random, and trigger answers if you say specific things, like 'why' or 'how'? Think a bit how machine learning might make this type of work less manual as you extend your bot. You can use NLTK or TextBlob libraries to make your tasks easier.</p>"},{"location":"6-NLP/2-Tasks/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A new bot.py file is presented and documented A new bot file is presented but it contains bugs A file is not presented"},{"location":"6-NLP/3-Translation-Sentiment/","title":"Translation and sentiment analysis with ML","text":"<p>In the previous lessons you learned how to build a basic bot using <code>TextBlob</code>, a library that embeds ML behind-the-scenes to perform basic NLP tasks such as noun phrase extraction. Another important challenge in computational linguistics is accurate translation of a sentence from one spoken or written language to another.</p>"},{"location":"6-NLP/3-Translation-Sentiment/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>Translation is a very hard problem compounded by the fact that there are thousands of languages and each can have very different grammar rules. One approach is to convert the formal grammar rules for one language, such as English, into a non-language dependent structure, and then translate it by converting back to another language. This approach means that you would take the following steps:</p> <ol> <li>Identification. Identify or tag the words in input language into nouns, verbs etc.</li> <li>Create translation. Produce a direct translation of each word in the target language format.</li> </ol>"},{"location":"6-NLP/3-Translation-Sentiment/#example-sentence-english-to-irish","title":"Example sentence, English to Irish","text":"<p>In 'English', the sentence I feel happy is three words in the order:</p> <ul> <li>subject (I)</li> <li>verb (feel)</li> <li>adjective (happy)</li> </ul> <p>However, in the 'Irish' language, the same sentence has a very different grammatical structure - emotions like \"happy\" or \"sad\" are expressed as being upon you.</p> <p>The English phrase <code>I feel happy</code> in Irish would be <code>T\u00e1 athas orm</code>. A literal translation would be <code>Happy is upon me</code>.</p> <p>An Irish speaker translating to English would say <code>I feel happy</code>, not <code>Happy is upon me</code>, because they understand the meaning of the sentence, even if the words and sentence structure are different.</p> <p>The formal order for the sentence in Irish are:</p> <ul> <li>verb (T\u00e1 or is)</li> <li>adjective (athas, or happy)</li> <li>subject (orm, or upon me)</li> </ul>"},{"location":"6-NLP/3-Translation-Sentiment/#translation","title":"Translation","text":"<p>A naive translation program might translate words only, ignoring the sentence structure.</p> <p>\u2705 If you've learned a second (or third or more) language as an adult, you might have started by thinking in your native language, translating a concept word by word in your head to the second language, and then speaking out your translation. This is similar to what naive translation computer programs are doing. It's important to get past this phase to attain fluency!</p> <p>Naive translation leads to bad (and sometimes hilarious) mistranslations: <code>I feel happy</code> translates literally to <code>Mise bhraitheann athas</code> in Irish. That means (literally) <code>me feel happy</code> and is not a valid Irish sentence. Even though English and Irish are languages spoken on two closely neighboring islands, they are very different languages with different grammar structures.</p> <p>You can watch some videos about Irish linguistic traditions such as this one</p>"},{"location":"6-NLP/3-Translation-Sentiment/#machine-learning-approaches","title":"Machine learning approaches","text":"<p>So far, you've learned about the formal rules approach to natural language processing. Another approach is to ignore the meaning of the words, and instead use machine learning to detect patterns. This can work in translation if you have lots of text (a corpus) or texts (corpora) in both the origin and target languages.</p> <p>For instance, consider the case of Pride and Prejudice, a well-known English novel written by Jane Austen in 1813. If you consult the book in English and a human translation of the book in French, you could detect phrases in one that are idiomatically translated into the other. You'll do that in a minute.</p> <p>For instance, when an English phrase such as <code>I have no money</code> is translated literally to French, it might become <code>Je n'ai pas de monnaie</code>. \"Monnaie\" is a tricky french 'false cognate', as 'money' and 'monnaie' are not synonymous. A better translation that a human might make would be <code>Je n'ai pas d'argent</code>, because it better conveys the meaning that you have no money (rather than 'loose change' which is the meaning of 'monnaie').</p> <p></p> <p>Image by Jen Looper</p> <p>If an ML model has enough human translations to build a model on, it can improve the accuracy of translations by identifying common patterns in texts that have been previously translated by expert human speakers of both languages.</p>"},{"location":"6-NLP/3-Translation-Sentiment/#exercise-translation","title":"Exercise - translation","text":"<p>You can use <code>TextBlob</code> to translate sentences. Try the famous first line of Pride and Prejudice:</p> <pre><code>from textblob import TextBlob\n\nblob = TextBlob(\n    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!\"\n)\nprint(blob.translate(to=\"fr\"))\n</code></pre> <p><code>TextBlob</code> does a pretty good job at the translation: \"C'est une v\u00e9rit\u00e9 universellement reconnue, qu'un homme c\u00e9libataire en possession d'une bonne fortune doit avoir besoin d'une femme!\". </p> <p>It can be argued that TextBlob's translation is far more exact, in fact, than the 1932 French translation of the book by V. Leconte and Ch. Pressoir:</p> <p>\"C'est une v\u00e9rit\u00e9 universelle qu'un c\u00e9libataire pourvu d'une belle fortune doit avoir envie de se marier, et, si peu que l'on sache de son sentiment \u00e0 cet egard, lorsqu'il arrive dans une nouvelle r\u00e9sidence, cette id\u00e9e est si bien fix\u00e9e dans l'esprit de ses voisins qu'ils le consid\u00e8rent sur-le-champ comme la propri\u00e9t\u00e9 l\u00e9gitime de l'une ou l'autre de leurs filles.\"</p> <p>In this case, the translation informed by ML does a better job than the human translator who is unnecessarily putting words in the original author's mouth for 'clarity'.</p> <p>What's going on here? and why is TextBlob so good at translation? Well, behind the scenes, it's using Google translate, a sophisticated AI able to parse millions of phrases to predict the best strings for the task at hand. There's nothing manual going on here and you need an internet connection to use <code>blob.translate</code>.</p> <p>\u2705 Try some more sentences. Which is better, ML or human translation? In which cases?</p>"},{"location":"6-NLP/3-Translation-Sentiment/#sentiment-analysis","title":"Sentiment analysis","text":"<p>Another area where machine learning can work very well is sentiment analysis. A non-ML approach to sentiment is to identify words and phrases which are 'positive' and 'negative'. Then, given a new piece of text, calculate the total value of the positive, negative and neutral words to identify the overall sentiment. </p> <p>This approach is easily tricked as you may have seen in the Marvin task - the sentence <code>Great, that was a wonderful waste of time, I'm glad we are lost on this dark road</code> is a sarcastic, negative sentiment sentence, but the simple algorithm detects 'great', 'wonderful', 'glad' as positive and 'waste', 'lost' and 'dark' as negative. The overall sentiment is swayed by these conflicting words.</p> <p>\u2705 Stop a second and think about how we convey sarcasm as human speakers. Tone inflection plays a large role. Try to say the phrase \"Well, that film was awesome\" in different ways to discover how your voice conveys meaning.</p>"},{"location":"6-NLP/3-Translation-Sentiment/#ml-approaches","title":"ML approaches","text":"<p>The ML approach would be to manually gather negative and positive bodies of text - tweets, or movie reviews, or anything where the human has given a score and a written opinion. Then NLP techniques can be applied to opinions and scores, so that patterns emerge (e.g., positive movie reviews tend to have the phrase 'Oscar worthy' more than negative movie reviews, or positive restaurant reviews say 'gourmet' much more than 'disgusting').</p> <p>\u2696\ufe0f Example: If you worked in a politician's office and there was some new law being debated, constituents might write to the office with emails supporting or emails against the particular new law. Let's say you are tasked with reading the emails and sorting them in 2 piles, for and against. If there were a lot of emails, you might be overwhelmed attempting to read them all. Wouldn't it be nice if a bot could read them all for you, understand them and tell you in which pile each email belonged? </p> <p>One way to achieve that is to use Machine Learning. You would train the model with a portion of the against emails and a portion of the for emails. The model would tend to associate phrases and words with the against side and the for side, but it would not understand any of the content, only that certain words and patterns were more likely to appear in an against or a for email. You could test it with some emails that you had not used to train the model, and see if it came to the same conclusion as you did. Then, once you were happy with the accuracy of the model, you could process future emails without having to read each one.</p> <p>\u2705 Does this process sound like processes you have used in previous lessons?</p>"},{"location":"6-NLP/3-Translation-Sentiment/#exercise-sentimental-sentences","title":"Exercise - sentimental sentences","text":"<p>Sentiment is measured in with a polarity of -1 to 1, meaning -1 is the most negative sentiment, and 1 is the most positive. Sentiment is also measured with an 0 - 1 score for objectivity (0) and subjectivity (1).</p> <p>Take another look at Jane Austen's Pride and Prejudice. The text is available here at Project Gutenberg. The sample below shows a short program which analyses the sentiment of first and last sentences from the book and display its sentiment polarity and subjectivity/objectivity score.</p> <p>You should use the <code>TextBlob</code> library (described above) to determine <code>sentiment</code> (you do not have to write your own sentiment calculator) in the following task.</p> <pre><code>from textblob import TextBlob\n\nquote1 = \"\"\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\"\"\n\nquote2 = \"\"\"Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them.\"\"\"\n\nsentiment1 = TextBlob(quote1).sentiment\nsentiment2 = TextBlob(quote2).sentiment\n\nprint(quote1 + \" has a sentiment of \" + str(sentiment1))\nprint(quote2 + \" has a sentiment of \" + str(sentiment2))\n</code></pre> <p>You see the following output:</p> <pre><code>It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want # of a wife. has a sentiment of Sentiment(polarity=0.20952380952380953, subjectivity=0.27142857142857146)\n\nDarcy, as well as Elizabeth, really loved them; and they were\n     both ever sensible of the warmest gratitude towards the persons\n      who, by bringing her into Derbyshire, had been the means of\n      uniting them. has a sentiment of Sentiment(polarity=0.7, subjectivity=0.8)\n</code></pre>"},{"location":"6-NLP/3-Translation-Sentiment/#challenge-check-sentiment-polarity","title":"Challenge - check sentiment polarity","text":"<p>Your task is to determine, using sentiment polarity, if Pride and Prejudice has more absolutely positive sentences than absolutely negative ones. For this task, you may assume that a polarity score of 1 or -1 is absolutely positive or negative respectively.</p> <p>Steps:</p> <ol> <li>Download a copy of Pride and Prejudice from Project Gutenberg as a .txt file. Remove the metadata at the start and end of the file, leaving only the original text</li> <li>Open the file in Python and extract the contents as a string</li> <li>Create a TextBlob using the book string</li> <li>Analyse each sentence in the book in a loop</li> <li>If the polarity is 1 or -1 store the sentence in an array or list of positive or negative messages</li> <li>At the end, print out all the positive sentences and negative sentences (separately) and the number of each.</li> </ol> <p>Here is a sample solution.</p> <p>\u2705 Knowledge Check</p> <ol> <li>The sentiment is based on words used in the sentence, but does the code understand the words?</li> <li>Do you think the sentiment polarity is accurate, or in other words, do you agree with the scores?</li> <li>In particular, do you agree or disagree with the absolute positive polarity of the following sentences?<ul> <li>\u201cWhat an excellent father you have, girls!\u201d said she, when the door was shut.</li> <li>\u201cYour examination of Mr. Darcy is over, I presume,\u201d said Miss Bingley; \u201cand pray what is the result?\u201d \u201cI am perfectly convinced by it that Mr. Darcy has no defect.</li> <li>How wonderfully these sort of things occur!</li> <li>I have the greatest dislike in the world to that sort of thing.</li> <li>Charlotte is an excellent manager, I dare say.</li> <li>\u201cThis is delightful indeed!</li> <li>I am so happy!</li> <li>Your idea of the ponies is delightful.</li> </ul> </li> <li>The next 3 sentences were scored with an absolute positive sentiment, but on close reading, they are not positive sentences. Why did the sentiment analysis think they were positive sentences?<ul> <li>Happy shall I be, when his stay at Netherfield is over!\u201d \u201cI wish I could say anything to comfort you,\u201d replied Elizabeth; \u201cbut it is wholly out of my power.</li> <li>If I could but see you as happy!</li> <li>Our distress, my dear Lizzy, is very great.</li> </ul> </li> <li>Do you agree or disagree with the absolute negative polarity of the following sentences?<ul> <li>Everybody is disgusted with his pride.</li> <li>\u201cI should like to know how he behaves among strangers.\u201d \u201cYou shall hear then\u2014but prepare yourself for something very dreadful.</li> <li>The pause was to Elizabeth\u2019s feelings dreadful.</li> <li>It would be dreadful!</li> </ul> </li> </ol> <p>\u2705 Any aficionado of Jane Austen will understand that she often uses her books to critique the more ridiculous aspects of English Regency society. Elizabeth Bennett, the main character in Pride and Prejudice, is a keen social observer (like the author) and her language is often heavily nuanced. Even Mr. Darcy (the love interest in the story) notes Elizabeth's playful and teasing use of language: \"I have had the pleasure of your acquaintance long enough to know that you find great enjoyment in occasionally professing opinions which in fact are not your own.\"</p>"},{"location":"6-NLP/3-Translation-Sentiment/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Can you make Marvin even better by extracting other features from the user input?</p>"},{"location":"6-NLP/3-Translation-Sentiment/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"6-NLP/3-Translation-Sentiment/#review-self-study","title":"Review &amp; Self Study","text":"<p>There are many ways to extract sentiment from text. Think of the business applications that might make use of this technique. Think about how it can go awry. Read more about sophisticated enterprise-ready systems that analyze sentiment such as Azure Text Analysis. Test some of the Pride and Prejudice sentences above and see if it can detect nuance.</p>"},{"location":"6-NLP/3-Translation-Sentiment/#assignment","title":"Assignment","text":"<p>Poetic license</p>"},{"location":"6-NLP/3-Translation-Sentiment/assignment/","title":"Poetic license","text":""},{"location":"6-NLP/3-Translation-Sentiment/assignment/#instructions","title":"Instructions","text":"<p>In this notebook you can find over 500 Emily Dickinson poems previously analyzed for sentiment using Azure text analytics. Using this dataset, analyze it using the techniques described in the lesson. Does the suggested sentiment of a poem match the more sophistic Azure service's decision? Why or why not, in your opinion? Does anything surprise you?</p>"},{"location":"6-NLP/3-Translation-Sentiment/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with a solid analysis of an author's sample output The notebook is incomplete or does not perform analysis No notebook is presented"},{"location":"6-NLP/3-Translation-Sentiment/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"6-NLP/3-Translation-Sentiment/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/","title":"Sentiment analysis with hotel reviews - processing the data","text":"<p>In this section you will use the techniques in the previous lessons to do some exploratory data analysis of a large dataset. Once you have a good understanding of the usefulness of the various columns, you will learn: </p> <ul> <li>how to remove the unnecessary columns</li> <li>how to calculate some new data based on the existing columns</li> <li>how to save the resulting dataset for use in the final challenge</li> </ul>"},{"location":"6-NLP/4-Hotel-Reviews-1/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"6-NLP/4-Hotel-Reviews-1/#introduction","title":"Introduction","text":"<p>So far you've learned about how text data is quite unlike numerical types of data. If it's text that was written or spoken by a human, if can be analysed to find patterns and frequencies, sentiment and meaning. This lesson takes you into a real data set with a real challenge: 515K Hotel Reviews Data in Europe and includes a CC0: Public Domain license. It was scraped from Booking.com from public sources. The creator of the dataset was Jiashen Liu.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#preparation","title":"Preparation","text":"<p>You will need:</p> <ul> <li>The ability to run .ipynb notebooks using Python 3</li> <li>pandas</li> <li>NLTK, which you should install locally</li> <li>The data set which is available on Kaggle 515K Hotel Reviews Data in Europe. It is around 230 MB unzipped. Download it to the root <code>/data</code> folder associated with these NLP lessons.</li> </ul>"},{"location":"6-NLP/4-Hotel-Reviews-1/#exploratory-data-analysis","title":"Exploratory data analysis","text":"<p>This challenge assumes that you are building a hotel recommendation bot using sentiment analysis and guest reviews scores. The dataset you will be using includes reviews of 1493 different hotels in 6 cities. </p> <p>Using Python, a dataset of hotel reviews, and NLTK's sentiment analysis you could find out:</p> <ul> <li>What are the most frequently used words and phrases in reviews?</li> <li>Do the official tags describing a hotel correlate with review scores (e.g. are the more negative reviews for a particular hotel for  Family with young children than by Solo traveller, perhaps indicating it is better for Solo travellers?)</li> <li>Do the NLTK sentiment scores 'agree' with the hotel reviewer's numerical score?</li> </ul>"},{"location":"6-NLP/4-Hotel-Reviews-1/#dataset","title":"Dataset","text":"<p>Let's explore the dataset that you've downloaded and saved locally. Open the file in an editor like VS Code or even Excel.</p> <p>The headers in the dataset are as follows:</p> <p>Hotel_Address, Additional_Number_of_Scoring, Review_Date, Average_Score, Hotel_Name, Reviewer_Nationality, Negative_Review, Review_Total_Negative_Word_Counts, Total_Number_of_Reviews, Positive_Review, Review_Total_Positive_Word_Counts, Total_Number_of_Reviews_Reviewer_Has_Given, Reviewer_Score, Tags, days_since_review, lat, lng</p> <p>Here they are grouped in a way that might be easier to examine: </p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#hotel-columns","title":"Hotel columns","text":"<ul> <li><code>Hotel_Name</code>, <code>Hotel_Address</code>, <code>lat</code> (latitude), <code>lng</code> (longitude)</li> <li>Using lat and lng you could plot a map with Python showing the hotel locations (perhaps color coded for negative and positive reviews)</li> <li>Hotel_Address is not obviously useful to us, and we'll probably replace that with a country for easier sorting &amp; searching</li> </ul> <p>Hotel Meta-review columns</p> <ul> <li><code>Average_Score</code></li> <li>According to the dataset creator, this column is the Average Score of the hotel, calculated based on the latest comment in the last year. This seems like an unusual way to calculate the score, but it is the data scraped so we may take it as face value for now. </li> </ul> <p>\u2705 Based on the other columns in this data, can you think of another way to calculate the average score?</p> <ul> <li><code>Total_Number_of_Reviews</code></li> <li>The total number of reviews this hotel has received - it is not clear (without writing some code) if this refers to the reviews in the dataset.</li> <li><code>Additional_Number_of_Scoring</code></li> <li>This means a review score was given but no positive or negative review was written by the reviewer</li> </ul> <p>Review columns</p> <ul> <li><code>Reviewer_Score</code></li> <li>This is a numerical value with at most 1 decimal place between the min and max values 2.5 and 10</li> <li>It is not explained why 2.5 is the lowest score possible</li> <li><code>Negative_Review</code></li> <li>If a reviewer wrote nothing, this field will have \"No Negative\"</li> <li>Note that a reviewer may write a positive review in the Negative review column (e.g. \"there is nothing bad about this hotel\")</li> <li><code>Review_Total_Negative_Word_Counts</code></li> <li>Higher negative word counts indicate a lower score (without checking the sentimentality)</li> <li><code>Positive_Review</code></li> <li>If a reviewer wrote nothing, this field will have \"No Positive\"</li> <li>Note that a reviewer may write a negative review in the Positive review column (e.g. \"there is nothing good about this hotel at all\")</li> <li><code>Review_Total_Positive_Word_Counts</code></li> <li>Higher positive word counts indicate a higher score (without checking the sentimentality)</li> <li><code>Review_Date</code> and <code>days_since_review</code></li> <li>A freshness or staleness measure might be applied to a review (older reviews might not be as accurate as newer ones because hotel management changed, or renovations have been done, or a pool was added etc.)</li> <li><code>Tags</code></li> <li>These are short descriptors that a reviewer may select to describe the type of guest they were (e.g. solo or family), the type of room they had, the length of stay and how the review was submitted. </li> <li>Unfortunately, using these tags is problematic, check the section below which discusses their usefulness</li> </ul> <p>Reviewer columns</p> <ul> <li><code>Total_Number_of_Reviews_Reviewer_Has_Given</code></li> <li>This might be a factor in a recommendation model, for instance, if you could determine that more prolific reviewers with hundreds of reviews were more likely to be negative rather than positive. However, the reviewer of any particular review is not identified with a unique code, and therefore cannot be linked to a set of reviews. There are 30 reviewers with 100 or more reviews, but it's hard to see how this can aid the recommendation model.</li> <li><code>Reviewer_Nationality</code></li> <li>Some people might think that certain nationalities are more likely to give a positive or negative review because of a national inclination. Be careful building such anecdotal views into your models. These are national (and sometimes racial) stereotypes, and each reviewer was an individual who wrote a review based on their experience. It may have been filtered through many lenses such as their previous hotel stays, the distance travelled, and their personal temperament. Thinking that their nationality was the reason for a review score is hard to justify.</li> </ul>"},{"location":"6-NLP/4-Hotel-Reviews-1/#examples","title":"Examples","text":"Average  Score Total Number   Reviews Reviewer   Score Negative Review Positive   Review Tags 7.8 1945 2.5 This is  currently not a hotel but a construction site I was terrorized from early  morning and all day with unacceptable building noise while resting after a  long trip and working in the room People were working all day i e with  jackhammers in the adjacent rooms I asked for a room change but no silent  room was available To make things worse I was overcharged I checked out in  the evening since I had to leave very early flight and received an appropriate  bill A day later the hotel made another charge without my consent in excess  of booked price It's a terrible place Don't punish yourself by booking  here Nothing  Terrible place Stay away Business trip                                Couple Standard Double  Room Stayed 2 nights <p>As you can see, this guest did not have a happy stay at this hotel. The hotel has a good average score of 7.8 and 1945 reviews, but this reviewer gave it 2.5 and wrote 115 words about how negative their stay was. If they wrote nothing at all in the Positive_Review column, you might surmise there was nothing positive, but alas they wrote 7 words of warning. If we just counted words instead of the meaning, or sentiment of the words, we might have a skewed view of the reviewer's intent. Strangely, their score of 2.5 is confusing, because if that hotel stay was so bad, why give it any points at all? Investigating the dataset closely, you'll see that the lowest possible score is 2.5, not 0. The highest possible score is 10.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#tags","title":"Tags","text":"<p>As mentioned above, at first glance, the idea to use <code>Tags</code> to categorize the data makes sense. Unfortunately these tags are not standardized, which means that in a given hotel, the options might be Single room, Twin room, and Double room, but in the next hotel, they are Deluxe Single Room, Classic Queen Room, and Executive King Room. These might be the same things, but there are so many variations that the choice becomes:</p> <ol> <li> <p>Attempt to change all terms to a single standard, which is very difficult, because it is not clear what the conversion path would be in each case (e.g. Classic single room maps to Single room but Superior Queen Room with Courtyard Garden or City View is much harder to map)</p> </li> <li> <p>We can take an NLP approach and measure the frequency of certain terms like Solo, Business Traveller, or Family with young kids as they apply to each hotel, and factor that into the recommendation  </p> </li> </ol> <p>Tags are usually (but not always) a single field containing a list of 5 to 6 comma separated values aligning to Type of trip, Type of guests, Type of room, Number of nights, and Type of device review was submitted on. However, because some reviewers don't fill in each field (they might leave one blank), the values are not always in the same order.</p> <p>As an example, take Type of group. There are 1025 unique possibilities in this field in the <code>Tags</code> column, and unfortunately only some of them refer to a group (some are the type of room etc.). If you filter only the ones that mention family, the results contain many Family room type results. If you include the term with, i.e. count the Family with values, the results are better, with over 80,000 of the 515,000 results containing the phrase \"Family with young children\" or \"Family with older children\".</p> <p>This means the tags column is not completely useless to us, but it will take some work to make it useful.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#average-hotel-score","title":"Average hotel score","text":"<p>There are a number of oddities or discrepancies with the data set that I can't figure out, but are illustrated here so you are aware of them when building your models. If you figure it out, please let us know in the discussion section!</p> <p>The dataset has the following columns relating to the average score and number of reviews: </p> <ol> <li>Hotel_Name</li> <li>Additional_Number_of_Scoring</li> <li>Average_Score</li> <li>Total_Number_of_Reviews</li> <li>Reviewer_Score  </li> </ol> <p>The single hotel with the most reviews in this dataset is Britannia International Hotel Canary Wharf with 4789 reviews out of 515,000. But if we look at the <code>Total_Number_of_Reviews</code> value for this hotel, it is 9086. You might surmise that there are many more scores without reviews, so perhaps we should add in the <code>Additional_Number_of_Scoring</code> column value. That value is 2682, and adding it to 4789 gets us 7,471 which is still 1615 short of the <code>Total_Number_of_Reviews</code>. </p> <p>If you take the <code>Average_Score</code> columns, you might surmise it is the average of the reviews in the dataset, but the description from Kaggle is \"Average Score of the hotel, calculated based on the latest comment in the last year\". That doesn't seem that useful, but we can calculate our own average based on the reviews scores in the data set. Using the same hotel as an example, the average hotel score is given as 7.1 but the calculated score (average reviewer score in the dataset) is 6.8. This is close, but not the same value, and we can only guess that the scores given in the <code>Additional_Number_of_Scoring</code> reviews increased the average to 7.1. Unfortunately with no way to test or prove that assertion, it is difficult to use or trust <code>Average_Score</code>, <code>Additional_Number_of_Scoring</code> and <code>Total_Number_of_Reviews</code> when they are based on, or refer to, data we do not have.</p> <p>To complicate things further, the hotel with the second highest number of reviews has a calculated average score of 8.12 and the dataset <code>Average_Score</code> is 8.1. Is this correct score a coincidence or is the first hotel a discrepancy? </p> <p>On the possibility that these hotel might be an outlier, and that maybe most of the values tally up (but some do not for some reason) we will write a short program next to explore the values in the dataset and determine the correct usage (or non-usage) of the values.</p> <p>\ud83d\udea8 A note of caution</p> <p>When working with this dataset you will write code that calculates something from the text without having to read or analyse the text yourself. This is the essence of NLP, interpreting meaning or sentiment without having to have a human do it. However, it is possible that you will read some of the negative reviews. I would urge you not to, because you don't have to. Some of them are silly, or irrelevant negative hotel reviews, such as  \"The weather wasn't great\", something beyond the control of the hotel, or indeed, anyone. But there is a dark side to some reviews too. Sometimes the negative reviews are racist, sexist, or ageist. This is unfortunate but to be expected in a dataset scraped off a public website. Some reviewers leave reviews that you would find distasteful, uncomfortable, or upsetting. Better to let the code measure the sentiment than read them yourself and be upset. That said, it is a minority that write such things, but they exist all the same. </p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#exercise-data-exploration","title":"Exercise -  Data exploration","text":""},{"location":"6-NLP/4-Hotel-Reviews-1/#load-the-data","title":"Load the data","text":"<p>That's enough examining the data visually, now you'll write some code and get some answers! This section uses the pandas library. Your very first task is to ensure you can load and read the CSV data. The pandas library has a fast CSV loader, and the result is placed in a dataframe, as in previous lessons. The CSV we are loading has over half a million rows, but only 17 columns. Pandas gives you lots of powerful ways to interact with a dataframe, including the ability to perform operations on every row. </p> <p>From here on in this lesson, there will be code snippets and some explanations of the code and some discussion about what the results mean. Use the included notebook.ipynb for your code.</p> <p>Let's start with loading the data file you be using:</p> <pre><code># Load the hotel reviews from CSV\nimport pandas as pd\nimport time\n# importing time so the start and end time can be used to calculate file loading time\nprint(\"Loading data file now, this could take a while depending on file size\")\nstart = time.time()\n# df is 'DataFrame' - make sure you downloaded the file to the data folder\ndf = pd.read_csv('../../data/Hotel_Reviews.csv')\nend = time.time()\nprint(\"Loading took \" + str(round(end - start, 2)) + \" seconds\")\n</code></pre> <p>Now that the data is loaded, we can perform some operations on it. Keep this code at the top of your program for the next part.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#explore-the-data","title":"Explore the data","text":"<p>In this case, the data is already clean, that means that it is ready to work with, and does not have characters in other languages that might trip up algorithms expecting only English characters. </p> <p>\u2705 You might have to work with data that required some initial processing to format it before applying NLP techniques, but not this time. If you had to, how would you handle non-English characters?</p> <p>Take a moment to ensure that once the data is loaded, you can explore it with code. It's very easy to want to focus on the <code>Negative_Review</code> and <code>Positive_Review</code> columns. They are filled with natural text for your NLP algorithms to process. But wait! Before you jump into the NLP and sentiment, you should follow the code below to ascertain if the values given in the dataset match the values you calculate with pandas.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#dataframe-operations","title":"Dataframe operations","text":"<p>The first task in this lesson is to check if the following assertions are correct by writing some code that examines the data frame (without changing it).</p> <p>Like many programming tasks, there are several ways to complete this, but good advice is to do it in the simplest, easiest way you can, especially if it will be easier to understand when you come back to this code in the future. With dataframes, there is a comprehensive API that will often have a way to do what you want efficiently.</p> <p>Treat the following questions as coding tasks and attempt to answer them without looking at the solution. </p> <ol> <li>Print out the shape of the data frame you have just loaded (the shape is the number of rows and columns)</li> <li>Calculate the frequency count for reviewer nationalities:</li> <li>How many distinct values are there for the column <code>Reviewer_Nationality</code> and what are they?</li> <li>What reviewer nationality is the most common in the dataset (print country and number of reviews)?</li> <li>What are the next top 10 most frequently found nationalities, and their frequency count?</li> <li>What was the most frequently reviewed hotel for each of the top 10 most reviewer nationalities?</li> <li>How many reviews are there per hotel (frequency count of hotel) in the dataset?</li> <li>While there is an <code>Average_Score</code> column for each hotel in the dataset, you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel). Add a new column to your dataframe with the column header <code>Calc_Average_Score</code> that contains that calculated average. </li> <li>Do any hotels have the same (rounded to 1 decimal place) <code>Average_Score</code> and <code>Calc_Average_Score</code>?</li> <li>Try writing a Python function that takes a Series (row) as an argument and compares the values, printing out a message when the values are not equal. Then use the <code>.apply()</code> method to process every row with the function.</li> <li>Calculate and print out how many rows have column <code>Negative_Review</code> values of \"No Negative\" </li> <li>Calculate and print out how many rows have column <code>Positive_Review</code> values of \"No Positive\"</li> <li>Calculate and print out how many rows have column <code>Positive_Review</code> values of \"No Positive\" and <code>Negative_Review</code> values of \"No Negative\"</li> </ol>"},{"location":"6-NLP/4-Hotel-Reviews-1/#code-answers","title":"Code answers","text":"<ol> <li>Print out the shape of the data frame you have just loaded (the shape is the number of rows and columns)</li> </ol> <pre><code>print(\"The shape of the data (rows, cols) is \" + str(df.shape))\n&gt; The shape of the data (rows, cols) is (515738, 17)\n</code></pre> <ol> <li> <p>Calculate the frequency count for reviewer nationalities:</p> </li> <li> <p>How many distinct values are there for the column <code>Reviewer_Nationality</code> and what are they?</p> </li> <li>What reviewer nationality is the most common in the dataset (print country and number of reviews)?</li> </ol> <pre><code># value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality\nnationality_freq = df[\"Reviewer_Nationality\"].value_counts()\nprint(\"There are \" + str(nationality_freq.size) + \" different nationalities\")\n# print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data\nprint(nationality_freq) \n\nThere are 227 different nationalities\n United Kingdom               245246\n United States of America      35437\n Australia                     21686\n Ireland                       14827\n United Arab Emirates          10235\n                               ...  \n Comoros                           1\n Palau                             1\n Northern Mariana Islands          1\n Cape Verde                        1\n Guinea                            1\nName: Reviewer_Nationality, Length: 227, dtype: int64\n</code></pre> <ol> <li> <p>What are the next top 10 most frequently found nationalities, and their frequency count?</p> <pre><code>print(\"The highest frequency reviewer nationality is \" + str(nationality_freq.index[0]).strip() + \" with \" + str(nationality_freq[0]) + \" reviews.\")\n# Notice there is a leading space on the values, strip() removes that for printing\n# What is the top 10 most common nationalities and their frequencies?\nprint(\"The next 10 highest frequency reviewer nationalities are:\")\nprint(nationality_freq[1:11].to_string())\n\nThe highest frequency reviewer nationality is United Kingdom with 245246 reviews.\nThe next 10 highest frequency reviewer nationalities are:\n United States of America     35437\n Australia                    21686\n Ireland                      14827\n United Arab Emirates         10235\n Saudi Arabia                  8951\n Netherlands                   8772\n Switzerland                   8678\n Germany                       7941\n Canada                        7894\n France                        7296\n</code></pre> </li> <li> <p>What was the most frequently reviewed hotel for each of the top 10 most reviewer nationalities?</p> </li> </ol> <pre><code># What was the most frequently reviewed hotel for the top 10 nationalities\n# Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)\nfor nat in nationality_freq[:10].index:\n   # First, extract all the rows that match the criteria into a new dataframe\n   nat_df = df[df[\"Reviewer_Nationality\"] == nat]   \n   # Now get the hotel freq\n   freq = nat_df[\"Hotel_Name\"].value_counts()\n   print(\"The most reviewed hotel for \" + str(nat).strip() + \" was \" + str(freq.index[0]) + \" with \" + str(freq[0]) + \" reviews.\") \n\nThe most reviewed hotel for United Kingdom was Britannia International Hotel Canary Wharf with 3833 reviews.\nThe most reviewed hotel for United States of America was Hotel Esther a with 423 reviews.\nThe most reviewed hotel for Australia was Park Plaza Westminster Bridge London with 167 reviews.\nThe most reviewed hotel for Ireland was Copthorne Tara Hotel London Kensington with 239 reviews.\nThe most reviewed hotel for United Arab Emirates was Millennium Hotel London Knightsbridge with 129 reviews.\nThe most reviewed hotel for Saudi Arabia was The Cumberland A Guoman Hotel with 142 reviews.\nThe most reviewed hotel for Netherlands was Jaz Amsterdam with 97 reviews.\nThe most reviewed hotel for Switzerland was Hotel Da Vinci with 97 reviews.\nThe most reviewed hotel for Germany was Hotel Da Vinci with 86 reviews.\nThe most reviewed hotel for Canada was St James Court A Taj Hotel London with 61 reviews.\n</code></pre> <ol> <li>How many reviews are there per hotel (frequency count of hotel) in the dataset?</li> </ol> <p><pre><code># First create a new dataframe based on the old one, removing the uneeded columns\nhotel_freq_df = df.drop([\"Hotel_Address\", \"Additional_Number_of_Scoring\", \"Review_Date\", \"Average_Score\", \"Reviewer_Nationality\", \"Negative_Review\", \"Review_Total_Negative_Word_Counts\", \"Positive_Review\", \"Review_Total_Positive_Word_Counts\", \"Total_Number_of_Reviews_Reviewer_Has_Given\", \"Reviewer_Score\", \"Tags\", \"days_since_review\", \"lat\", \"lng\"], axis = 1)\n\n# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found\nhotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')\n\n# Get rid of all the duplicated rows\nhotel_freq_df = hotel_freq_df.drop_duplicates(subset = [\"Hotel_Name\"])\ndisplay(hotel_freq_df) \n</code></pre>    |                 Hotel_Name                 | Total_Number_of_Reviews | Total_Reviews_Found |    | :----------------------------------------: | :---------------------: | :-----------------: |    | Britannia International Hotel Canary Wharf |          9086           |        4789         |    |    Park Plaza Westminster Bridge London    |          12158          |        4169         |    |   Copthorne Tara Hotel London Kensington   |          7105           |        3578         |    |                    ...                     |           ...           |         ...         |    |       Mercure Paris Porte d Orleans        |           110           |         10          |    |                Hotel Wagner                |           135           |         10          |    |            Hotel Gallitzinberg             |           173           |          8          |</p> <p>You may notice that the counted in the dataset results do not match the value in <code>Total_Number_of_Reviews</code>. It is unclear if this value in the dataset represented the total number of reviews the hotel had, but not all were scraped, or some other calculation. <code>Total_Number_of_Reviews</code> is not used in the model because of this unclarity.</p> <ol> <li>While there is an <code>Average_Score</code> column for each hotel in the dataset, you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel). Add a new column to your dataframe with the column header <code>Calc_Average_Score</code> that contains that calculated average. Print out the columns <code>Hotel_Name</code>, <code>Average_Score</code>, and <code>Calc_Average_Score</code>.</li> </ol> <pre><code># define a function that takes a row and performs some calculation with it\ndef get_difference_review_avg(row):\n  return row[\"Average_Score\"] - row[\"Calc_Average_Score\"]\n\n# 'mean' is mathematical word for 'average'\ndf['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n\n# Add a new column with the difference between the two average scores\ndf[\"Average_Score_Difference\"] = df.apply(get_difference_review_avg, axis = 1)\n\n# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)\nreview_scores_df = df.drop_duplicates(subset = [\"Hotel_Name\"])\n\n# Sort the dataframe to find the lowest and highest average score difference\nreview_scores_df = review_scores_df.sort_values(by=[\"Average_Score_Difference\"])\n\ndisplay(review_scores_df[[\"Average_Score_Difference\", \"Average_Score\", \"Calc_Average_Score\", \"Hotel_Name\"]])\n</code></pre> <p>You may also wonder about the <code>Average_Score</code> value and why it is sometimes different from the calculated average score. As we can't know why some of the values match, but others have a difference, it's safest in this case to use the review scores that we have to calculate the average ourselves. That said, the differences are usually very small, here are the hotels with the greatest deviation from the dataset average and the calculated average:</p> Average_Score_Difference Average_Score Calc_Average_Score Hotel_Name -0.8 7.7 8.5 Best Western Hotel Astoria -0.7 8.8 9.5 Hotel Stendhal Place Vend me Paris MGallery -0.7 7.5 8.2 Mercure Paris Porte d Orleans -0.7 7.9 8.6 Renaissance Paris Vendome Hotel -0.5 7.0 7.5 Hotel Royal Elys es ... ... ... ... 0.7 7.5 6.8 Mercure Paris Op ra Faubourg Montmartre 0.8 7.1 6.3 Holiday Inn Paris Montparnasse Pasteur 0.9 6.8 5.9 Villa Eugenie 0.9 8.6 7.7 MARQUIS Faubourg St Honor Relais Ch teaux 1.3 7.2 5.9 Kube Hotel Ice Bar <p>With only 1 hotel having a difference of score greater than 1, it means we can probably ignore the difference and use the calculated average score.</p> <ol> <li> <p>Calculate and print out how many rows have column <code>Negative_Review</code> values of \"No Negative\" </p> </li> <li> <p>Calculate and print out how many rows have column <code>Positive_Review</code> values of \"No Positive\"</p> </li> <li> <p>Calculate and print out how many rows have column <code>Positive_Review</code> values of \"No Positive\" and <code>Negative_Review</code> values of \"No Negative\"</p> </li> </ol> <pre><code># with lambdas:\nstart = time.time()\nno_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == \"No Negative\" else False , axis=1)\nprint(\"Number of No Negative reviews: \" + str(len(no_negative_reviews[no_negative_reviews == True].index)))\n\nno_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == \"No Positive\" else False , axis=1)\nprint(\"Number of No Positive reviews: \" + str(len(no_positive_reviews[no_positive_reviews == True].index)))\n\nboth_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == \"No Negative\" and x['Positive_Review'] == \"No Positive\" else False , axis=1)\nprint(\"Number of both No Negative and No Positive reviews: \" + str(len(both_no_reviews[both_no_reviews == True].index)))\nend = time.time()\nprint(\"Lambdas took \" + str(round(end - start, 2)) + \" seconds\")\n\nNumber of No Negative reviews: 127890\nNumber of No Positive reviews: 35946\nNumber of both No Negative and No Positive reviews: 127\nLambdas took 9.64 seconds\n</code></pre>"},{"location":"6-NLP/4-Hotel-Reviews-1/#another-way","title":"Another way","text":"<p>Another way count items without Lambdas, and use sum to count the rows:</p> <pre><code># without lambdas (using a mixture of notations to show you can use both)\nstart = time.time()\nno_negative_reviews = sum(df.Negative_Review == \"No Negative\")\nprint(\"Number of No Negative reviews: \" + str(no_negative_reviews))\n\nno_positive_reviews = sum(df[\"Positive_Review\"] == \"No Positive\")\nprint(\"Number of No Positive reviews: \" + str(no_positive_reviews))\n\nboth_no_reviews = sum((df.Negative_Review == \"No Negative\") &amp; (df.Positive_Review == \"No Positive\"))\nprint(\"Number of both No Negative and No Positive reviews: \" + str(both_no_reviews))\n\nend = time.time()\nprint(\"Sum took \" + str(round(end - start, 2)) + \" seconds\")\n\nNumber of No Negative reviews: 127890\nNumber of No Positive reviews: 35946\nNumber of both No Negative and No Positive reviews: 127\nSum took 0.19 seconds\n</code></pre> <p>You may have noticed that there are 127 rows that have both \"No Negative\" and \"No Positive\" values for the columns <code>Negative_Review</code> and <code>Positive_Review</code> respectively. That means that the reviewer gave the hotel a numerical score, but declined to write either a positive or negative review. Luckily this is a small amount of rows (127 out of 515738, or 0.02%), so it probably won't skew our model or results in any particular direction, but you might not have expected a data set of reviews to have rows with no reviews, so it's worth exploring the data to discover rows like this.</p> <p>Now that you have explored the dataset, in the next lesson you will filter the data and add some sentiment analysis.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#challenge","title":"\ud83d\ude80Challenge","text":"<p>This lesson demonstrates, as we saw in previous lessons, how critically important it is to understand your data and its foibles before performing operations on it. Text-based data, in particular, bears careful scrutiny. Dig through various text-heavy datasets and see if you can discover areas that could introduce bias or skewed sentiment into a model. </p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"6-NLP/4-Hotel-Reviews-1/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take this Learning Path on NLP to discover tools to try when building speech and text-heavy models.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/#assignment","title":"Assignment","text":"<p>NLTK</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/assignment/","title":"NLTK","text":""},{"location":"6-NLP/4-Hotel-Reviews-1/assignment/#instructions","title":"Instructions","text":"<p>NLTK is a well-known library for use in computational linguistics and NLP. Take this opportunity to read through the 'NLTK book' and try out its exercises. In this ungraded assignment, you will get to know this library more deeply.</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"6-NLP/4-Hotel-Reviews-1/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/","title":"Sentiment analysis with hotel reviews","text":"<p>Now that you have explored the dataset in detail, it's time to filter the columns and then use NLP techniques on the dataset to gain new insights about the hotels.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"6-NLP/5-Hotel-Reviews-2/#filtering-sentiment-analysis-operations","title":"Filtering &amp; Sentiment Analysis Operations","text":"<p>As you've probably noticed, the dataset has a few issues. Some columns are filled with useless information, others seem incorrect. If they are correct, it's unclear how they were calculated, and answers cannot be independently verified by your own calculations.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#exercise-a-bit-more-data-processing","title":"Exercise: a bit more data processing","text":"<p>Clean the data just a bit more. Add columns that will be useful later, change the values in other columns, and drop certain columns completely.</p> <ol> <li> <p>Initial column processing</p> </li> <li> <p>Drop <code>lat</code> and <code>lng</code></p> </li> <li> <p>Replace <code>Hotel_Address</code> values with the following values (if the address contains the same of the city and the country, change it to just the city and the country).</p> <p>These are the only cities and countries in the dataset:</p> <p>Amsterdam, Netherlands</p> <p>Barcelona, Spain</p> <p>London, United Kingdom</p> <p>Milan, Italy</p> <p>Paris, France</p> <p>Vienna, Austria </p> <pre><code>def replace_address(row):\n    if \"Netherlands\" in row[\"Hotel_Address\"]:\n        return \"Amsterdam, Netherlands\"\n    elif \"Barcelona\" in row[\"Hotel_Address\"]:\n        return \"Barcelona, Spain\"\n    elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n        return \"London, United Kingdom\"\n    elif \"Milan\" in row[\"Hotel_Address\"]:        \n        return \"Milan, Italy\"\n    elif \"France\" in row[\"Hotel_Address\"]:\n        return \"Paris, France\"\n    elif \"Vienna\" in row[\"Hotel_Address\"]:\n        return \"Vienna, Austria\" \n\n# Replace all the addresses with a shortened, more useful form\ndf[\"Hotel_Address\"] = df.apply(replace_address, axis = 1)\n# The sum of the value_counts() should add up to the total number of reviews\nprint(df[\"Hotel_Address\"].value_counts())\n</code></pre> <p>Now you can query country level data:</p> <pre><code>display(df.groupby(\"Hotel_Address\").agg({\"Hotel_Name\": \"nunique\"}))\n</code></pre> Hotel_Address Hotel_Name Amsterdam, Netherlands 105 Barcelona, Spain 211 London, United Kingdom 400 Milan, Italy 162 Paris, France 458 Vienna, Austria 158 </li> <li> <p>Process Hotel Meta-review columns</p> </li> <li> <p>Drop <code>Additional_Number_of_Scoring</code></p> </li> <li> <p>Replace <code>Total_Number_of_Reviews</code> with the total number of reviews for that hotel that are actually in the dataset </p> </li> <li> <p>Replace <code>Average_Score</code> with our own calculated score</p> </li> </ol> <pre><code># Drop `Additional_Number_of_Scoring`\ndf.drop([\"Additional_Number_of_Scoring\"], axis = 1, inplace=True)\n# Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values\ndf.Total_Number_of_Reviews = df.groupby('Hotel_Name').transform('count')\ndf.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n</code></pre> <ol> <li> <p>Process review columns</p> </li> <li> <p>Drop <code>Review_Total_Negative_Word_Counts</code>, <code>Review_Total_Positive_Word_Counts</code>, <code>Review_Date</code> and <code>days_since_review</code></p> </li> <li> <p>Keep <code>Reviewer_Score</code>, <code>Negative_Review</code>, and <code>Positive_Review</code> as they are,</p> </li> <li> <p>Keep <code>Tags</code> for now</p> <ul> <li>We'll be doing some additional filtering operations on the tags in the next section and then tags will be dropped</li> </ul> </li> <li> <p>Process reviewer columns</p> </li> <li> <p>Drop <code>Total_Number_of_Reviews_Reviewer_Has_Given</code></p> </li> <li> <p>Keep <code>Reviewer_Nationality</code></p> </li> </ol>"},{"location":"6-NLP/5-Hotel-Reviews-2/#tag-columns","title":"Tag columns","text":"<p>The <code>Tag</code> column is problematic as it is a list (in text form) stored in the column. Unfortunately the order and number of sub sections in this column are not always the same. It's hard for a human to identify the correct phrases to be interested in, because there are 515,000 rows, and 1427 hotels, and each has slightly different options a reviewer could choose. This is where NLP shines. You can scan the text and find the most common phrases, and count them.</p> <p>Unfortunately, we are not interested in single words, but multi-word phrases (e.g. Business trip). Running a multi-word frequency distribution algorithm on that much data (6762646 words) could take an extraordinary amount of time, but without looking at the data, it would seem that is a necessary expense. This is where exploratory data analysis comes in useful, because you've seen a sample of the tags such as <code>[' Business trip  ', ' Solo traveler ', ' Single Room ', ' Stayed 5 nights ', ' Submitted from  a mobile device ']</code> , you can begin to ask if it's possible to greatly reduce the processing you have to do. Luckily, it is - but first you need to follow a few steps to ascertain the tags of interest.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#filtering-tags","title":"Filtering tags","text":"<p>Remember that the goal of the dataset is to add sentiment and columns that will help you choose the best hotel (for yourself or maybe a client tasking you to make a hotel recommendation bot). You need to ask yourself if the tags are useful or not in the final dataset. Here is one interpretation (if you needed the dataset for other reasons different tags might stay in/out of the selection):</p> <ol> <li>The type of trip is relevant, and that should stay</li> <li>The type of guest group is important, and that should stay</li> <li>The type of room, suite, or studio that the guest stayed in is irrelevant (all hotels have basically the same rooms)</li> <li>The device the review was submitted on is irrelevant</li> <li>The number of nights reviewer stayed for could be relevant if you attributed longer stays with them liking the hotel more, but it's a stretch, and probably irrelevant</li> </ol> <p>In summary, keep 2 kinds of tags and remove the others.</p> <p>First, you don't want to count the tags until they are in a better format, so that means removing the square brackets and quotes. You can do this several ways, but you want the fastest as it could take a long time to process a lot of data. Luckily, pandas has an easy way to do each of these steps.</p> <pre><code># Remove opening and closing brackets\ndf.Tags = df.Tags.str.strip(\"[']\")\n# remove all quotes too\ndf.Tags = df.Tags.str.replace(\" ', '\", \",\", regex = False)\n</code></pre> <p>Each tag becomes something like: <code>Business trip, Solo traveler, Single Room, Stayed 5 nights, Submitted from a mobile device</code>. </p> <p>Next we find a problem. Some reviews, or rows, have 5 columns, some 3, some 6. This is a result of how the dataset was created, and hard to fix. You want to get a frequency count of each phrase, but they are in different order in each review, so the count might be off, and a hotel might not get a tag assigned to it that it deserved.</p> <p>Instead you will use the different order to our advantage, because each tag is multi-word but also separated by a comma! The simplest way to do this is to create 6 temporary columns with each tag inserted in to the column corresponding to its order in the tag. You can then merge the 6 columns into one big column and run the <code>value_counts()</code> method on the resulting column. Printing that out, you'll see there was 2428 unique tags. Here is a small sample:</p> Tag Count Leisure trip 417778 Submitted from a mobile device 307640 Couple 252294 Stayed 1 night 193645 Stayed 2 nights 133937 Solo traveler 108545 Stayed 3 nights 95821 Business trip 82939 Group 65392 Family with young children 61015 Stayed 4 nights 47817 Double Room 35207 Standard Double Room 32248 Superior Double Room 31393 Family with older children 26349 Deluxe Double Room 24823 Double or Twin Room 22393 Stayed 5 nights 20845 Standard Double or Twin Room 17483 Classic Double Room 16989 Superior Double or Twin Room 13570 2 rooms 12393 <p>Some of the common tags like <code>Submitted from a mobile device</code> are of no use to us, so it might be a smart thing to remove them before counting phrase occurrence, but it is such a fast operation you can leave them in and ignore them.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#removing-the-length-of-stay-tags","title":"Removing the length of stay tags","text":"<p>Removing these tags is step 1, it reduces the total number of tags to be considered slightly. Note you do not remove them from the dataset, just choose to remove them from consideration as values to  count/keep in the reviews dataset.</p> Length of stay Count Stayed 1 night 193645 Stayed  2 nights 133937 Stayed 3 nights 95821 Stayed  4 nights 47817 Stayed 5 nights 20845 Stayed  6 nights 9776 Stayed 7 nights 7399 Stayed  8 nights 2502 Stayed 9 nights 1293 ... ... <p>There are a huge variety of rooms, suites, studios, apartments and so on. They all mean roughly the same thing and not relevant to you, so remove them from consideration.</p> Type of room Count Double Room 35207 Standard  Double Room 32248 Superior Double Room 31393 Deluxe  Double Room 24823 Double or Twin Room 22393 Standard  Double or Twin Room 17483 Classic Double Room 16989 Superior  Double or Twin Room 13570 <p>Finally, and this is delightful (because it didn't take much processing at all), you will be left with the following useful tags:</p> Tag Count Leisure trip 417778 Couple 252294 Solo  traveler 108545 Business trip 82939 Group (combined with Travellers with friends) 67535 Family with young children 61015 Family  with older children 26349 With a  pet 1405 <p>You could argue that <code>Travellers with friends</code> is the same as <code>Group</code> more or less, and that would be fair to combine the two as above. The code for identifying the correct tags is the Tags notebook.</p> <p>The final step is to create new columns for each of these tags. Then, for every review row, if the <code>Tag</code> column matches one of the new columns, add a 1, if not, add a 0. The end result will be a count of how many reviewers chose this hotel (in aggregate) for, say, business vs leisure, or to bring a pet to, and this is useful information when recommending a hotel.</p> <pre><code># Process the Tags into new columns\n# The file Hotel_Reviews_Tags.py, identifies the most important tags\n# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, \n# Family with young children, Family with older children, With a pet\ndf[\"Leisure_trip\"] = df.Tags.apply(lambda tag: 1 if \"Leisure trip\" in tag else 0)\ndf[\"Couple\"] = df.Tags.apply(lambda tag: 1 if \"Couple\" in tag else 0)\ndf[\"Solo_traveler\"] = df.Tags.apply(lambda tag: 1 if \"Solo traveler\" in tag else 0)\ndf[\"Business_trip\"] = df.Tags.apply(lambda tag: 1 if \"Business trip\" in tag else 0)\ndf[\"Group\"] = df.Tags.apply(lambda tag: 1 if \"Group\" in tag or \"Travelers with friends\" in tag else 0)\ndf[\"Family_with_young_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with young children\" in tag else 0)\ndf[\"Family_with_older_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with older children\" in tag else 0)\ndf[\"With_a_pet\"] = df.Tags.apply(lambda tag: 1 if \"With a pet\" in tag else 0)\n</code></pre>"},{"location":"6-NLP/5-Hotel-Reviews-2/#save-your-file","title":"Save your file","text":"<p>Finally, save the dataset as it is now with a new name.</p> <pre><code>df.drop([\"Review_Total_Negative_Word_Counts\", \"Review_Total_Positive_Word_Counts\", \"days_since_review\", \"Total_Number_of_Reviews_Reviewer_Has_Given\"], axis = 1, inplace=True)\n\n# Saving new data file with calculated columns\nprint(\"Saving results to Hotel_Reviews_Filtered.csv\")\ndf.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)\n</code></pre>"},{"location":"6-NLP/5-Hotel-Reviews-2/#sentiment-analysis-operations","title":"Sentiment Analysis Operations","text":"<p>In this final section, you will apply sentiment analysis to the review columns and save the results in a dataset.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#exercise-load-and-save-the-filtered-data","title":"Exercise: load and save the filtered data","text":"<p>Note that now you are loading the filtered dataset that was saved in the previous section, not the original dataset.</p> <pre><code>import time\nimport pandas as pd\nimport nltk as nltk\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\n# Load the filtered hotel reviews from CSV\ndf = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')\n\n# You code will be added here\n\n\n# Finally remember to save the hotel reviews with new NLP data added\nprint(\"Saving results to Hotel_Reviews_NLP.csv\")\ndf.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)\n</code></pre>"},{"location":"6-NLP/5-Hotel-Reviews-2/#removing-stop-words","title":"Removing stop words","text":"<p>If you were to run Sentiment Analysis on the Negative and Positive review columns, it could take a long time. Tested on a powerful test laptop with fast CPU,it took 12 - 14 minutes depending on which sentiment library was used. That's a (relatively) long time, so worth investigating if that can be speeded up. </p> <p>Removing stop words, or common English words that do not change the sentiment of a sentence, is the first step. By removing them, the sentiment analysis should run faster, but not be less accurate (as the stop words do not affect sentiment, but they do slow down the analysis). </p> <p>The longest negative review was 395 words, but after removing the stop words, it is 195 words.</p> <p>Removing the stop words is also a fast operation, removing the stop words from 2 review columns over 515,000 rows took 3.3 seconds on the test device. It could take slightly more or less time for you depending on your device CPU speed, RAM, whether you have an SSD or not, and some other factors. The relative shortness of the operation means that if it improves the sentiment analysis time, then it is worth doing.</p> <pre><code>from nltk.corpus import stopwords\n\n# Load the hotel reviews from CSV\ndf = pd.read_csv(\"../../data/Hotel_Reviews_Filtered.csv\")\n\n# Remove stop words - can be slow for a lot of text!\n# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches\n# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends\nstart = time.time()\ncache = set(stopwords.words(\"english\"))\ndef remove_stopwords(review):\n    text = \" \".join([word for word in review.split() if word not in cache])\n    return text\n\n# Remove the stop words from both columns\ndf.Negative_Review = df.Negative_Review.apply(remove_stopwords)   \ndf.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n</code></pre>"},{"location":"6-NLP/5-Hotel-Reviews-2/#performing-sentiment-analysis","title":"Performing sentiment analysis","text":"<p>Now you should calculate the sentiment analysis for both negative and positive review columns, and store the result in 2 new columns. The test of the sentiment will be to compare it to the reviewer's score for the same review. For instance, if the sentiment thinks the negative review had a sentiment of 1 (extremely positive sentiment) and a positive review sentiment of 1, but the reviewer gave the hotel the lowest score possible, then either the review text doesn't match the score, or the sentiment analyser could not recognize the sentiment correctly. You should expect some sentiment scores to be completely wrong, and often that will be explainable, e.g. the review could be extremely sarcastic \"Of course I LOVED sleeping in a room with no heating\" and the sentiment analyser thinks that's positive sentiment, even though a human reading it would know it was sarcasm. </p> <p>NLTK supplies different sentiment analyzers to learn with, and you can substitute them and see if the sentiment is more or less accurate. The VADER sentiment analysis is used here.</p> <p>Hutto, C.J. &amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.</p> <pre><code>from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Create the vader sentiment analyser (there are others in NLTK you can try too)\nvader_sentiment = SentimentIntensityAnalyzer()\n# Hutto, C.J. &amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n\n# There are 3 possibilities of input for a review:\n# It could be \"No Negative\", in which case, return 0\n# It could be \"No Positive\", in which case, return 0\n# It could be a review, in which case calculate the sentiment\ndef calc_sentiment(review):    \n    if review == \"No Negative\" or review == \"No Positive\":\n        return 0\n    return vader_sentiment.polarity_scores(review)[\"compound\"]    \n</code></pre> <p>Later in your program when you are ready to calculate sentiment, you can apply it to each review as follows:</p> <pre><code># Add a negative sentiment and positive sentiment column\nprint(\"Calculating sentiment columns for both positive and negative reviews\")\nstart = time.time()\ndf[\"Negative_Sentiment\"] = df.Negative_Review.apply(calc_sentiment)\ndf[\"Positive_Sentiment\"] = df.Positive_Review.apply(calc_sentiment)\nend = time.time()\nprint(\"Calculating sentiment took \" + str(round(end - start, 2)) + \" seconds\")\n</code></pre> <p>This takes approximately 120 seconds on my computer, but it will vary on each computer. If you want to print of the results and see if the sentiment matches the review:</p> <pre><code>df = df.sort_values(by=[\"Negative_Sentiment\"], ascending=True)\nprint(df[[\"Negative_Review\", \"Negative_Sentiment\"]])\ndf = df.sort_values(by=[\"Positive_Sentiment\"], ascending=True)\nprint(df[[\"Positive_Review\", \"Positive_Sentiment\"]])\n</code></pre> <p>The very last thing to do with the file before using it in the challenge, is to save it! You should also consider reordering all your new columns so they are easy to work with (for a human, it's a cosmetic change).</p> <pre><code># Reorder the columns (This is cosmetic, but to make it easier to explore the data later)\ndf = df.reindex([\"Hotel_Name\", \"Hotel_Address\", \"Total_Number_of_Reviews\", \"Average_Score\", \"Reviewer_Score\", \"Negative_Sentiment\", \"Positive_Sentiment\", \"Reviewer_Nationality\", \"Leisure_trip\", \"Couple\", \"Solo_traveler\", \"Business_trip\", \"Group\", \"Family_with_young_children\", \"Family_with_older_children\", \"With_a_pet\", \"Negative_Review\", \"Positive_Review\"], axis=1)\n\nprint(\"Saving results to Hotel_Reviews_NLP.csv\")\ndf.to_csv(r\"../data/Hotel_Reviews_NLP.csv\", index = False)\n</code></pre> <p>You should run the entire code for the analysis notebook (after you've run your filtering notebook to generate the Hotel_Reviews_Filtered.csv file).</p> <p>To review, the steps are:</p> <ol> <li>Original dataset file Hotel_Reviews.csv is explored in the previous lesson with the explorer notebook</li> <li>Hotel_Reviews.csv is filtered by the filtering notebook resulting in Hotel_Reviews_Filtered.csv</li> <li>Hotel_Reviews_Filtered.csv is processed by the sentiment analysis notebook resulting in Hotel_Reviews_NLP.csv</li> <li>Use Hotel_Reviews_NLP.csv in the NLP Challenge below</li> </ol>"},{"location":"6-NLP/5-Hotel-Reviews-2/#conclusion","title":"Conclusion","text":"<p>When you started, you had a dataset with columns and data but not all of it could be verified or used. You've explored the data, filtered out what you don't need, converted tags into something useful, calculated your own averages, added some sentiment columns and hopefully, learned some interesting things about processing natural text.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"6-NLP/5-Hotel-Reviews-2/#challenge","title":"Challenge","text":"<p>Now that you have your dataset analyzed for sentiment, see if you can use strategies you've learned in this curriculum (clustering, perhaps?) to determine patterns around sentiment. </p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#review-self-study","title":"Review &amp; Self Study","text":"<p>Take this Learn module to learn more and use different tools to explore sentiment in text.</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/#assignment","title":"Assignment","text":"<p>Try a different dataset</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/assignment/","title":"Try a different dataset","text":""},{"location":"6-NLP/5-Hotel-Reviews-2/assignment/#instructions","title":"Instructions","text":"<p>Now that you have learned about using NLTK to assign sentiment to text, try a different dataset. You will probably need to do some data processing around it, so create a notebook and document your thought process. What do you discover?</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A complete notebook and dataset are presented with well-documented cells explaining how the sentiment is assigned The notebook is missing good explanations The notebook is flawed"},{"location":"6-NLP/5-Hotel-Reviews-2/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"6-NLP/5-Hotel-Reviews-2/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"6-NLP/data/","title":"Index","text":"<p>Download the hotel review data to this folder.</p>"},{"location":"7-TimeSeries/","title":"Introduction to time series forecasting","text":"<p>What is time series forecasting? It's about predicting future events by analyzing trends of the past.</p>"},{"location":"7-TimeSeries/#regional-topic-worldwide-electricity-usage","title":"Regional topic: worldwide electricity usage \u2728","text":"<p>In these two lessons, you will be introduced to time series forecasting, a somewhat lesser known area of machine learning that is nevertheless extremely valuable for industry and business applications, among other fields. While neural networks can be used to enhance the utility of these models, we will study them in the context of classical machine learning as models help predict future performance based on the past.</p> <p>Our regional focus is electrical usage in the world, an interesting dataset to learn about forecasting future power usage based on patterns of past load. You can see how this kind of forecasting can be extremely helpful in a business environment.</p> <p></p> <p>Photo by Peddi Sai hrithik of electrical towers on a road in Rajasthan on Unsplash</p>"},{"location":"7-TimeSeries/#lessons","title":"Lessons","text":"<ol> <li>Introduction to time series forecasting</li> <li>Building ARIMA time series models</li> <li>Building Support Vector Regressor for time series forcasting</li> </ol>"},{"location":"7-TimeSeries/#credits","title":"Credits","text":"<p>\"Introduction to time series forecasting\" was written with \u26a1\ufe0f by Francesca Lazzeri and Jen Looper. The notebooks first appeared online in the Azure \"Deep Learning For Time Series\" repo originally written by Francesca Lazzeri. The SVR lesson was written by Anirban Mukherjee</p>"},{"location":"7-TimeSeries/README.zh-cn/","title":"\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b80\u4ecb","text":"<p>\u4ec0\u4e48\u662f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff1f\u5b83\u5c31\u662f\u901a\u8fc7\u5206\u6790\u8fc7\u53bb\u7684\u8d8b\u52bf\u6765\u9884\u6d4b\u672a\u6765\u7684\u4e8b\u4ef6\u3002</p>"},{"location":"7-TimeSeries/README.zh-cn/#_2","title":"\u533a\u57df\u4e3b\u9898\uff1a\u5168\u7403\u7528\u7535\u91cf\u2728","text":"<p>\u5728\u8fd9\u4e24\u8282\u8bfe\u4e2d\uff0c\u4f60\u5c06\u4e86\u89e3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u8fd9\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u9c9c\u4e3a\u4eba\u77e5\u7684\u9886\u57df\uff0c\u4f46\u5bf9\u5de5\u4e1a\u548c\u5546\u4e1a\u5e94\u7528\u7a0b\u5e8f\u4ee5\u53ca\u5176\u4ed6\u9886\u57df\u975e\u5e38\u6709\u4ef7\u503c\u3002\u867d\u7136\u795e\u7ecf\u7f51\u7edc\u53ef\u7528\u4e8e\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u6211\u4eec\u5c06\u5728\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u80cc\u666f\u4e0b\u7814\u7a76\u5b83\u4eec\uff0c\u56e0\u4e3a\u6a21\u578b\u6709\u52a9\u4e8e\u6839\u636e\u8fc7\u53bb\u9884\u6d4b\u672a\u6765\u7684\u8868\u73b0\u3002</p> <p>\u6211\u4eec\u7684\u91cd\u70b9\u662f\u4e16\u754c\u4e0a\u7684\u7528\u7535\u91cf\uff0c\u8fd9\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u6839\u636e\u8fc7\u53bb\u7684\u7528\u7535\u91cf\u8d1f\u8f7d\u6765\u9884\u6d4b\u672a\u6765\u7684\u7528\u7535\u91cf\u3002\u4f60\u53ef\u4ee5\u770b\u5230\u8fd9\u79cd\u9884\u6d4b\u5728\u5546\u4e1a\u73af\u5883\u4e2d\u975e\u5e38\u6709\u7528\u3002</p> <p></p> <p>Peddi Sai hrithik \u6444\u4e8e Unsplash \u3002</p>"},{"location":"7-TimeSeries/README.zh-cn/#_3","title":"\u8bfe\u7a0b","text":"<ol> <li>\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ecb\u7ecd</li> <li>\u6784\u5efa ARIMA \u65f6\u95f4\u5e8f\u5217\u6a21\u578b</li> <li>\u6784\u5efa\u652f\u6301\u5411\u91cf\u56de\u5f52\u5668\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b</li> </ol>"},{"location":"7-TimeSeries/README.zh-cn/#_4","title":"\u4f5c\u8005","text":"<p>\u201c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b80\u4ecb\u201d \u7531 Francesca Lazzeri \u548c Jen Looper \u7528 \u26a1\ufe0f \u7f16\u5199\u3002</p> <p>\u7b14\u8bb0\u672c\u9996\u5148\u51fa\u73b0\u5728 Azure \u201c\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u201d\u5b58\u50a8\u5e93 \u6700\u521d\u7531 Francesca Lazzeri \u7f16\u5199\u3002SVR \u8bfe\u7531 Anirban Mukherjee \u7f16\u5199\u3002</p>"},{"location":"7-TimeSeries/1-Introduction/","title":"Introduction to time series forecasting","text":"<p>Sketchnote by Tomomi Imura</p> <p>In this lesson and the following one, you will learn a bit about time series forecasting, an interesting and valuable part of a ML scientist's repertoire that is a bit less known than other topics. Time series forecasting is a sort of 'crystal ball': based on past performance of a variable such as price, you can predict its future potential value.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video about time series forecasting</p>"},{"location":"7-TimeSeries/1-Introduction/#pre-lecture-quiz","title":"Pre-lecture quiz","text":"<p>It's a useful and interesting field with real value to business, given its direct application to problems of pricing, inventory, and supply chain issues. While deep learning techniques have started to be used to gain more insights to better predict future performance, time series forecasting remains a field greatly informed by classic ML techniques.</p> <p>Penn State's useful time series curriculum can be found here</p>"},{"location":"7-TimeSeries/1-Introduction/#introduction","title":"Introduction","text":"<p>Suppose you maintain an array of smart parking meters that provide data about how often they are used and for how long over time.</p> <p>What if you could predict, based on the meter's past performance, its future value according to the laws of supply and demand?</p> <p>Accurately predicting when to act so as to achieve your goal is a challenge that could be tackled by time series forecasting. It wouldn't make folks happy to be charged more in busy times when they're looking for a parking spot, but it would be a sure way to generate revenue to clean the streets!</p> <p>Let's explore some of the types of time series algorithms and start a notebook to clean and prepare some data. The data you will analyze  is taken from the GEFCom2014 forecasting competition. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014. Given the historical patterns of electricity load and temperature, you can predict future values of electricity load.</p> <p>In this example, you'll learn how to forecast one time step ahead, using historical load data only. Before starting, however, it's useful to understand what's going on behind the scenes.</p>"},{"location":"7-TimeSeries/1-Introduction/#some-definitions","title":"Some definitions","text":"<p>When encountering the term 'time series' you need to understand its use in several different contexts.</p> <p>\ud83c\udf93 Time series</p> <p>In mathematics, \"a time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time.\" An example of a time series is the daily closing value of the Dow Jones Industrial Average. The use of time series plots and statistical modeling is frequently encountered in signal processing, weather forecasting, earthquake prediction, and other fields where events occur and data points can be plotted over time.</p> <p>\ud83c\udf93 Time series analysis</p> <p>Time series analysis, is the analysis of the above mentioned time series data. Time series data can take distinct forms, including 'interrupted time series' which detects patterns in a time series' evolution before and after an interrupting event. The type of analysis needed for the time series, depends on the nature of the data. Time series data itself can take the form of series of numbers or characters.</p> <p>The analysis to be performed, uses a variety of methods, including frequency-domain and time-domain, linear and nonlinear, and more. Learn more about the many ways to analyze this type of data.</p> <p>\ud83c\udf93 Time series forecasting</p> <p>Time series forecasting is the use of a model to predict future values based on patterns displayed by previously gathered data as it occurred in the past. While it is possible to use regression models to explore time series data, with time indices as x variables on a plot, such data is best analyzed using special types of models.</p> <p>Time series data is a list of ordered observations, unlike data that can be analyzed by linear regression.   The most common one is ARIMA, an acronym that stands for \"Autoregressive Integrated Moving Average\".</p> <p>ARIMA models \"relate the present value of a series to past values and past prediction errors.\" They are most appropriate for analyzing time-domain data, where data is ordered over time.</p> <p>There are several types of ARIMA models, which you can learn about here and which you will touch on in the next lesson.</p> <p>In the next lesson, you will build an ARIMA model using Univariate Time Series, which focuses on one variable that changes its value over time. An example of this type of data is this dataset that records the monthly C02 concentration at the Mauna Loa Observatory:</p> CO2 YearMonth Year Month 330.62 1975.04 1975 1 331.40 1975.13 1975 2 331.87 1975.21 1975 3 333.18 1975.29 1975 4 333.92 1975.38 1975 5 333.43 1975.46 1975 6 331.85 1975.54 1975 7 330.01 1975.63 1975 8 328.51 1975.71 1975 9 328.41 1975.79 1975 10 329.25 1975.88 1975 11 330.97 1975.96 1975 12 <p>\u2705 Identify the variable that changes over time in this dataset</p>"},{"location":"7-TimeSeries/1-Introduction/#time-series-data-characteristics-to-consider","title":"Time Series data characteristics to consider","text":"<p>When looking at time series data, you might notice that it has certain characteristics that you need to take into account and mitigate to better understand its patterns. If you consider time series data as potentially providing a 'signal' that you want to analyze, these characteristics can be thought of as 'noise'. You often will need to reduce this 'noise' by offsetting some of these characteristics using some statistical techniques.</p> <p>Here are some concepts you should know to be able to work with time series:</p> <p>\ud83c\udf93 Trends</p> <p>Trends are defined as measurable increases and decreases over time. Read more. In the context of time series, it's about how to use and, if necessary, remove trends from your time series.</p> <p>\ud83c\udf93 Seasonality</p> <p>Seasonality is defined as periodic fluctuations, such as holiday rushes that might affect sales, for example. Take a look at how different types of plots display seasonality in data.</p> <p>\ud83c\udf93 Outliers</p> <p>Outliers are far away from the standard data variance.</p> <p>\ud83c\udf93 Long-run cycle</p> <p>Independent of seasonality, data might display a long-run cycle such as an economic down-turn that lasts longer than a year.</p> <p>\ud83c\udf93 Constant variance</p> <p>Over time, some data display constant fluctuations, such as energy usage per day and night.</p> <p>\ud83c\udf93 Abrupt changes</p> <p>The data might display an abrupt change that might need further analysis. The abrupt shuttering of businesses due to COVID, for example, caused changes in data.</p> <p>\u2705 Here is a sample time series plot showing daily in-game currency spent over a few years. Can you identify any of the characteristics listed above in this data?</p> <p></p>"},{"location":"7-TimeSeries/1-Introduction/#exercise-getting-started-with-power-usage-data","title":"Exercise - getting started with power usage data","text":"<p>Let's get started creating a time series model to predict future power usage given past usage.</p> <p>The data in this example is taken from the GEFCom2014 forecasting competition. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014.</p> <p>Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli and Rob J. Hyndman, \"Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond\", International Journal of Forecasting, vol.32, no.3, pp 896-913, July-September, 2016.</p> <ol> <li> <p>In the <code>working</code> folder of this lesson, open the notebook.ipynb file. Start by adding libraries that will help you load and visualize data</p> <pre><code>import os\nimport matplotlib.pyplot as plt\nfrom common.utils import load_data\n%matplotlib inline\n</code></pre> <p>Note, you are using the files from the included <code>common</code> folder which set up your environment and handle downloading the data.</p> </li> <li> <p>Next, examine the data as a dataframe calling <code>load_data()</code> and <code>head()</code>:</p> <pre><code>data_dir = './data'\nenergy = load_data(data_dir)[['load']]\nenergy.head()\n</code></pre> <p>You can see that there are two columns representing date and load:</p> load 2012-01-01 00:00:00 2698.0 2012-01-01 01:00:00 2558.0 2012-01-01 02:00:00 2444.0 2012-01-01 03:00:00 2402.0 2012-01-01 04:00:00 2403.0 </li> <li> <p>Now, plot the data calling <code>plot()</code>:</p> <pre><code>energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p> </li> <li> <p>Now, plot the first week of July 2014, by providing it as input to the <code>energy</code> in <code>[from date]: [to date]</code> pattern:</p> <pre><code>energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p> <p>A beautiful plot! Take a look at these plots and see if you can determine any of the characteristics listed above. What can we surmise by visualizing the data?</p> </li> </ol> <p>In the next lesson, you will create an ARIMA model to create some forecasts.</p>"},{"location":"7-TimeSeries/1-Introduction/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Make a list of all the industries and areas of inquiry you can think of that would benefit from time series forecasting. Can you think of an application of these techniques in the arts? In Econometrics? Ecology? Retail? Industry? Finance? Where else?</p>"},{"location":"7-TimeSeries/1-Introduction/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"7-TimeSeries/1-Introduction/#review-self-study","title":"Review &amp; Self Study","text":"<p>Although we won't cover them here, neural networks are sometimes used to enhance classic methods of time series forecasting. Read more about them in this article</p>"},{"location":"7-TimeSeries/1-Introduction/#assignment","title":"Assignment","text":"<p>Visualize some more time series</p>"},{"location":"7-TimeSeries/1-Introduction/assignment/","title":"Visualize some more Time Series","text":""},{"location":"7-TimeSeries/1-Introduction/assignment/#instructions","title":"Instructions","text":"<p>You've begun to learn about Time Series Forecasting by looking at the type of data that requires this special modeling. You've visualized some data around energy. Now, look around for some other data that would benefit from Time Series Forecasting. Find three examples (try Kaggle and Azure Open Datasets) and create a notebook to visualize them. Notate any special characteristics they have (seasonality, abrupt changes, or other trends) in the notebook.</p>"},{"location":"7-TimeSeries/1-Introduction/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement Three datasets are plotted and explained in a notebook Two datasets are plotted and explained in a notebook Few datasets are plotted or explained in a notebook or the data presented is insufficient"},{"location":"7-TimeSeries/1-Introduction/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"7-TimeSeries/1-Introduction/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"7-TimeSeries/2-ARIMA/","title":"Time series forecasting with ARIMA","text":"<p>In the previous lesson, you learned a bit about time series forecasting and loaded a dataset showing the fluctuations of electrical load over a time period.</p> <p></p> <p>\ud83c\udfa5 Click the image above for a video: A brief introduction to ARIMA models. The example is done in R, but the concepts are universal.</p>"},{"location":"7-TimeSeries/2-ARIMA/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"7-TimeSeries/2-ARIMA/#introduction","title":"Introduction","text":"<p>In this lesson, you will discover a specific way to build models with ARIMA: AutoRegressive Integrated Moving Average. ARIMA models are particularly suited to fit data that shows non-stationarity.</p>"},{"location":"7-TimeSeries/2-ARIMA/#general-concepts","title":"General concepts","text":"<p>To be able to work with ARIMA, there are some concepts you need to know about:</p> <ul> <li> <p>\ud83c\udf93 Stationarity. From a statistical context, stationarity refers to data whose distribution does not change when shifted in time. Non-stationary data, then, shows fluctuations due to trends that must be transformed to be analyzed. Seasonality, for example, can introduce fluctuations in data and can be eliminated by a process of 'seasonal-differencing'.</p> </li> <li> <p>\ud83c\udf93 Differencing. Differencing data, again from a statistical context, refers to the process of transforming non-stationary data to make it stationary by removing its non-constant trend. \"Differencing removes the changes in the level of a time series, eliminating trend and seasonality and consequently stabilizing the mean of the time series.\" Paper by Shixiong et al</p> </li> </ul>"},{"location":"7-TimeSeries/2-ARIMA/#arima-in-the-context-of-time-series","title":"ARIMA in the context of time series","text":"<p>Let's unpack the parts of ARIMA to better understand how it helps us model time series and help us make predictions against it.</p> <ul> <li> <p>AR - for AutoRegressive. Autoregressive models, as the name implies, look 'back' in time to analyze previous values in your data and make assumptions about them. These previous values are called 'lags'. An example would be data that shows monthly sales of pencils. Each month's sales total would be considered an 'evolving variable' in the dataset. This model is built as the \"evolving variable of interest is regressed on its own lagged (i.e., prior) values.\" wikipedia</p> </li> <li> <p>I - for Integrated. As opposed to the similar 'ARMA' models, the 'I' in ARIMA refers to its integrated aspect. The data is 'integrated' when differencing steps are applied so as to eliminate non-stationarity.</p> </li> <li> <p>MA -  for Moving Average. The moving-average aspect of this model refers to the output variable that is determined by observing the current and past values of lags.</p> </li> </ul> <p>Bottom line: ARIMA is used to make a model fit the special form of time series data as closely as possible.</p>"},{"location":"7-TimeSeries/2-ARIMA/#exercise-build-an-arima-model","title":"Exercise - build an ARIMA model","text":"<p>Open the /working folder in this lesson and find the notebook.ipynb file.</p> <ol> <li> <p>Run the notebook to load the <code>statsmodels</code> Python library; you will need this for ARIMA models.</p> </li> <li> <p>Load necessary libraries</p> </li> <li> <p>Now, load up several more libraries useful for plotting data:</p> <pre><code>import os\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport math\n\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.preprocessing import MinMaxScaler\nfrom common.utils import load_data, mape\nfrom IPython.display import Image\n\n%matplotlib inline\npd.options.display.float_format = '{:,.2f}'.format\nnp.set_printoptions(precision=2)\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n</code></pre> </li> <li> <p>Load the data from the <code>/data/energy.csv</code> file into a Pandas dataframe and take a look:</p> <pre><code>energy = load_data('./data')[['load']]\nenergy.head(10)\n</code></pre> </li> <li> <p>Plot all the available energy data from January 2012 to December 2014. There should be no surprises as we saw this data in the last lesson:</p> <pre><code>energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p>Now, let's build a model!</p> </li> </ol>"},{"location":"7-TimeSeries/2-ARIMA/#create-training-and-testing-datasets","title":"Create training and testing datasets","text":"<p>Now your data is loaded, so you can separate it into train and test sets. You'll train your model on the train set. As usual, after the model has finished training, you'll evaluate its accuracy using the test set. You need to ensure that the test set covers a later period in time from the training set to ensure that the model does not gain information from future time periods.</p> <ol> <li> <p>Allocate a two-month period from September 1 to October 31, 2014 to the training set. The test set will include the two-month period of November 1 to December 31, 2014:</p> <pre><code>train_start_dt = '2014-11-01 00:00:00'\ntest_start_dt = '2014-12-30 00:00:00'\n</code></pre> <p>Since this data reflects the daily consumption of energy, there is a strong seasonal pattern, but the consumption is most similar to the consumption in more recent days.</p> </li> <li> <p>Visualize the differences:</p> <pre><code>energy[(energy.index &lt; test_start_dt) &amp; (energy.index &gt;= train_start_dt)][['load']].rename(columns={'load':'train'}) \\\n    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \\\n    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p> <p>Therefore, using a relatively small window of time for training the data should be sufficient.</p> <p>Note: Since the function we use to fit the ARIMA model uses in-sample validation during fitting, we will omit validation data.</p> </li> </ol>"},{"location":"7-TimeSeries/2-ARIMA/#prepare-the-data-for-training","title":"Prepare the data for training","text":"<p>Now, you need to prepare the data for training by performing filtering and scaling of your data. Filter your dataset to only include the time periods and columns you need, and scaling to ensure the data is projected in the interval 0,1.</p> <ol> <li> <p>Filter the original dataset to include only the aforementioned time periods per set and only including the needed column 'load' plus the date:</p> <pre><code>train = energy.copy()[(energy.index &gt;= train_start_dt) &amp; (energy.index &lt; test_start_dt)][['load']]\ntest = energy.copy()[energy.index &gt;= test_start_dt][['load']]\n\nprint('Training data shape: ', train.shape)\nprint('Test data shape: ', test.shape)\n</code></pre> <p>You can see the shape of the data:</p> <pre><code>Training data shape:  (1416, 1)\nTest data shape:  (48, 1)\n</code></pre> </li> <li> <p>Scale the data to be in the range (0, 1).</p> <pre><code>scaler = MinMaxScaler()\ntrain['load'] = scaler.fit_transform(train)\ntrain.head(10)\n</code></pre> </li> <li> <p>Visualize the original vs. scaled data:</p> <pre><code>energy[(energy.index &gt;= train_start_dt) &amp; (energy.index &lt; test_start_dt)][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12)\ntrain.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12)\nplt.show()\n</code></pre> <p></p> <p>The original data</p> <p></p> <p>The scaled data</p> </li> <li> <p>Now that you have calibrated the scaled data, you can scale the test data:</p> <pre><code>test['load'] = scaler.transform(test)\ntest.head()\n</code></pre> </li> </ol>"},{"location":"7-TimeSeries/2-ARIMA/#implement-arima","title":"Implement ARIMA","text":"<p>It's time to implement ARIMA! You'll now use the <code>statsmodels</code> library that you installed earlier.</p> <p>Now you need to follow several steps</p> <ol> <li>Define the model by calling <code>SARIMAX()</code> and passing in the model parameters: p, d, and q parameters, and P, D, and Q parameters.</li> <li>Prepare the model for the training data by calling the fit() function.</li> <li>Make predictions calling the <code>forecast()</code> function and specifying the number of steps (the <code>horizon</code>) to forecast.</li> </ol> <p>\ud83c\udf93 What are all these parameters for? In an ARIMA model there are 3 parameters that are used to help model the major aspects of a time series: seasonality, trend, and noise. These parameters are:</p> <p><code>p</code>: the parameter associated with the auto-regressive aspect of the model, which incorporates past values. <code>d</code>: the parameter associated with the integrated part of the model, which affects the amount of differencing (\ud83c\udf93 remember differencing \ud83d\udc46?) to apply to a time series. <code>q</code>: the parameter associated with the moving-average part of the model.</p> <p>Note: If your data has a seasonal aspect - which this one does - , we use a seasonal ARIMA model (SARIMA). In that case you need to use another set of parameters: <code>P</code>, <code>D</code>, and <code>Q</code> which describe the same associations as <code>p</code>, <code>d</code>, and <code>q</code>, but correspond to the seasonal components of the model.</p> <ol> <li> <p>Start by setting your preferred horizon value. Let's try 3 hours:</p> <pre><code># Specify the number of steps to forecast ahead\nHORIZON = 3\nprint('Forecasting horizon:', HORIZON, 'hours')\n</code></pre> <p>Selecting the best values for an ARIMA model's parameters can be challenging as it's somewhat subjective and time intensive. You might consider using an <code>auto_arima()</code> function from the <code>pyramid</code> library,</p> </li> <li> <p>For now try some manual selections to find a good model.</p> <pre><code>order = (4, 1, 0)\nseasonal_order = (1, 1, 0, 24)\n\nmodel = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)\nresults = model.fit()\n\nprint(results.summary())\n</code></pre> <p>A table of results is printed.</p> </li> </ol> <p>You've built your first model! Now we need to find a way to evaluate it.</p>"},{"location":"7-TimeSeries/2-ARIMA/#evaluate-your-model","title":"Evaluate your model","text":"<p>To evaluate your model, you can perform the so-called <code>walk forward</code> validation. In practice, time series models are re-trained each time a new data becomes available. This allows the model to make the best forecast at each time step.</p> <p>Starting at the beginning of the time series using this technique, train the model on the train data set. Then make a prediction on the next time step. The prediction is evaluated against the known value. The training set is then expanded to include the known value and the process is repeated.</p> <p>Note: You should keep the training set window fixed for more efficient training so that every time you add a new observation to the training set, you remove the observation from the beginning of the set.</p> <p>This process provides a more robust estimation of how the model will perform in practice. However, it comes at the computation cost of creating so many models. This is acceptable if the data is small or if the model is simple, but could be an issue at scale.</p> <p>Walk-forward validation is the gold standard of time series model evaluation and is recommended for your own projects.</p> <ol> <li> <p>First, create a test data point for each HORIZON step.</p> <pre><code>test_shifted = test.copy()\n\nfor t in range(1, HORIZON+1):\n    test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')\n\ntest_shifted = test_shifted.dropna(how='any')\ntest_shifted.head(5)\n</code></pre> load load+1 load+2 2014-12-30 00:00:00 0.33 0.29 0.27 2014-12-30 01:00:00 0.29 0.27 0.27 2014-12-30 02:00:00 0.27 0.27 0.30 2014-12-30 03:00:00 0.27 0.30 0.41 2014-12-30 04:00:00 0.30 0.41 0.57 <p>The data is shifted horizontally according to its horizon point.</p> </li> <li> <p>Make predictions on your test data using this sliding window approach in a loop the size of the test data length:</p> <pre><code>%%time\ntraining_window = 720 # dedicate 30 days (720 hours) for training\n\ntrain_ts = train['load']\ntest_ts = test_shifted\n\nhistory = [x for x in train_ts]\nhistory = history[(-training_window):]\n\npredictions = list()\n\norder = (2, 1, 0)\nseasonal_order = (1, 1, 0, 24)\n\nfor t in range(test_ts.shape[0]):\n    model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)\n    model_fit = model.fit()\n    yhat = model_fit.forecast(steps = HORIZON)\n    predictions.append(yhat)\n    obs = list(test_ts.iloc[t])\n    # move the training window\n    history.append(obs[0])\n    history.pop(0)\n    print(test_ts.index[t])\n    print(t+1, ': predicted =', yhat, 'expected =', obs)\n</code></pre> <p>You can watch the training occurring:</p> <pre><code>2014-12-30 00:00:00\n1 : predicted = [0.32 0.29 0.28] expected = [0.32945389435989236, 0.2900626678603402, 0.2739480752014323]\n\n2014-12-30 01:00:00\n2 : predicted = [0.3  0.29 0.3 ] expected = [0.2900626678603402, 0.2739480752014323, 0.26812891674127126]\n\n2014-12-30 02:00:00\n3 : predicted = [0.27 0.28 0.32] expected = [0.2739480752014323, 0.26812891674127126, 0.3025962399283795]\n</code></pre> </li> <li> <p>Compare the predictions to the actual load:</p> <pre><code>eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\neval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]\neval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')\neval_df['actual'] = np.array(np.transpose(test_ts)).ravel()\neval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])\neval_df.head()\n</code></pre> <p>Output |     |            | timestamp | h   | prediction | actual   | | --- | ---------- | --------- | --- | ---------- | -------- | | 0   | 2014-12-30 | 00:00:00  | t+1 | 3,008.74   | 3,023.00 | | 1   | 2014-12-30 | 01:00:00  | t+1 | 2,955.53   | 2,935.00 | | 2   | 2014-12-30 | 02:00:00  | t+1 | 2,900.17   | 2,899.00 | | 3   | 2014-12-30 | 03:00:00  | t+1 | 2,917.69   | 2,886.00 | | 4   | 2014-12-30 | 04:00:00  | t+1 | 2,946.99   | 2,963.00 |</p> <p>Observe the hourly data's prediction, compared to the actual load. How accurate is this?</p> </li> </ol>"},{"location":"7-TimeSeries/2-ARIMA/#check-model-accuracy","title":"Check model accuracy","text":"<p>Check the accuracy of your model by testing its mean absolute percentage error (MAPE) over all the predictions.</p> <p>\ud83e\uddee Show me the math</p> <p></p> <p>MAPE is used to show prediction accuracy as a ratio defined by the above formula. The difference between actual<sub>t</sub> and predicted<sub>t</sub> is divided by the actual<sub>t</sub>. \"The absolute value in this calculation is summed for every forecasted point in time and divided by the number of fitted points n.\" wikipedia</p> <ol> <li> <p>Express equation in code:</p> <pre><code>if(HORIZON &gt; 1):\n    eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n    print(eval_df.groupby('h')['APE'].mean())\n</code></pre> </li> <li> <p>Calculate one step's MAPE:</p> <pre><code>print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')\n</code></pre> <p>One step forecast MAPE:  0.5570581332313952 %</p> </li> <li> <p>Print the multi-step forecast MAPE:</p> <pre><code>print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')\n</code></pre> <pre><code>Multi-step forecast MAPE:  1.1460048657704118 %\n</code></pre> <p>A nice low number is best: consider that a forecast that has a MAPE of 10 is off by 10%.</p> </li> <li> <p>But as always, it's easier to see this kind of accuracy measurement visually, so let's plot it:</p> <pre><code> if(HORIZON == 1):\n    ## Plotting single step forecast\n    eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))\n\nelse:\n    ## Plotting multi step forecast\n    plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n    for t in range(1, HORIZON+1):\n        plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values\n\n    fig = plt.figure(figsize=(15, 8))\n    ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n    ax = fig.add_subplot(111)\n    for t in range(1, HORIZON+1):\n        x = plot_df['timestamp'][(t-1):]\n        y = plot_df['t+'+str(t)][0:len(x)]\n        ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))\n\n    ax.legend(loc='best')\n\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p> </li> </ol> <p>\ud83c\udfc6 A very nice plot, showing a model with good accuracy. Well done!</p>"},{"location":"7-TimeSeries/2-ARIMA/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Dig into the ways to test the accuracy of a Time Series Model. We touch on MAPE in this lesson, but are there other methods you could use? Research them and annotate them. A helpful document can be found here</p>"},{"location":"7-TimeSeries/2-ARIMA/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"7-TimeSeries/2-ARIMA/#review-self-study","title":"Review &amp; Self Study","text":"<p>This lesson touches on only the basics of Time Series Forecasting with ARIMA. Take some time to deepen your knowledge by digging into this repository and its various model types to learn other ways to build Time Series models.</p>"},{"location":"7-TimeSeries/2-ARIMA/#assignment","title":"Assignment","text":"<p>A new ARIMA model</p>"},{"location":"7-TimeSeries/2-ARIMA/assignment/","title":"A new ARIMA model","text":""},{"location":"7-TimeSeries/2-ARIMA/assignment/#instructions","title":"Instructions","text":"<p>Now that you have built an ARIMA model, build a new one with fresh data (try one of these datasets from Duke. Annotate your work in a notebook, visualize the data and your model, and test its accuracy using MAPE.</p>"},{"location":"7-TimeSeries/2-ARIMA/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with a new ARIMA model built, tested and explained with visualizations and accuracy stated. The notebook presented is not annotated or contains bugs An incomplete notebook is presented"},{"location":"7-TimeSeries/2-ARIMA/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"7-TimeSeries/2-ARIMA/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"7-TimeSeries/3-SVR/","title":"Time Series Forecasting with Support Vector Regressor","text":"<p>In the previous lesson, you learned how to use ARIMA model to make time series predictions. Now you'll be looking at Support Vector Regressor model which is a regressor model used to predict continuous data.</p>"},{"location":"7-TimeSeries/3-SVR/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"7-TimeSeries/3-SVR/#introduction","title":"Introduction","text":"<p>In this lesson, you will discover a specific way to build models with SVM: Support Vector Machine for regression, or SVR: Support Vector Regressor. </p>"},{"location":"7-TimeSeries/3-SVR/#svr-in-the-context-of-time-series-1","title":"SVR in the context of time series [^1]","text":"<p>Before understanding the importance of SVR in time series prediction, here are some of the important concepts that you need to know:</p> <ul> <li>Regression: Supervised learning technique to predict continuous values from a given set of inputs. The idea is to fit a curve (or line) in the feature space that has the maximum number of data points. Click here for more information.</li> <li>Support Vector Machine (SVM): A type of supervised machine learning model used for classification, regression and outliers detection. The model is a hyperplane in the feature space, which in case of classification acts as a boundary, and in case of regression acts as the best-fit line. In SVM, a Kernel function is generally used to transform the dataset to a space of higher number of dimensions, so that they can be easily separable. Click here for more information on SVMs.</li> <li>Support Vector Regressor (SVR): A type of SVM, to find the best fit line (which in the case of SVM is a hyperplane) that has the maximum number of data points.</li> </ul>"},{"location":"7-TimeSeries/3-SVR/#why-svr-1","title":"Why SVR? [^1]","text":"<p>In the last lesson you learned about ARIMA, which is a very successful statistical linear method to forecast time series data. However, in many cases, time series data have non-linearity, which cannot be mapped by linear models. In such cases, the ability of SVM to consider non-linearity in the data for regression tasks makes SVR successful in time series forecasting.</p>"},{"location":"7-TimeSeries/3-SVR/#exercise-build-an-svr-model","title":"Exercise - build an SVR model","text":"<p>The first few steps for data preparation are the same as that of the previous lesson on ARIMA. </p> <p>Open the /working folder in this lesson and find the notebook.ipynb file.[^2]</p> <ol> <li>Run the notebook and import the necessary libraries:  [^2]</li> </ol> <pre><code>import sys\nsys.path.append('../../')\n</code></pre> <pre><code>import os\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport math\n\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import MinMaxScaler\nfrom common.utils import load_data, mape\n</code></pre> <ol> <li>Load the data from the <code>/data/energy.csv</code> file into a Pandas dataframe and take a look:  [^2]</li> </ol> <pre><code>energy = load_data('../../data')[['load']]\n</code></pre> <ol> <li>Plot all the available energy data from January 2012 to December 2014: [^2]</li> </ol> <pre><code>energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p> <p>Now, let's build our SVR model.</p>"},{"location":"7-TimeSeries/3-SVR/#create-training-and-testing-datasets","title":"Create training and testing datasets","text":"<p>Now your data is loaded, so you can separate it into train and test sets. Then you'll reshape the data to create a time-step based dataset which will be needed for the SVR. You'll train your model on the train set. After the model has finished training, you'll evaluate its accuracy on the training set, testing set and then the full dataset to see the overall performance. You need to ensure that the test set covers a later period in time from the training set to ensure that the model does not gain information from future time periods [^2] (a situation known as Overfitting).</p> <ol> <li>Allocate a two-month period from September 1 to October 31, 2014 to the training set. The test set will include the two-month period of November 1 to December 31, 2014: [^2]</li> </ol> <pre><code>train_start_dt = '2014-11-01 00:00:00'\ntest_start_dt = '2014-12-30 00:00:00'\n</code></pre> <ol> <li>Visualize the differences: [^2]</li> </ol> <pre><code>energy[(energy.index &lt; test_start_dt) &amp; (energy.index &gt;= train_start_dt)][['load']].rename(columns={'load':'train'}) \\\n    .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \\\n    .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)\nplt.xlabel('timestamp', fontsize=12)\nplt.ylabel('load', fontsize=12)\nplt.show()\n</code></pre> <p></p>"},{"location":"7-TimeSeries/3-SVR/#prepare-the-data-for-training","title":"Prepare the data for training","text":"<p>Now, you need to prepare the data for training by performing filtering and scaling of your data. Filter your dataset to only include the time periods and columns you need, and scaling to ensure the data is projected in the interval 0,1.</p> <ol> <li>Filter the original dataset to include only the aforementioned time periods per set and only including the needed column 'load' plus the date: [^2]</li> </ol> <pre><code>train = energy.copy()[(energy.index &gt;= train_start_dt) &amp; (energy.index &lt; test_start_dt)][['load']]\ntest = energy.copy()[energy.index &gt;= test_start_dt][['load']]\n\nprint('Training data shape: ', train.shape)\nprint('Test data shape: ', test.shape)\n</code></pre> <pre><code>Training data shape:  (1416, 1)\nTest data shape:  (48, 1)\n</code></pre> <ol> <li>Scale the training data to be in the range (0, 1): [^2]</li> </ol> <pre><code>scaler = MinMaxScaler()\ntrain['load'] = scaler.fit_transform(train)\n</code></pre> <ol> <li>Now, you scale the testing data: [^2]</li> </ol> <pre><code>test['load'] = scaler.transform(test)\n</code></pre>"},{"location":"7-TimeSeries/3-SVR/#create-data-with-time-steps-1","title":"Create data with time-steps [^1]","text":"<p>For the SVR, you transform the input data to be of the form <code>[batch, timesteps]</code>. So, you reshape the existing <code>train_data</code> and <code>test_data</code> such that there is a new dimension which refers to the timesteps. </p> <pre><code># Converting to numpy arrays\ntrain_data = train.values\ntest_data = test.values\n</code></pre> <p>For this example, we take <code>timesteps = 5</code>. So, the inputs to the model are the data for the first 4 timesteps, and the output will be the data for the 5th timestep.</p> <pre><code>timesteps=5\n</code></pre> <p>Converting training data to 2D tensor using nested list comprehension:</p> <pre><code>train_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]\ntrain_data_timesteps.shape\n</code></pre> <pre><code>(1412, 5)\n</code></pre> <p>Converting testing data to 2D tensor:</p> <pre><code>test_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]\ntest_data_timesteps.shape\n</code></pre> <pre><code>(44, 5)\n</code></pre> <p>Selecting inputs and outputs from training and testing data:</p> <pre><code>x_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]\nx_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n</code></pre> <pre><code>(1412, 4) (1412, 1)\n(44, 4) (44, 1)\n</code></pre>"},{"location":"7-TimeSeries/3-SVR/#implement-svr-1","title":"Implement SVR [^1]","text":"<p>Now, it's time to implement SVR. To read more about this implementation, you can refer to this documentation. For our implementation, we follow these steps:</p> <ol> <li>Define the model by calling <code>SVR()</code> and passing in the model hyperparameters: kernel, gamma, c and epsilon</li> <li>Prepare the model for the training data by calling the <code>fit()</code> function</li> <li>Make predictions calling the <code>predict()</code> function</li> </ol> <p>Now we create an SVR model. Here we use the RBF kernel, and set the hyperparameters gamma, C and epsilon as 0.5, 10 and 0.05 respectively.</p> <pre><code>model = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)\n</code></pre>"},{"location":"7-TimeSeries/3-SVR/#fit-the-model-on-training-data-1","title":"Fit the model on training data [^1]","text":"<pre><code>model.fit(x_train, y_train[:,0])\n</code></pre> <pre><code>SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.05, gamma=0.5,\n    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n</code></pre>"},{"location":"7-TimeSeries/3-SVR/#make-model-predictions-1","title":"Make model predictions [^1]","text":"<pre><code>y_train_pred = model.predict(x_train).reshape(-1,1)\ny_test_pred = model.predict(x_test).reshape(-1,1)\n\nprint(y_train_pred.shape, y_test_pred.shape)\n</code></pre> <pre><code>(1412, 1) (44, 1)\n</code></pre> <p>You've built your SVR! Now we need to evaluate it.</p>"},{"location":"7-TimeSeries/3-SVR/#evaluate-your-model-1","title":"Evaluate your model [^1]","text":"<p>For evaluation, first we will scale back the data to our original scale. Then, to check the performance, we will plot the original and predicted time series plot, and also print the MAPE result.</p> <p>Scale the predicted and original output:</p> <pre><code># Scaling the predictions\ny_train_pred = scaler.inverse_transform(y_train_pred)\ny_test_pred = scaler.inverse_transform(y_test_pred)\n\nprint(len(y_train_pred), len(y_test_pred))\n</code></pre> <pre><code># Scaling the original values\ny_train = scaler.inverse_transform(y_train)\ny_test = scaler.inverse_transform(y_test)\n\nprint(len(y_train), len(y_test))\n</code></pre>"},{"location":"7-TimeSeries/3-SVR/#check-model-performance-on-training-and-testing-data-1","title":"Check model performance on training and testing data [^1]","text":"<p>We extract the timestamps from the dataset to show in the x-axis of our plot. Note that we are using the first <code>timesteps-1</code> values as out input for the first output, so the timestamps for the output will start after that.</p> <pre><code>train_timestamps = energy[(energy.index &lt; test_start_dt) &amp; (energy.index &gt;= train_start_dt)].index[timesteps-1:]\ntest_timestamps = energy[test_start_dt:].index[timesteps-1:]\n\nprint(len(train_timestamps), len(test_timestamps))\n</code></pre> <pre><code>1412 44\n</code></pre> <p>Plot the predictions for training data:</p> <pre><code>plt.figure(figsize=(25,6))\nplt.plot(train_timestamps, y_train, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(train_timestamps, y_train_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.title(\"Training data prediction\")\nplt.show()\n</code></pre> <p></p> <p>Print MAPE for training data</p> <pre><code>print('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')\n</code></pre> <pre><code>MAPE for training data: 1.7195710200875551 %\n</code></pre> <p>Plot the predictions for testing data</p> <pre><code>plt.figure(figsize=(10,3))\nplt.plot(test_timestamps, y_test, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(test_timestamps, y_test_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.show()\n</code></pre> <p></p> <p>Print MAPE for testing data</p> <pre><code>print('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')\n</code></pre> <pre><code>MAPE for testing data:  1.2623790187854018 %\n</code></pre> <p>\ud83c\udfc6 You have a very good result on the testing dataset!</p>"},{"location":"7-TimeSeries/3-SVR/#check-model-performance-on-full-dataset-1","title":"Check model performance on full dataset [^1]","text":"<pre><code># Extracting load values as numpy array\ndata = energy.copy().values\n\n# Scaling\ndata = scaler.transform(data)\n\n# Transforming to 2D tensor as per model input requirement\ndata_timesteps=np.array([[j for j in data[i:i+timesteps]] for i in range(0,len(data)-timesteps+1)])[:,:,0]\nprint(\"Tensor shape: \", data_timesteps.shape)\n\n# Selecting inputs and outputs from data\nX, Y = data_timesteps[:,:timesteps-1],data_timesteps[:,[timesteps-1]]\nprint(\"X shape: \", X.shape,\"\\nY shape: \", Y.shape)\n</code></pre> <pre><code>Tensor shape:  (26300, 5)\nX shape:  (26300, 4) \nY shape:  (26300, 1)\n</code></pre> <pre><code># Make model predictions\nY_pred = model.predict(X).reshape(-1,1)\n\n# Inverse scale and reshape\nY_pred = scaler.inverse_transform(Y_pred)\nY = scaler.inverse_transform(Y)\n</code></pre> <pre><code>plt.figure(figsize=(30,8))\nplt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(Y_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.show()\n</code></pre> <pre><code>print('MAPE: ', mape(Y_pred, Y)*100, '%')\n</code></pre> <pre><code>MAPE:  2.0572089029888656 %\n</code></pre> <p>\ud83c\udfc6 Very nice plots, showing a model with good accuracy. Well done!</p>"},{"location":"7-TimeSeries/3-SVR/#challenge","title":"\ud83d\ude80Challenge","text":"<ul> <li>Try to tweak the hyperparameters (gamma, C, epsilon) while creating the model and evaluate on the data to see which set of hyperparameters give the best results on the testing data. To know more about these hyperparameters, you can refer to the  document here. </li> <li>Try to use different kernel functions for the model and analyze their performances on the dataset. A helpful document can be found here.</li> <li>Try using different values for <code>timesteps</code> for the model to look back to make prediction.</li> </ul>"},{"location":"7-TimeSeries/3-SVR/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"7-TimeSeries/3-SVR/#review-self-study","title":"Review &amp; Self Study","text":"<p>This lesson was to introduce the application of SVR for Time Series Forecasting. To read more about SVR, you can refer to this blog. This documentation on scikit-learn provides a more comprehensive explanation about SVMs in general, SVRs and also other implementation details such as the different kernel functions that can be used, and their parameters.</p>"},{"location":"7-TimeSeries/3-SVR/#assignment","title":"Assignment","text":"<p>A new SVR model</p>"},{"location":"7-TimeSeries/3-SVR/#credits","title":"Credits","text":"<p>[^1]: The text, code and output in this section was contributed by @AnirbanMukherjeeXD [^2]: The text, code and output in this section was taken from ARIMA</p>"},{"location":"7-TimeSeries/3-SVR/assignment/","title":"A new SVR model","text":""},{"location":"7-TimeSeries/3-SVR/assignment/#instructions-1","title":"Instructions [^1]","text":"<p>Now that you have built an SVR model, build a new one with fresh data (try one of these datasets from Duke). Annotate your work in a notebook, visualize the data and your model, and test its accuracy using appropriate plots and MAPE. Also try tweaking the different hyperparameters and also using different values for the timesteps.</p>"},{"location":"7-TimeSeries/3-SVR/assignment/#rubric-1","title":"Rubric [^1]","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with an SVR model built, tested and explained with visualizations and accuracy stated. The notebook presented is not annotated or contains bugs. An incomplete notebook is presented <p>[^1]:The text in this section was based on the assignment from ARIMA</p>"},{"location":"8-Reinforcement/","title":"Introduction to reinforcement learning","text":"<p>Reinforcement learning, RL, is seen as one of the basic machine learning paradigms, next to supervised learning and unsupervised learning. RL is all about decisions: delivering the right decisions or at least learning from them.</p> <p>Imagine you have a simulated environment such as the stock market. What happens if you impose a given regulation? Does it have a positive or negative effect? If something negative happens, you need to take this negative reinforcement, learn from it, and change course. If it's a positive outcome, you need to build on that positive reinforcement.</p> <p></p> <p>Peter and his friends need to escape the hungry wolf! Image by Jen Looper</p>"},{"location":"8-Reinforcement/#regional-topic-peter-and-the-wolf-russia","title":"Regional topic: Peter and the Wolf (Russia)","text":"<p>Peter and the Wolf is a musical fairy tale written by a Russian composer Sergei Prokofiev. It is a story about young pioneer Peter, who bravely goes out of his house to the forest clearing to chase the wolf. In this section, we will train machine learning algorithms that will help Peter:</p> <ul> <li>Explore the surrounding area and build an optimal navigation map</li> <li>Learn how to use a skateboard and balance on it, in order to move around faster.</li> </ul> <p></p> <p>\ud83c\udfa5 Click the image above to listen to Peter and the Wolf by Prokofiev</p>"},{"location":"8-Reinforcement/#reinforcement-learning","title":"Reinforcement learning","text":"<p>In previous sections, you have seen two examples of machine learning problems:</p> <ul> <li>Supervised, where we have datasets that suggest sample solutions to the problem we want to solve. Classification and regression are supervised learning tasks.</li> <li>Unsupervised, in which we do not have labeled training data. The main example of unsupervised learning is Clustering.</li> </ul> <p>In this section, we will introduce you to a new type of learning problem that does not require labeled training data. There are several types of such problems:</p> <ul> <li>Semi-supervised learning, where we have a lot of unlabeled data that can be used to pre-train the model.</li> <li>Reinforcement learning, in which an agent learns how to behave by performing experiments in some simulated environment.</li> </ul>"},{"location":"8-Reinforcement/#example-computer-game","title":"Example - computer game","text":"<p>Suppose you want to teach a computer to play a game, such as chess, or Super Mario. For the computer to play a game, we need it to predict which move to make in each of the game states. While this may seem like a classification problem, it is not - because we do not have a dataset with states and corresponding actions. While we may have some data like existing chess matches or recording of players playing Super Mario, it is likely that that data will not sufficiently cover a large enough number of possible states.</p> <p>Instead of looking for existing game data, Reinforcement Learning (RL) is based on the idea of making the computer play many times and observing the result. Thus, to apply Reinforcement Learning, we need two things:</p> <ul> <li> <p>An environment and a simulator which allow us to play a game many times. This simulator would define all the game rules as well as possible states and actions.</p> </li> <li> <p>A reward function, which would tell us how well we did during each move or game.</p> </li> </ul> <p>The main difference between other types of machine learning and RL is that in RL we typically do not know whether we win or lose until we finish the game. Thus, we cannot say whether a certain move alone is good or not - we only receive a reward at the end of the game. And our goal is to design algorithms that will allow us to train a model under  uncertain conditions. We will learn about one RL algorithm called Q-learning.</p>"},{"location":"8-Reinforcement/#lessons","title":"Lessons","text":"<ol> <li>Introduction to reinforcement learning and Q-Learning</li> <li>Using a gym simulation environment</li> </ol>"},{"location":"8-Reinforcement/#credits","title":"Credits","text":"<p>\"Introduction to Reinforcement Learning\" was written with \u2665\ufe0f by Dmitry Soshnikov</p>"},{"location":"8-Reinforcement/README.zh-cn/","title":"\u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb","text":"<p>\u5f3a\u5316\u5b66\u4e60 (RL, Reinforcement Learning)\uff0c\u662f\u57fa\u672c\u7684\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u4e4b\u4e00\uff08\u4ec5\u6b21\u4e8e\u76d1\u7763\u5b66\u4e60 (Supervised Learning) \u548c\u65e0\u76d1\u7763\u5b66\u4e60(Unsupervised Learning)\uff09\u3002\u5f3a\u5316\u5b66\u4e60\u548c\u300c\u7b56\u7565\u300d\u606f\u606f\u76f8\u5173\uff1a\u5b83\u5e94\u5f53\u4ea7\u751f\u6b63\u786e\u7684\u7b56\u7565\uff0c\u6216\u4ece\u9519\u8bef\u7684\u7b56\u7565\u4e2d\u5b66\u4e60\u3002</p> <p>\u5047\u8bbe\u6709\u4e00\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u6bd4\u5982\u8bf4\u80a1\u5e02\u3002\u5f53\u6211\u4eec\u7528\u67d0\u4e00\u4e2a\u89c4\u5219\u6765\u9650\u5236\u8fd9\u4e2a\u5e02\u573a\u65f6\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\u8fd9\u4e2a\u89c4\u5219\uff08\u6216\u8005\u8bf4\u7b56\u7565\uff09\u6709\u79ef\u6781\u6216\u6d88\u6781\u7684\u5f71\u54cd\u5417\uff1f\u5982\u679c\u5b83\u7684\u5f71\u54cd\u662f\u6b63\u9762\u7684\uff0c\u6211\u4eec\u9700\u8981\u4ece\u8fd9\u79cd_\u8d1f\u9762\u5f3a\u5316_\u4e2d\u5b66\u4e60\uff0c\u6539\u53d8\u6211\u4eec\u7684\u7b56\u7565\u3002\u5982\u679c\u5b83\u7684\u5f71\u54cd\u662f\u6b63\u9762\u7684\uff0c\u6211\u4eec\u9700\u8981\u5728\u8fd9\u79cd_\u79ef\u6781\u5f3a\u5316_\u7684\u57fa\u7840\u4e0a\u518d\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002</p> <p></p> <p>\u5f7c\u5f97\u548c\u4ed6\u7684\u670b\u53cb\u4eec\u5f97\u4ece\u9965\u997f\u7684\u72fc\u8fd9\u513f\u9003\u6389\uff01\u56fe\u7247\u6765\u81ea Jen Looper</p>"},{"location":"8-Reinforcement/README.zh-cn/#_2","title":"\u672c\u8282\u4e3b\u9898\uff1a\u5f7c\u5f97\u4e0e\u72fc\uff08\u4fc4\u7f57\u65af\uff09","text":"<p>\u5f7c\u5f97\u4e0e\u72fc \u662f\u4fc4\u7f57\u65af\u4f5c\u66f2\u5bb6\u8c22\u5c14\u76d6\u00b7\u666e\u7f57\u79d1\u83f2\u8036\u592b\u521b\u4f5c\u7684\u97f3\u4e50\u7ae5\u8bdd\u3002\u5b83\u8bb2\u8ff0\u4e86\u5f7c\u5f97\u52c7\u6562\u5730\u8d70\u51fa\u5bb6\u95e8\uff0c\u5230\u68ee\u6797\u4e2d\u592e\u8ffd\u9010\u72fc\u7684\u6545\u4e8b\u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u8bad\u7ec3\u5e2e\u52a9\u5f7c\u5f97\u8ffd\u72fc\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1a</p> <ul> <li>\u63a2\u7d22\u5468\u8fb9\u533a\u57df\u5e76\u6784\u5efa\u6700\u4f73\u5730\u56fe</li> <li>\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u6ed1\u677f\u5e76\u5728\u6ed1\u677f\u4e0a\u4fdd\u6301\u5e73\u8861\uff0c\u4ee5\u4fbf\u66f4\u5feb\u5730\u79fb\u52a8\u3002</li> </ul> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u9762\u7684\u56fe\u7247\uff0c\u542c\u666e\u7f57\u79d1\u83f2\u8036\u592b\u7684\u300a\u5f7c\u5f97\u4e0e\u72fc\u300b</p>"},{"location":"8-Reinforcement/README.zh-cn/#_3","title":"\u5f3a\u5316\u5b66\u4e60","text":"<p>\u5728\u524d\u9762\u7684\u90e8\u5206\u4e2d\uff0c\u60a8\u5df2\u7ecf\u770b\u5230\u4e86\u4e24\u7c7b\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u4f8b\u5b50\uff1a</p> <ul> <li>\u76d1\u7763\uff0c\u5728\u6709\u5df2\u7ecf\u6807\u8bb0\u7684\uff0c\u6697\u542b\u89e3\u51b3\u65b9\u6848\u7684\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u3002 \u5206\u7c7b \u548c \u56de\u5f52 \u662f\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u3002</li> <li>\u65e0\u76d1\u7763\uff0c\u5728\u6211\u4eec\u6ca1\u6709\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u3002\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u4e3b\u8981\u4f8b\u5b50\u662f \u805a\u7c7b\u3002</li> </ul> <p>\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u4e00\u7c7b\u65b0\u7684\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u5b83\u4e0d\u9700\u8981\u5df2\u7ecf\u6807\u8bb0\u7684\u8bad\u7ec3\u6570\u636e \u2014\u2014 \u6bd4\u5982\u8fd9\u4e24\u7c7b\u95ee\u9898\uff1a</p> <ul> <li>\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u6211\u4eec\u6709\u5f88\u591a\u672a\u6807\u8bb0\u7684\u3001\u53ef\u4ee5\u7528\u6765\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002</li> <li>\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u673a\u5668\u901a\u8fc7\u5728\u67d0\u79cd\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u6765\u5b66\u4e60\u6700\u4f73\u7b56\u7565\u3002</li> </ul>"},{"location":"8-Reinforcement/README.zh-cn/#-","title":"\u793a\u4f8b - \u7535\u8111\u6e38\u620f","text":"<p>\u5047\u8bbe\u6211\u4eec\u8981\u6559\u4f1a\u8ba1\u7b97\u673a\u73a9\u67d0\u4e00\u6b3e\u6e38\u620f \u2014\u2014 \u4f8b\u5982\u56fd\u9645\u8c61\u68cb\uff0c\u6216\u8005 \u8d85\u7ea7\u9a6c\u91cc\u5965\u3002\u4e3a\u4e86\u8ba9\u8ba1\u7b97\u673a\u5b66\u4f1a\u73a9\u6e38\u620f\uff0c\u6211\u4eec\u9700\u8981\u5b83\u9884\u6d4b\u5728\u6bcf\u4e2a\u6e38\u620f\u300c\u72b6\u6001\u300d\u4e0b\uff0c\u5b83\u5e94\u8be5\u505a\u4ec0\u4e48\u300c\u64cd\u4f5c\u300d\u3002\u867d\u7136\u8fd9\u770b\u8d77\u6765\u50cf\u662f\u4e00\u4e2a\u5206\u7c7b\u95ee\u9898\uff0c\u4f46\u4e8b\u5b9e\u5e76\u975e\u5982\u6b64\uff0c\u56e0\u4e3a\u6211\u4eec\u5e76\u6ca1\u6709\u50cf\u8fd9\u6837\u7684\uff0c\u5305\u542b\u300c\u72b6\u6001\u300d\u548c\u72b6\u6001\u5bf9\u5e94\u7684\u300c\u64cd\u4f5c\u300d\u7684\u6570\u636e\u96c6\u3002\u6211\u4eec\u53ea\u6709\u4e00\u4e9b\u6709\u9650\u7684\u6570\u636e\uff0c\u6bd4\u5982\u6765\u81ea\u56fd\u9645\u8c61\u68cb\u6bd4\u8d5b\u7684\u8bb0\u5f55\uff0c\u6216\u8005\u662f\u73a9\u5bb6\u73a9\u8d85\u7ea7\u9a6c\u91cc\u5965\u7684\u8bb0\u5f55\u3002\u8fd9\u4e9b\u6570\u636e\u53ef\u80fd\u65e0\u6cd5\u6db5\u76d6\u8db3\u591f\u591a\u7684\u300c\u72b6\u6001\u300d\u3002</p> <p>\u4e0d\u540c\u4e8e\u8fd9\u79cd\u9700\u8981\u5927\u91cf\u73b0\u6709\u7684\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u57fa\u4e8e\u8ba9\u8ba1\u7b97\u673a\u591a\u6b21\u73a9\u5e76\u89c2\u5bdf\u73a9\u7684\u7ed3\u679c\u7684\u60f3\u6cd5\u3002\u56e0\u6b64\uff0c\u8981\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6211\u4eec\u9700\u8981\u4e24\u4e2a\u8981\u7d20\uff1a</p> <ul> <li> <p>\u73af\u5883\u548c\u6a21\u62df\u5668\uff0c\u5b83\u4eec\u5141\u8bb8\u6211\u4eec\u591a\u6b21\u73a9\u6e38\u620f\u3002\u8be5\u6a21\u62df\u5668\u5e94\u8be5\u5b9a\u4e49\u6240\u6709\u6e38\u620f\u89c4\u5219\uff0c\u4ee5\u53ca\u53ef\u80fd\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u3002</p> </li> <li> <p>\u5956\u52b1\u51fd\u6570\uff0c\u5b83\u4f1a\u544a\u8bc9\u6211\u4eec\u6bcf\u4e2a\u6bcf\u4e00\u6b65\uff08\u6216\u8005\u6bcf\u5c40\u6e38\u620f\uff09\u7684\u8868\u73b0\u5982\u4f55\u3002</p> </li> </ul> <p>\u5176\u4ed6\u7c7b\u578b\u7684\u673a\u5668\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60 (RL) \u4e4b\u95f4\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e\uff0c\u5728 RL \u4e2d\uff0c\u6211\u4eec\u901a\u5e38\u5728\u5b8c\u6210\u6e38\u620f\u4e4b\u524d\uff0c\u90fd\u4e0d\u77e5\u9053\u6211\u4eec\u662f\u8d62\u8fd8\u662f\u8f93\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e0d\u80fd\u8bf4\u5355\u72ec\u7684\u67d0\u4e2a\u52a8\u4f5c\u662f\u4e0d\u662f\u300c\u597d\u300d\u7684 - \u6211\u4eec\u53ea\u4f1a\u5728\u6e38\u620f\u7ed3\u675f\u65f6\u83b7\u5f97\u5956\u52b1\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u8fd9\u79cd\u4e0d\u786e\u5b9a\u7684\u6761\u4ef6\u4e0b\u8bad\u7ec3\u6a21\u578b\u3002\u6211\u4eec\u5c06\u4e86\u89e3\u4e00\u79cd\u79f0\u4e3a Q-learning \u7684 RL \u7b97\u6cd5\u3002</p>"},{"location":"8-Reinforcement/README.zh-cn/#_4","title":"\u8bfe\u7a0b","text":"<ol> <li>\u5f3a\u5316\u5b66\u4e60\u548c Q-Learning \u4ecb\u7ecd</li> <li>\u4f7f\u7528 Gym \u6a21\u62df\u73af\u5883</li> </ol>"},{"location":"8-Reinforcement/README.zh-cn/#_5","title":"\u672c\u6587\u4f5c\u8005","text":"<p>\u201c\u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb\u201d \u7531 Dmitry Soshnikov \u7528 \u2665\ufe0f \u7f16\u5199</p>"},{"location":"8-Reinforcement/1-QLearning/","title":"Introduction to Reinforcement Learning and Q-Learning","text":"<p>Sketchnote by Tomomi Imura</p> <p>Reinforcement learning involves three important concepts: the agent, some states, and a set of actions per state. By executing an action in a specified state, the agent is given a reward. Again imagine the computer game Super Mario. You are Mario, you are in a game level, standing next to a cliff edge. Above you is a coin. You being Mario, in a game level, at a specific position ... that's your state. Moving one step to the right (an action) will take you over the edge, and that would give you a low numerical score. However, pressing the jump button would let you score a point and you would stay alive. That's a positive outcome and that should award you a positive numerical score.</p> <p>By using reinforcement learning and a simulator (the game), you can learn how to play the game to maximize the reward which is staying alive and scoring as many points as possible.</p> <p></p> <p>\ud83c\udfa5 Click the image above to hear Dmitry discuss Reinforcement Learning</p>"},{"location":"8-Reinforcement/1-QLearning/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"8-Reinforcement/1-QLearning/#prerequisites-and-setup","title":"Prerequisites and Setup","text":"<p>In this lesson, we will be experimenting with some code in Python. You should be able to run the Jupyter Notebook code from this lesson, either on your computer or somewhere in the cloud.</p> <p>You can open the lesson notebook and walk through this lesson to build.</p> <p>Note: If you are opening this code from the cloud, you also need to fetch the <code>rlboard.py</code> file, which is used in the notebook code. Add it to the same directory as the notebook.</p>"},{"location":"8-Reinforcement/1-QLearning/#introduction","title":"Introduction","text":"<p>In this lesson, we will explore the world of Peter and the Wolf, inspired by a musical fairy tale by a Russian composer, Sergei Prokofiev. We will use Reinforcement Learning to let Peter explore his environment, collect tasty apples and avoid meeting the wolf.</p> <p>Reinforcement Learning (RL) is a learning technique that allows us to learn an optimal behavior of an agent in some environment by running many experiments. An agent in this environment should have some goal, defined by a reward function.</p>"},{"location":"8-Reinforcement/1-QLearning/#the-environment","title":"The environment","text":"<p>For simplicity, let's consider Peter's world to be a square board of size <code>width</code> x <code>height</code>, like this:</p> <p></p> <p>Each cell in this board can either be:</p> <ul> <li>ground, on which Peter and other creatures can walk.</li> <li>water, on which you obviously cannot walk.</li> <li>a tree or grass, a place where you can rest.</li> <li>an apple, which represents something Peter would be glad to find in order to feed himself.</li> <li>a wolf, which is dangerous and should be avoided.</li> </ul> <p>There is a separate Python module, <code>rlboard.py</code>, which contains the code to work with this environment. Because this code is not important for understanding our concepts, we will import the module and use it to create the sample board (code block 1):</p> <pre><code>from rlboard import *\n\nwidth, height = 8,8\nm = Board(width,height)\nm.randomize(seed=13)\nm.plot()\n</code></pre> <p>This code should print a picture of the environment similar to the one above.</p>"},{"location":"8-Reinforcement/1-QLearning/#actions-and-policy","title":"Actions and policy","text":"<p>In our example, Peter's goal would be able to find an apple, while avoiding the wolf and other obstacles. To do this, he can essentially walk around until he finds an apple.</p> <p>Therefore, at any position, he can choose between one of the following actions: up, down, left and right.</p> <p>We will define those actions as a dictionary, and map them to pairs of corresponding coordinate changes. For example, moving right (<code>R</code>) would correspond to a pair <code>(1,0)</code>. (code block 2):</p> <pre><code>actions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\naction_idx = { a : i for i,a in enumerate(actions.keys()) }\n</code></pre> <p>To sum up, the strategy and goal of this scenario are as follows:</p> <ul> <li> <p>The strategy, of our agent (Peter) is defined by a so-called policy. A policy is a function that returns the action at any given state. In our case, the state of the problem is represented by the board, including the current position of the player.</p> </li> <li> <p>The goal, of reinforcement learning is to eventually learn a good policy that will allow us to solve the problem efficiently. However, as a baseline, let's consider the simplest policy called random walk.</p> </li> </ul>"},{"location":"8-Reinforcement/1-QLearning/#random-walk","title":"Random walk","text":"<p>Let's first solve our problem by implementing a random walk strategy. With random walk, we will randomly choose the next action from the allowed actions, until we reach the apple (code block 3).</p> <ol> <li> <p>Implement the random walk with the below code:</p> <pre><code>def random_policy(m):\n    return random.choice(list(actions))\n\ndef walk(m,policy,start_position=None):\n    n = 0 # number of steps\n    # set initial position\n    if start_position:\n        m.human = start_position \n    else:\n        m.random_start()\n    while True:\n        if m.at() == Board.Cell.apple:\n            return n # success!\n        if m.at() in [Board.Cell.wolf, Board.Cell.water]:\n            return -1 # eaten by wolf or drowned\n        while True:\n            a = actions[policy(m)]\n            new_pos = m.move_pos(m.human,a)\n            if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:\n                m.move(a) # do the actual move\n                break\n        n+=1\n\nwalk(m,random_policy)\n</code></pre> <p>The call to <code>walk</code> should return the length of the corresponding path, which can vary from one run to another. </p> </li> <li> <p>Run the walk experiment a number of times (say, 100), and print the resulting statistics (code block 4):</p> <pre><code>def print_statistics(policy):\n    s,w,n = 0,0,0\n    for _ in range(100):\n        z = walk(m,policy)\n        if z&lt;0:\n            w+=1\n        else:\n            s += z\n            n += 1\n    print(f\"Average path length = {s/n}, eaten by wolf: {w} times\")\n\nprint_statistics(random_policy)\n</code></pre> <p>Note that the average length of a path is around 30-40 steps, which is quite a lot, given the fact that the average distance to the nearest apple is around 5-6 steps.</p> <p>You can also see what Peter's movement looks like during the random walk:</p> <p></p> </li> </ol>"},{"location":"8-Reinforcement/1-QLearning/#reward-function","title":"Reward function","text":"<p>To make our policy more intelligent, we need to understand which moves are \"better\" than others. To do this, we need to define our goal.</p> <p>The goal can be defined in terms of a reward function, which will return some score value for each state. The higher the number, the better the reward function. (code block 5)</p> <pre><code>move_reward = -0.1\ngoal_reward = 10\nend_reward = -10\n\ndef reward(m,pos=None):\n    pos = pos or m.human\n    if not m.is_valid(pos):\n        return end_reward\n    x = m.at(pos)\n    if x==Board.Cell.water or x == Board.Cell.wolf:\n        return end_reward\n    if x==Board.Cell.apple:\n        return goal_reward\n    return move_reward\n</code></pre> <p>An interesting thing about reward functions is that in most cases, we are only given a substantial reward at the end of the game. This means that our algorithm should somehow remember \"good\" steps that lead to a positive reward at the end, and increase their importance. Similarly, all moves that lead to bad results should be discouraged.</p>"},{"location":"8-Reinforcement/1-QLearning/#q-learning","title":"Q-Learning","text":"<p>An algorithm that we will discuss here is called Q-Learning. In this algorithm, the policy is defined by a function (or a data structure) called a Q-Table. It records the \"goodness\" of each of the actions in a given state.</p> <p>It is called a Q-Table because it is often convenient to represent it as a table, or multi-dimensional array. Since our board has dimensions <code>width</code> x <code>height</code>, we can represent the Q-Table using a numpy array with shape <code>width</code> x <code>height</code> x <code>len(actions)</code>: (code block 6)</p> <pre><code>Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)\n</code></pre> <p>Notice that we initialize all the values of the Q-Table with an equal value, in our case - 0.25. This corresponds to the \"random walk\" policy, because all moves in each state are equally good. We can pass the Q-Table to the <code>plot</code> function in order to visualize the table on the board: <code>m.plot(Q)</code>.</p> <p></p> <p>In the center of each cell there is an \"arrow\" that indicates the preferred direction of movement. Since all directions are equal, a dot is displayed.</p> <p>Now we need to run the simulation, explore our environment, and learn a better distribution of Q-Table values, which will allow us to find the path to the apple much faster.</p>"},{"location":"8-Reinforcement/1-QLearning/#essence-of-q-learning-bellman-equation","title":"Essence of Q-Learning: Bellman Equation","text":"<p>Once we start moving, each action will have a corresponding reward, i.e. we can theoretically select the next action based on the highest immediate reward. However, in most states, the move will not achieve our goal of reaching the apple, and thus we cannot immediately decide which direction is better.</p> <p>Remember that it is not the immediate result that matters, but rather the final result, which we will obtain at the end of the simulation.</p> <p>In order to account for this delayed reward, we need to use the principles of dynamic programming, which allow us to think about out problem recursively.</p> <p>Suppose we are now at the state s, and we want to move to the next state s'. By doing so, we will receive the immediate reward r(s,a), defined by the reward function, plus some future reward. If we suppose that our Q-Table correctly reflects the \"attractiveness\" of each action, then at state s' we will chose an action a that corresponds to maximum value of Q(s',a'). Thus, the best possible future reward we could get at state s will be defined as <code>max</code><sub>a'</sub>Q(s',a') (maximum here is computed over all possible actions a' at state s').</p> <p>This gives the Bellman formula for calculating the value of the Q-Table at state s, given action a:</p> <p></p> <p>Here \u03b3 is the so-called discount factor that determines to which extent you should prefer the current reward over the future reward and vice versa.</p>"},{"location":"8-Reinforcement/1-QLearning/#learning-algorithm","title":"Learning Algorithm","text":"<p>Given the equation above, we can now write pseudo-code for our learning algorithm:</p> <ul> <li>Initialize Q-Table Q with equal numbers for all states and actions</li> <li>Set learning rate \u03b1 \u2190 1</li> <li>Repeat simulation many times</li> <li>Start at random position</li> <li>Repeat         1. Select an action a at state s         2. Execute action by moving to a new state s'         3. If we encounter end-of-game condition, or total reward is too small - exit simulation         4. Compute reward r at the new state         5. Update Q-Function according to Bellman equation: Q(s,a) \u2190 (1-\u03b1)Q(s,a)+\u03b1(r+\u03b3 max<sub>a'</sub>Q(s',a'))         6. s \u2190 s'         7. Update the total reward and decrease \u03b1.</li> </ul>"},{"location":"8-Reinforcement/1-QLearning/#exploit-vs-explore","title":"Exploit vs. explore","text":"<p>In the algorithm above, we did not specify how exactly we should choose an action at step 2.1. If we are choosing the action randomly, we will randomly explore the environment, and we are quite likely to die often as well as explore areas where we would not normally go. An alternative approach would be to exploit the Q-Table values that we already know, and thus to choose the best action (with higher Q-Table value) at state s. This, however, will prevent us from exploring other states, and it's likely we might not find the optimal solution.</p> <p>Thus, the best approach is to strike a balance between exploration and exploitation. This can be done by choosing the action at state s with probabilities proportional to values in the Q-Table. In the beginning, when Q-Table values are all the same, it would correspond to a random selection, but as we learn more about our environment, we would be more likely to follow the optimal route while allowing the agent to choose the unexplored path once in a while.</p>"},{"location":"8-Reinforcement/1-QLearning/#python-implementation","title":"Python implementation","text":"<p>We are now ready to implement the learning algorithm. Before we do that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions.</p> <ol> <li> <p>Create a function <code>probs()</code>:</p> <pre><code>def probs(v,eps=1e-4):\n    v = v-v.min()+eps\n    v = v/v.sum()\n    return v\n</code></pre> <p>We add a few <code>eps</code> to the original vector in order to avoid division by 0 in the initial case, when all components of the vector are identical.</p> </li> </ol> <p>Run them learning algorithm through 5000 experiments, also called epochs: (code block 8) <pre><code>    for epoch in range(5000):\n\n        # Pick initial point\n        m.random_start()\n\n        # Start travelling\n        n=0\n        cum_reward = 0\n        while True:\n            x,y = m.human\n            v = probs(Q[x,y])\n            a = random.choices(list(actions),weights=v)[0]\n            dpos = actions[a]\n            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode\n            r = reward(m)\n            cum_reward += r\n            if r==end_reward or cum_reward &lt; -1000:\n                lpath.append(n)\n                break\n            alpha = np.exp(-n / 10e5)\n            gamma = 0.5\n            ai = action_idx[a]\n            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n            n+=1\n</code></pre></p> <p>After executing this algorithm, the Q-Table should be updated with values that define the attractiveness of different actions at each step. We can try to visualize the Q-Table by plotting a vector at each cell that will point in the desired direction of movement. For simplicity, we draw a small circle instead of an arrow head.</p> <p></p>"},{"location":"8-Reinforcement/1-QLearning/#checking-the-policy","title":"Checking the policy","text":"<p>Since the Q-Table lists the \"attractiveness\" of each action at each state, it is quite easy to use it to define the efficient navigation in our world. In the simplest case, we can select the action corresponding to the highest Q-Table value: (code block 9)</p> <pre><code>def qpolicy_strict(m):\n        x,y = m.human\n        v = probs(Q[x,y])\n        a = list(actions)[np.argmax(v)]\n        return a\n\nwalk(m,qpolicy_strict)\n</code></pre> <p>If you try the code above several times, you may notice that sometimes it \"hangs\", and you need to press the STOP button in the notebook to interrupt it. This happens because there could be situations when two states \"point\" to each other in terms of optimal Q-Value, in which case the agents ends up moving between those states indefinitely.</p>"},{"location":"8-Reinforcement/1-QLearning/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Task 1: Modify the <code>walk</code> function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.</p> <p>Task 2: Modify the <code>walk</code> function so that it does not go back to the places where it has already been previously. This will prevent <code>walk</code> from looping, however, the agent can still end up being \"trapped\" in a location from which it is unable to escape.</p>"},{"location":"8-Reinforcement/1-QLearning/#navigation","title":"Navigation","text":"<p>A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that <code>print_statistics</code> runs the simulation 100 times): (code block 10)</p> <pre><code>def qpolicy(m):\n        x,y = m.human\n        v = probs(Q[x,y])\n        a = random.choices(list(actions),weights=v)[0]\n        return a\n\nprint_statistics(qpolicy)\n</code></pre> <p>After running this code, you should get a much smaller average path length than before, in the range of 3-6.</p>"},{"location":"8-Reinforcement/1-QLearning/#investigating-the-learning-process","title":"Investigating the learning process","text":"<p>As we have mentioned, the learning process is a balance between exploration and exploration of gained knowledge about the structure of problem space. We have seen that the results of learning (the ability to help an agent to find a short path to the goal) has improved, but it is also interesting to observe how the average path length behaves during the learning process:</p> <p></p> <p>The learnings can be summarized as:</p> <ul> <li> <p>Average path length increases. What we see here is that at first, the average path length increases. This is probably due to the fact that when we know nothing about the environment, we are likely to get trapped in bad states, water or wolf. As we learn more and start using this knowledge, we can explore the environment for longer, but we still do not know where the apples are very well.</p> </li> <li> <p>Path length decrease, as we learn more. Once we learn enough, it becomes easier for the agent to achieve the goal, and the path length starts to decrease. However, we are still open to exploration, so we often diverge away from the best path, and explore new options, making the path longer than optimal.</p> </li> <li> <p>Length increase abruptly. What we also observe on this graph is that at some point, the length increased abruptly. This indicates the stochastic nature of the process, and that we can at some point \"spoil\" the Q-Table coefficients by overwriting them with new values. This ideally should be minimized by decreasing learning rate (for example, towards the end of training, we only adjust Q-Table values by a small value).</p> </li> </ul> <p>Overall, it is important to remember that the success and quality of the learning process significantly depends on parameters, such as learning rate, learning rate decay, and discount factor. Those are often called hyperparameters, to distinguish them from parameters, which we optimize during training (for example, Q-Table coefficients). The process of finding the best hyperparameter values is called hyperparameter optimization, and it deserves a separate topic.</p>"},{"location":"8-Reinforcement/1-QLearning/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"8-Reinforcement/1-QLearning/#assignment","title":"Assignment","text":"<p>A More Realistic World</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/","title":"\u5f3a\u5316\u5b66\u4e60\u548c Q-Learning \u4ecb\u7ecd","text":"<p>\u4f5c\u8005 Tomomi Imura</p> <p>\u5f3a\u5316\u5b66\u4e60\u6d89\u53ca\u4e09\u4e2a\u91cd\u8981\u6982\u5ff5\uff1a\u4ee3\u7406\u3001\u4e00\u4e9b\u72b6\u6001\u548c\u6bcf\u4e2a\u72b6\u6001\u7684\u4e00\u7ec4\u52a8\u4f5c\u3002\u901a\u8fc7\u5728\u6307\u5b9a\u72b6\u6001\u4e0b\u6267\u884c\u4e00\u4e2a\u52a8\u4f5c\uff0c\u4ee3\u7406\u4f1a\u5f97\u5230\u5956\u52b1\u3002\u60f3\u8c61\u4e00\u4e0b\u7535\u8111\u6e38\u620f\u8d85\u7ea7\u9a6c\u91cc\u5965\u3002\u4f60\u662f\u9a6c\u91cc\u5965\uff0c\u4f60\u5728\u4e00\u4e2a\u6e38\u620f\u5173\u5361\u4e2d\uff0c\u7ad9\u5728\u60ac\u5d16\u8fb9\u4e0a\u3002\u5728\u4f60\u4e0a\u9762\u662f\u4e00\u679a\u786c\u5e01\u3002\u4f60\u662f\u9a6c\u91cc\u5965\uff0c\u5728\u6e38\u620f\u5173\u5361\u91cc\uff0c\u5728\u7279\u5b9a\u4f4d\u7f6e......\u8fd9\u5c31\u662f\u4f60\u7684\u72b6\u6001\u3002\u5411\u53f3\u79fb\u52a8\u4e00\u6b65\uff08\u4e00\u4e2a\u52a8\u4f5c\uff09\u4f1a\u8ba9\u8dcc\u4e0b\u60ac\u5d16\uff0c\u800c\u4e14\u4f1a\u5f97\u5230\u4e00\u4e2a\u4f4e\u5206\u3002\u7136\u800c\uff0c\u6309\u4e0b\u8df3\u8dc3\u6309\u94ae\u4f1a\u8ba9\u4f60\u6d3b\u4e0b\u6765\u5e76\u5f97\u5206\u3002\u8fd9\u662f\u4e00\u4e2a\u79ef\u6781\u7684\u7ed3\u679c\uff0c\u5b83\u4f1a\u7ed9\u4f60\u4e00\u4e2a\u79ef\u6781\u3001\u6b63\u5411\u7684\u5206\u6570\u3002</p> <p>\u901a\u8fc7\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u62df\u5668\uff08\u6e38\u620f\uff09\uff0c\u4f60\u53ef\u4ee5\u5b66\u4e60\u5982\u4f55\u73a9\u6e38\u620f\u4ee5\u6700\u5927\u5316\u5956\u52b1\uff0c\u65e2\u80fd\u591f\u6d3b\u4e0b\u53bb\u8fd8\u53ef\u4ee5\u83b7\u5f97\u5c3d\u53ef\u80fd\u591a\u7684\u79ef\u5206\u3002</p> <p></p> <p>\ud83c\udfa5 \u70b9\u51fb\u4e0a\u56fe\u89c2\u770b Dmitry \u8ba8\u8bba\u5f3a\u5316\u5b66\u4e60</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_2","title":"\u5148\u51b3\u6761\u4ef6\u548c\u8bbe\u7f6e","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u7528 Python \u4ee3\u7801\u505a\u4e00\u4e9b\u8bd5\u9a8c\u3002\u4f60\u5e94\u8be5\u80fd\u591f\u5728\u4f60\u7684\u8ba1\u7b97\u673a\u4e0a\u6216\u4e91\u4e2d\u7684\u67d0\u4e2a\u5730\u65b9\u8fd0\u884c\u672c\u8bfe\u7a0b\u4e2d\u7684 Jupyter Notebook \u4ee3\u7801\u3002</p> <p>\u4f60\u53ef\u4ee5\u6253\u5f00\u8bfe\u672c\u7b14\u8bb0\u672c \u5e76\u901a\u8fc7\u5b66\u4e60\u672c\u8bfe\u8fdb\u884c\u7f16\u8bd1\u3001\u8fd0\u884c\u3002</p> <p>\u6ce8\u610f\uff1a \u5982\u679c\u4f60\u662f\u4ece\u4e91\u7aef\u6253\u5f00\u6b64\u4ee3\u7801\uff0c\u4f60\u8fd8\u9700\u8981\u83b7\u53d6\u7b14\u8bb0\u672c\u4ee3\u7801\u4e2d\u4f7f\u7528\u7684 <code>rlboard.py</code> \u6587\u4ef6\u3002\u5c06\u5176\u6dfb\u52a0\u5230\u4e0e\u7b14\u8bb0\u672c\u76f8\u540c\u7684\u76ee\u5f55\u4e2d\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_3","title":"\u4ecb\u7ecd","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u7d22 \u5f7c\u5f97\u4e0e\u72fc \u7684\u4e16\u754c\uff0c\u5176\u7075\u611f\u6765\u81ea\u4fc4\u7f57\u65af\u4f5c\u66f2\u5bb6 Sergei Prokofiev\u7684\u97f3\u4e50\u7ae5\u8bdd\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8ba9\u5f7c\u5f97\u63a2\u7d22\u4ed6\u7684\u73af\u5883\uff0c\u6536\u96c6\u7f8e\u5473\u7684\u82f9\u679c\u5e76\u907f\u514d\u9047\u5230\u72fc\u3002</p> <p>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u4e00\u79cd\u5b66\u4e60\u6280\u672f\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u901a\u8fc7\u8fd0\u884c\u8bb8\u591a\u5b9e\u9a8c\u6765\u5b66\u4e60\u4ee3\u7406\u5728\u67d0\u4e9b\u73af\u5883\u4e2d\u7684\u6700\u4f73\u884c\u4e3a\u3002\u8fd9\u79cd\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u5e94\u8be5\u6709\u4e00\u4e9b\u76ee\u6807\uff0c\u7531\u5956\u52b1\u51fd\u6570\u5b9a\u4e49\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_4","title":"\u73af\u5883","text":"<p>\u4e3a\u7b80\u5355\u8d77\u89c1\uff0c\u8ba9\u6211\u4eec\u5c06 Peter \u7684\u4e16\u754c\u89c6\u4e3a\u4e00\u4e2a\u5927\u5c0f\u4e3a <code>width</code> x <code>height</code> \u7684\u65b9\u677f\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <p></p> <p>\u8be5\u677f\u4e2d\u7684\u6bcf\u4e2a\u5355\u5143\u683c\u53ef\u4ee5\u662f\uff1a</p> <ul> <li>\u5730\u9762\uff0c\u5f7c\u5f97\u548c\u5176\u4ed6\u751f\u7269\u53ef\u4ee5\u5728\u4e0a\u9762\u884c\u8d70\u3002</li> <li>\u6c34\uff0c\u4e0d\u80fd\u5728\u4e0a\u9762\u884c\u8d70\u3002</li> <li>\u6811\u6216\u8349\uff0c\u53ef\u4ee5\u4f11\u606f\u7684\u5730\u65b9\u3002</li> <li>\u82f9\u679c\uff0c\u4ee3\u8868\u5f7c\u5f97\u5e0c\u671b\u627e\u5230\u7528\u6765\u5582\u9971\u81ea\u5df1\u7684\u98df\u7269\u3002</li> <li>\u72fc\uff0c\u8fd9\u662f\u5371\u9669\u7684\uff0c\u5e94\u8be5\u907f\u514d\u9047\u5230\u3002</li> </ul> <p>\u6709\u4e00\u4e2a\u5355\u72ec\u7684 Python \u6a21\u5757 <code>rlboard.py</code>\uff0c\u5176\u4e2d\u5305\u542b\u5728\u6b64\u73af\u5883\u4e2d\u5de5\u4f5c\u7684\u4ee3\u7801\u3002\u56e0\u4e3a\u8fd9\u6bb5\u4ee3\u7801\u5bf9\u4e8e\u7406\u89e3\u6211\u4eec\u7684\u6982\u5ff5\u5e76\u4e0d\u91cd\u8981\uff0c\u6211\u4eec\u5c06\u5bfc\u5165\u8be5\u6a21\u5757\u5e76\u4f7f\u7528\u5b83\u6765\u521b\u5efa\u793a\u4f8b\u677f\uff08\u4ee3\u7801\u5757 1\uff09\uff1a</p> <pre><code>from rlboard import *\n\nwidth, height = 8,8\nm = Board(width,height)\nm.randomize(seed=13)\nm.plot()\n</code></pre> <p>\u8fd9\u6bb5\u4ee3\u7801\u4f1a\u6253\u5370\u4e00\u5f20\u7c7b\u4f3c\u4e8e\u4e0a\u9762\u7684\u73af\u5883\u56fe\u7247\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_5","title":"\u884c\u52a8\u548c\u7b56\u7565","text":"<p>\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u5f7c\u5f97\u7684\u76ee\u6807\u662f\u627e\u5230\u82f9\u679c\uff0c\u540c\u65f6\u907f\u5f00\u72fc\u548c\u5176\u4ed6\u969c\u788d\u7269\u3002\u4e3a\u6b64\uff0c\u4ed6\u53ef\u4ee5\u56db\u5904\u8d70\u52a8\uff0c\u76f4\u5230\u627e\u5230\u4e00\u4e2a\u82f9\u679c\u3002</p> <p>\u56e0\u6b64\uff0c\u5728\u4efb\u4f55\u4f4d\u7f6e\uff0c\u4ed6\u90fd\u53ef\u4ee5\u9009\u62e9\u4ee5\u4e0b\u52a8\u4f5c\u4e4b\u4e00\uff1a\u5411\u4e0a\u3001\u5411\u4e0b\u3001\u5411\u5de6\u548c\u5411\u53f3\u3002</p> <p>\u6211\u4eec\u5c06\u8fd9\u4e9b\u52a8\u4f5c\u5b9a\u4e49\u4e3a\u5b57\u5178\uff0c\u5e76\u5c06\u5b83\u4eec\u6620\u5c04\u5230\u76f8\u5e94\u7684\u5750\u6807\u53d8\u5316\u5bf9\u3002\u4f8b\u5982\uff0c\u5411\u53f3\u79fb\u52a8 (<code>R</code>) \u5c06\u5bf9\u5e94\u4e8e\u4e00\u5bf9 <code>(1,0)</code>\u3002\uff08\u4ee3\u7801\u5757 2\uff09\uff1a</p> <pre><code>actions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\naction_idx = { a : i for i,a in enumerate(actions.keys()) }\n</code></pre> <p>\u7efc\u4e0a\u6240\u8ff0\uff0c\u672c\u573a\u666f\u7684\u7b56\u7565\u548c\u76ee\u6807\u5982\u4e0b\uff1a</p> <ul> <li> <p>\u6211\u4eec\u7684\u4ee3\u7406\uff08\u5f7c\u5f97\uff09\u7684\u7b56\u7565\u7531\u4e00\u4e2a\u51fd\u6570\u5b9a\u4e49\uff0c\u5b83\u8fd4\u56de\u4efb\u4f55\u7ed9\u5b9a\u72b6\u6001\u4e0b\u7684\u52a8\u4f5c\u3002\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u95ee\u9898\u7684\u72b6\u6001\u7531\u68cb\u76d8\u8868\u793a\uff0c\u5305\u62ec\u73a9\u5bb6\u7684\u5f53\u524d\u4f4d\u7f6e\u3002</p> </li> <li> <p>\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u7684\u662f\u5b66\u4e60\u4e00\u4e2a\u597d\u7684\u7b56\u7565\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002\u4f46\u662f\uff0c\u4f5c\u4e3a\u57fa\u51c6\uff0c\u8ba9\u6211\u4eec\u8003\u8651\u79f0\u4e3a \u968f\u673a\u8d70\u52a8 \u7684\u6700\u7b80\u5355\u7b56\u7565\u3002</p> </li> </ul>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_6","title":"\u968f\u673a\u8d70\u52a8","text":"<p>\u8ba9\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5b9e\u65bd\u968f\u673a\u8d70\u52a8\u7b56\u7565\u6765\u89e3\u51b3\u6211\u4eec\u7684\u95ee\u9898\u3002\u901a\u8fc7\u968f\u673a\u8d70\u52a8\uff0c\u6211\u4eec\u4ece\u5141\u8bb8\u7684\u52a8\u4f5c\u4e2d\u968f\u673a\u9009\u62e9\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u76f4\u5230\u6211\u4eec\u627e\u5230\u82f9\u679c\uff08\u4ee3\u7801\u5757 3\uff09\u3002</p> <ol> <li> <p>\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u5b9e\u73b0\u968f\u673a\u8d70\u52a8\uff1a</p> <pre><code>def random_policy(m):\n    return random.choice(list(actions))\n\ndef walk(m,policy,start_position=None):\n    n = 0 # number of steps\n    # set initial position\n    if start_position:\n        m.human = start_position\n    else:\n        m.random_start()\n    while True:\n        if m.at() == Board.Cell.apple:\n            return n # success!\n        if m.at() in [Board.Cell.wolf, Board.Cell.water]:\n            return -1 # eaten by wolf or drowned\n        while True:\n            a = actions[policy(m)]\n            new_pos = m.move_pos(m.human,a)\n            if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:\n                m.move(a) # do the actual move\n                break\n        n+=1\n\nwalk(m,random_policy)\n</code></pre> <p>\u5bf9 <code>walk</code> \u7684\u8c03\u7528\u5e94\u8fd4\u56de\u76f8\u5e94\u8def\u5f84\u7684\u957f\u5ea6\uff0c\u8be5\u957f\u5ea6\u53ef\u80fd\u56e0\u6bcf\u6b21\u8fd0\u884c\u800c\u4e0d\u540c\u3002 </p> </li> <li> <p>\u591a\u6b21\u8fd0\u884c\u8be5\u5b9e\u9a8c\uff08\u4f8b\u5982 100 \u6b21\uff09\uff0c\u5e76\u6253\u5370\u7ed3\u679c\u7edf\u8ba1\u4fe1\u606f\uff08\u4ee3\u7801\u5757 4\uff09\uff1a</p> <p>```python def print_statistics(policy):     s,w,n = 0,0,0     for _ in range(100):         z = walk(m,policy)         if z&lt;0:             w+=1         else:             s += z             n += 1     print(f\"Average path length = {s/n}, eaten by wolf: {w} times\")</p> <p>print_statistics(random_policy)    ```</p> <p>\u8bf7\u6ce8\u610f\uff0c\u4e00\u6761\u8def\u5f84\u7684\u5e73\u5747\u957f\u5ea6\u7ea6\u4e3a 30-40 \u6b65\uff0c\u8003\u8651\u5230\u5230\u6700\u8fd1\u82f9\u679c\u7684\u5e73\u5747\u8ddd\u79bb\u7ea6\u4e3a 5-6 \u6b65\uff0c\u8fd9\u4e00\u6570\u5b57\u76f8\u5f53\u5927\u3002</p> <p>\u4f60\u8fd8\u53ef\u4ee5\u770b\u5230 Peter \u5728\u968f\u673a\u8d70\u52a8\u8fc7\u7a0b\u4e2d\u7684\u8fd0\u52a8\u60c5\u51b5\uff1a</p> <p></p> </li> </ol>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_7","title":"\u5956\u52b1\u51fd\u6570","text":"<p>\u4e3a\u4e86\u4f7f\u6211\u4eec\u7684\u7b56\u7565\u66f4\u52a0\u667a\u80fd\uff0c\u6211\u4eec\u9700\u8981\u4e86\u89e3\u54ea\u4e9b\u52a8\u4f5c\u6bd4\u5176\u4ed6\u52a8\u4f5c \"\u66f4\u597d\"\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5b9a\u4e49\u6211\u4eec\u7684\u76ee\u6807\u3002</p> <p>\u53ef\u4ee5\u6839\u636e\u5956\u52b1\u51fd\u6570\u6765\u5b9a\u4e49\u76ee\u6807\uff0c\u8be5\u51fd\u6570\u5c06\u4e3a\u6bcf\u4e2a\u72b6\u6001\u8fd4\u56de\u4e00\u4e9b\u5206\u6570\u503c\u3002\u6570\u5b57\u8d8a\u5927\uff0c\u5956\u52b1\u51fd\u6570\u8d8a\u597d\u3002\uff08\u4ee3\u7801\u5757 5\uff09</p> <pre><code>move_reward = -0.1\ngoal_reward = 10\nend_reward = -10\n\ndef reward(m,pos=None):\n    pos = pos \u6216 m.human\n    if not m.is_valid(pos):\n        return end_reward\n    x = m.at(pos)\n    if x==Board.Cell.water or x == Board.Cell.wolf\uff1a\n        return end_reward\n    if x==Board.Cell.apple\uff1a\n        return goal_reward\n    return move_reward\n</code></pre> <p>\u5173\u4e8e\u5956\u52b1\u51fd\u6570\u7684\u4e00\u4e2a\u6709\u8da3\u7684\u4e8b\u60c5\u662f\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ea\u5728\u6e38\u620f\u7ed3\u675f\u65f6\u624d\u5f97\u5230\u5b9e\u8d28\u6027\u7684\u5956\u52b1\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u7b97\u6cd5\u5e94\u8be5\u4ee5\u67d0\u79cd\u65b9\u5f0f\u8bb0\u4f4f\u6700\u7ec8\u5bfc\u81f4\u79ef\u6781\u5956\u52b1\u7684\"\u597d\"\u6b65\u9aa4\uff0c\u5e76\u589e\u52a0\u5b83\u4eec\u7684\u91cd\u8981\u6027\u3002\u540c\u6837\uff0c\u6240\u6709\u5bfc\u81f4\u4e0d\u826f\u7ed3\u679c\u7684\u4e3e\u52a8\u90fd\u5e94\u8be5\u88ab\u963b\u62e6\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#q-learning_1","title":"Q-Learning","text":"<p>\u6211\u4eec\u5c06\u5728\u8fd9\u91cc\u8ba8\u8bba\u7684\u4e00\u79cd\u53eb\u505a Q-Learning \u7684\u7b97\u6cd5\u3002\u5728\u8be5\u7b97\u6cd5\u4e2d\uff0c\u7b56\u7565\u7531\u79f0\u4e3a Q-Table \u7684\u51fd\u6570\uff08\u6216\u6570\u636e\u7ed3\u6784\uff09\u5b9a\u4e49\u3002\u5b83\u8bb0\u5f55\u4e86\u7ed9\u5b9a\u72b6\u6001\u4e0b\u6bcf\u4e2a\u52a8\u4f5c\u7684\"\u4f18\u70b9\"\u3002</p> <p>\u4e4b\u6240\u4ee5\u79f0\u4e3a Q-Table\uff0c\u662f\u56e0\u4e3a\u8868\u683c\u6216\u591a\u7ef4\u6570\u7ec4\u901a\u5e38\u8868\u793a\u8d77\u6765\u5f88\u65b9\u4fbf\u3002\u7531\u4e8e\u6211\u4eec\u7684\u68cb\u76d8\u5c3a\u5bf8\u4e3a \"width\"x\"height\"\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5f62\u72b6\u4e3a \"width\"x\"height\"x\"len(actions) \u7684 numpy \u6570\u7ec4\u6765\u8868\u793a Q-Table\uff1a\uff08\u4ee3\u7801\u57576\uff09</p> <pre><code>Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5c06 Q-Table \u7684\u6240\u6709\u503c\u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u76f8\u7b49\u7684\u503c\uff0c\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u4e3a \"-0.25\"\u3002\u8fd9\u5bf9\u5e94\u4e8e\"\u968f\u673a\u8d70\u52a8\"\u7b56\u7565\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u72b6\u6001\u4e2d\u7684\u6240\u6709\u79fb\u52a8\u90fd\u540c\u6837\u597d\u3002\u6211\u4eec\u53ef\u4ee5\u5c06 Q-Table \u4f20\u9012\u7ed9 <code>plot</code> \u51fd\u6570\uff0c\u4ee5\u4fbf\u5728\u677f\u4e0a\u53ef\u89c6\u5316\u8868\u683c\uff1a<code>m.plot(Q)</code>\u3002</p> <p></p> <p>\u5728\u6bcf\u4e2a\u5355\u5143\u683c\u7684\u4e2d\u5fc3\u6709\u4e00\u4e2a\"\u7bad\u5934\"\uff0c\u8868\u793a\u9996\u9009\u7684\u79fb\u52a8\u65b9\u5411\u3002\u7531\u4e8e\u6240\u6709\u65b9\u5411\u90fd\u76f8\u7b49\uff0c\u56e0\u6b64\u663e\u793a\u4e00\u4e2a\u70b9\u3002</p> <p>\u73b0\u5728\u6211\u4eec\u9700\u8981\u8fd0\u884c\u8fd9\u4e2a\u7a0b\u5e8f\uff0c\u63a2\u7d22\u6211\u4eec\u7684\u73af\u5883\uff0c\u5e76\u5b66\u4e60\u66f4\u597d\u7684 Q-Table \u503c\u5206\u5e03\uff0c\u8fd9\u5c06\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u5feb\u5730\u627e\u5230\u901a\u5f80\u82f9\u679c\u7684\u8def\u5f84\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#q-learning_2","title":"Q-Learning \u7684\u672c\u8d28\uff1a\u8d1d\u5c14\u66fc\u65b9\u7a0b","text":"<p>\u4e00\u65e6\u6211\u4eec\u5f00\u59cb\u79fb\u52a8\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u4f1a\u6709\u76f8\u5e94\u7684\u5956\u52b1\uff0c\u5373\u7406\u8bba\u4e0a\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6700\u9ad8\u7684\u5373\u65f6\u5956\u52b1\u6765\u9009\u62e9\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3002\u4f46\u662f\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\uff0c\u6b64\u4e3e\u4e0d\u4f1a\u5b9e\u73b0\u6211\u4eec\u5230\u8fbe\u82f9\u679c\u7684\u76ee\u6807\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u7acb\u5373\u51b3\u5b9a\u54ea\u4e2a\u65b9\u5411\u66f4\u597d\u3002</p> <p>\u8bf7\u8bb0\u4f4f\uff0c\u91cd\u8981\u7684\u4e0d\u662f\u76f4\u63a5\u7ed3\u679c\uff0c\u800c\u662f\u6211\u4eec\u5c06\u5728\u6a21\u62df\u7ed3\u675f\u65f6\u83b7\u5f97\u7684\u6700\u7ec8\u7ed3\u679c\u3002</p> <p>\u4e3a\u4e86\u89e3\u91ca\u8fd9\u79cd\u5ef6\u8fdf\u5956\u52b1\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u52a8\u6001\u89c4\u5212 \u7684\u539f\u5219\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u9012\u5f52\u5730\u601d\u8003\u95ee\u9898\u3002</p> <p>\u5047\u8bbe\u6211\u4eec\u73b0\u5728\u5904\u4e8e\u72b6\u6001 s\uff0c\u5e76\u4e14\u6211\u4eec\u60f3\u8981\u79fb\u52a8\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001 s'\u3002\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u6211\u4eec\u5c06\u6536\u5230\u7531\u5956\u52b1\u51fd\u6570\u5b9a\u4e49\u7684\u5373\u65f6\u5956\u52b1 r(s,a)\uff0c\u4ee5\u53ca\u4e00\u4e9b\u672a\u6765\u7684\u5956\u52b1\u3002\u5982\u679c\u6211\u4eec\u5047\u8bbe\u6211\u4eec\u7684 Q-Table \u6b63\u786e\u53cd\u6620\u4e86\u6bcf\u4e2a\u52a8\u4f5c\u7684\u201c\u5438\u5f15\u529b\u201d\uff0c\u90a3\u4e48\u5728\u72b6\u6001 s' \u6211\u4eec\u5c06\u9009\u62e9\u5bf9\u5e94\u4e8e Q(s',a') \u6700\u5927\u503c\u7684\u52a8\u4f5c a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u72b6\u6001 s \u83b7\u5f97\u7684\u6700\u4f73\u672a\u6765\u5956\u52b1\u5c06\u88ab\u5b9a\u4e49\u4e3a <code>max</code><sub>a'</sub>Q(s',a')\uff08\u8fd9\u91cc\u7684\u6700\u5927\u503c\u662f\u5728\u72b6\u6001 s' \u65f6\u6240\u6709\u53ef\u80fd\u7684\u52a8\u4f5c a' \u4e0a\u8ba1\u7b97\u7684)\u3002</p> <p>\u8fd9\u7ed9\u51fa\u4e86 Bellman \u516c\u5f0f\uff0c\u7528\u4e8e\u8ba1\u7b97\u72b6\u6001 s \u7684 Q-Table \u503c\uff0c\u7ed9\u5b9a\u52a8\u4f5c a\uff1a</p> <p></p> <p>\u8fd9\u91cc \u03b3 \u662f\u6240\u8c13\u7684\u6298\u6263\u56e0\u5b50\uff0c\u5b83\u51b3\u5b9a\u4e86\u4f60\u5e94\u8be5\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u66f4\u559c\u6b22\u5f53\u524d\u7684\u5956\u52b1\u800c\u4e0d\u662f\u672a\u6765\u7684\u5956\u52b1\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_8","title":"\u5b66\u4e60\u7b97\u6cd5","text":"<p>\u9274\u4e8e\u4e0a\u9762\u7684\u7b49\u5f0f\uff0c\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u4e3a\u6211\u4eec\u7684\u5b66\u4e60\u7b97\u6cd5\u7f16\u5199\u4f2a\u4ee3\u7801\uff1a</p> <ul> <li>\u7528\u76f8\u540c\u7684\u6570\u5b57\u4e3a\u6240\u6709\u72b6\u6001\u548c\u52a8\u4f5c\u521d\u59cb\u5316 Q-Table Q</li> <li>\u8bbe\u7f6e\u5b66\u4e60\u7387\u03b1 \u2190 1</li> <li>\u591a\u6b21\u91cd\u590d\u6a21\u62df</li> <li>\u968f\u673a\u4f4d\u7f6e\u5f00\u59cb</li> <li>\u91cd\u590d         1. \u5728\u72b6\u6001 s \u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c a         2.\u901a\u8fc7\u79fb\u52a8\u5230\u65b0\u72b6\u6001 s' \u6765\u6267\u884c\u52a8\u4f5c         3.\u5982\u679c\u6211\u4eec\u9047\u5230\u6e38\u620f\u7ed3\u675f\u7684\u60c5\u51b5\uff0c\u6216\u8005\u603b\u5956\u52b1\u592a\u5c11\u2014\u2014\u9000\u51fa\u6a21\u62df         4. \u8ba1\u7b97\u65b0\u72b6\u6001\u4e0b\u7684\u5956\u52b1 r         5. \u6839\u636e Bellman \u65b9\u7a0b\u66f4\u65b0 Q-Function\uff1a Q(s,a) \u2190 (1-\u03b1)Q(s,a)+\u03b1(r+\u03b3 max<sub>a'</sub>Q( s',a'))         6. s \u2190 s'         7. \u66f4\u65b0\u603b\u5956\u52b1\u5e76\u51cf\u5c11 \u03b1\u3002</li> </ul>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_9","title":"\u5229\u7528\u4e0e\u63a2\u7d22","text":"<p>\u5728\u4e0a\u9762\u7684\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u5728\u6b65\u9aa4 2.1 \u4e2d\u6211\u4eec\u5e94\u8be5\u5982\u4f55\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\u3002\u5982\u679c\u6211\u4eec\u968f\u673a\u9009\u62e9\u52a8\u4f5c\uff0c\u6211\u4eec\u4f1a\u968f\u673a\u63a2\u7d22\u73af\u5883\uff0c\u6211\u4eec\u5f88\u53ef\u80fd\u4f1a\u7ecf\u5e38\u6b7b\u4ea1\u4ee5\u53ca\u63a2\u7d22\u6211\u4eec\u901a\u5e38\u4e0d\u4f1a\u53bb\u7684\u533a\u57df\u3002\u53e6\u4e00\u79cd\u65b9\u6cd5\u662f\u5229\u7528\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u7684 Q-Table \u503c\uff0c\u4ece\u800c\u5728\u72b6\u6001 s \u9009\u62e9\u6700\u4f73\u52a8\u4f5c\uff08\u5177\u6709\u66f4\u9ad8\u7684 Q-Table \u503c\uff09\u3002\u7136\u800c\uff0c\u8fd9\u5c06\u963b\u6b62\u6211\u4eec\u63a2\u7d22\u5176\u4ed6\u72b6\u6001\uff0c\u800c\u4e14\u6211\u4eec\u53ef\u80fd\u627e\u4e0d\u5230\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u56e0\u6b64\uff0c\u6700\u597d\u7684\u65b9\u6cd5\u662f\u5728\u63a2\u7d22\u548c\u5f00\u53d1\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u9009\u62e9\u72b6\u6001 s \u7684\u52a8\u4f5c\u6765\u5b8c\u6210\uff0c\u6982\u7387\u4e0e Q \u8868\u4e2d\u7684\u503c\u6210\u6b63\u6bd4\u3002\u4e00\u5f00\u59cb\uff0c\u5f53 Q-Table \u503c\u90fd\u76f8\u540c\u65f6\uff0c\u5b83\u5c06\u5bf9\u5e94\u4e8e\u968f\u673a\u9009\u62e9\uff0c\u4f46\u662f\u968f\u7740\u6211\u4eec\u5bf9\u73af\u5883\u7684\u4e86\u89e3\u8d8a\u6765\u8d8a\u591a\uff0c\u6211\u4eec\u5c06\u66f4\u6709\u53ef\u80fd\u9075\u5faa\u6700\u4f73\u8def\u7ebf\uff0c\u540c\u65f6\u5141\u8bb8\u667a\u80fd\u4f53\u5076\u5c14\u9009\u62e9\u672a\u63a2\u7d22\u7684\u8def\u5f84\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#python","title":"Python \u5b9e\u73b0","text":"<p>\u6211\u4eec\u73b0\u5728\u51c6\u5907\u5b9e\u73b0\u5b66\u4e60\u7b97\u6cd5\u3002\u5728\u6211\u4eec\u8fd9\u6837\u505a\u4e4b\u524d\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u4e00\u4e9b\u51fd\u6570\u6765\u5c06 Q-Table \u4e2d\u7684\u4efb\u610f\u6570\u5b57\u8f6c\u6362\u4e3a\u76f8\u5e94\u52a8\u4f5c\u7684\u6982\u7387\u5411\u91cf\u3002</p> <ol> <li> <p>\u521b\u5efa\u4e00\u4e2a\u51fd\u6570 <code>probs()</code>\uff1a</p> <pre><code>def probs(v,eps=1e-4):\n    v = vv.min()+eps\n    v = v/v.sum()\n    return v\n</code></pre> <p>\u6211\u4eec\u5411\u539f\u59cb\u5411\u91cf\u6dfb\u52a0\u4e86\u4e00\u4e9b <code>eps</code>\uff0c\u4ee5\u907f\u514d\u5728\u521d\u59cb\u60c5\u51b5\u4e0b\u88ab 0 \u9664\uff0c\u6b64\u65f6\u5411\u91cf\u7684\u6240\u6709\u5206\u91cf\u90fd\u76f8\u540c\u3002</p> </li> </ol> <p>\u901a\u8fc7 5000 \u6b21\u5b9e\u9a8c\u8fd0\u884c\u4ed6\u4eec\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e5f\u79f0\u4e3a epochs\uff1a\uff08\u4ee3\u7801\u5757 8\uff09</p> <pre><code>   for epoch in range(5000):\n\n        # Pick initial point\n        m.random_start()\n\n        # Start travelling\n        n=0\n        cum_reward = 0\n        while True:\n            x,y = m.human\n            v = probs(Q[x,y])\n            a = random.choices(list(actions),weights=v)[0]\n            dpos = actions[a]\n            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode\n            r = reward(m)\n            cum_reward += r\n            if r==end_reward or cum_reward &lt; -1000:\n                lpath.append(n)\n                break\n            alpha = np.exp(-n / 10e5)\n            gamma = 0.5\n            ai = action_idx[a]\n            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n            n+=1\n</code></pre> <p>\u6267\u884c\u6b64\u7b97\u6cd5\u540e\uff0c\u5e94\u4f7f\u7528\u5b9a\u4e49\u6bcf\u4e2a\u6b65\u9aa4\u4e0d\u540c\u52a8\u4f5c\u7684\u5438\u5f15\u529b\u7684\u503c\u66f4\u65b0 Q-Table \u3002\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u901a\u8fc7\u5728\u6bcf\u4e2a\u5355\u5143\u683c\u4e0a\u7ed8\u5236\u4e00\u4e2a\u5411\u91cf\u6765\u53ef\u89c6\u5316 Q-Table\uff0c\u8be5\u5411\u91cf\u5c06\u6307\u5411\u6240\u9700\u7684\u79fb\u52a8\u65b9\u5411\u3002\u4e3a\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u753b\u4e00\u4e2a\u5c0f\u5706\u5708\u800c\u4e0d\u662f\u7bad\u5934\u3002</p> <p></p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_10","title":"\u68c0\u67e5\u7b56\u7565","text":"<p>\u7531\u4e8e Q-Table \u5217\u51fa\u4e86\u6bcf\u4e2a\u72b6\u6001\u4e0b\u6bcf\u4e2a\u52a8\u4f5c\u7684\"\u5438\u5f15\u529b\"\uff0c\u56e0\u6b64\u5f88\u5bb9\u6613\u4f7f\u7528\u5b83\u6765\u5b9a\u4e49\u6211\u4eec\u4e16\u754c\u4e2d\u7684\u9ad8\u6548\u5bfc\u822a\u3002\u5728\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u6700\u9ad8 Q-Table \u503c\u5bf9\u5e94\u7684 action\uff1a\uff08\u4ee3\u7801\u57579\uff09</p> <pre><code>def qpolicy_strict(m):\n    x,y = m.human\n    v = probs(Q[x,y])\n    a = list(actions)[np.argmax(v)]\n    return a\n\nwalk(m,qpolicy_strict)\n</code></pre> <p>\u5982\u679c\u4f60\u591a\u6b21\u5c1d\u8bd5\u4e0a\u9762\u7684\u4ee3\u7801\uff0c\u4f60\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\u5b83\u6709\u65f6\u4f1a\"\u6302\u8d77\"\uff0c\u4f60\u9700\u8981\u6309\u7b14\u8bb0\u672c\u4e2d\u7684 STOP \u6309\u94ae\u6765\u4e2d\u65ad\u5b83\u3002\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\u662f\u56e0\u4e3a\u53ef\u80fd\u5b58\u5728\u4e24\u79cd\u72b6\u6001\u5728\u6700\u4f73 Q \u503c\u65b9\u9762\"\u6307\u5411\"\u5f7c\u6b64\u7684\u60c5\u51b5\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4ee3\u7406\u6700\u7ec8\u4f1a\u5728\u8fd9\u4e9b\u72b6\u6001\u4e4b\u95f4\u65e0\u9650\u671f\u5730\u79fb\u52a8\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_11","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u4efb\u52a1 1\uff1a \u4fee\u6539 <code>walk</code> \u51fd\u6570\uff0c\u5c06\u8def\u5f84\u7684\u6700\u5927\u957f\u5ea6\u9650\u5236\u4e3a\u4e00\u5b9a\u7684\u6b65\u6570\uff08\u6bd4\u5982 100\uff09\uff0c\u5e76\u65f6\u4e0d\u65f6\u5730\u89c2\u5bdf\u4e0a\u9762\u7684\u4ee3\u7801\u8fd4\u56de\u503c\u3002</p> <p>\u4efb\u52a1 2\uff1a \u4fee\u6539 <code>walk</code> \u51fd\u6570\uff0c\u4f7f\u5176\u4e0d\u4f1a\u56de\u5230\u4e4b\u524d\u5df2\u7ecf\u53bb\u8fc7\u7684\u5730\u65b9\u3002\u8fd9\u5c06\u9632\u6b62 <code>walk</code> \u5faa\u73af\uff0c\u4f46\u662f\uff0c\u4ee3\u7406\u4ecd\u7136\u53ef\u80fd\u6700\u7ec8\"\u88ab\u56f0\"\u5728\u5b83\u65e0\u6cd5\u9003\u8131\u7684\u4f4d\u7f6e\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_12","title":"\u5bfc\u822a","text":"<p>\u66f4\u597d\u7684\u5bfc\u822a\u7b56\u7565\u662f\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684\uff0c\u5b83\u7ed3\u5408\u4e86\u5229\u7528\u548c\u63a2\u7d22\u3002\u5728\u8fd9\u4e2a\u7b56\u7565\u4e2d\uff0c\u6211\u4eec\u5c06\u4ee5\u4e00\u5b9a\u7684\u6982\u7387\u9009\u62e9\u6bcf\u4e2a\u52a8\u4f5c\uff0c\u4e0e Q-Table \u4e2d\u7684\u503c\u6210\u6bd4\u4f8b\u3002\u8fd9\u79cd\u7b56\u7565\u53ef\u80fd\u4ecd\u4f1a\u5bfc\u81f4\u4ee3\u7406\u8fd4\u56de\u5230\u5b83\u5df2\u7ecf\u63a2\u7d22\u8fc7\u7684\u4f4d\u7f6e\uff0c\u4f46\u662f\uff0c\u6b63\u5982\u4f60\u4ece\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d\u770b\u5230\u7684\uff0c\u5b83\u4f1a\u5bfc\u81f4\u5230\u8fbe\u6240\u9700\u4f4d\u7f6e\u7684\u5e73\u5747\u8def\u5f84\u975e\u5e38\u77ed\uff08\u8bf7\u8bb0\u4f4f\uff0c<code>print_statistics</code> \u8fd0\u884c\u6a21\u62df100\u6b21\uff09\uff1a\uff08\u4ee3\u7801\u575710\uff09</p> <pre><code>def qpolicy(m):\n    x,y = m.human\n    v = probs(Q[x,y])\n    a = random.choices(list(actions),weights=v)[0]\n    return a\n\nprint_statistics(qpolicy)\n</code></pre> <p>\u8fd0\u884c\u6b64\u4ee3\u7801\u540e\uff0c\u4f60\u5e94\u8be5\u83b7\u5f97\u6bd4\u4ee5\u524d\u5c0f\u5f97\u591a\u7684\u5e73\u5747\u8def\u5f84\u957f\u5ea6\uff0c\u8303\u56f4\u4e3a 3-6\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_13","title":"\u8c03\u67e5\u5b66\u4e60\u8fc7\u7a0b","text":"<p>\u6b63\u5982\u6211\u4eec\u5df2\u7ecf\u63d0\u5230\u7684\uff0c\u5b66\u4e60\u8fc7\u7a0b\u662f\u63a2\u7d22\u548c\u63a2\u7d22\u6709\u5173\u95ee\u9898\u7a7a\u95f4\u7ed3\u6784\u7684\u77e5\u8bc6\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u5b66\u4e60\u7684\u7ed3\u679c\uff08\u5e2e\u52a9\u4ee3\u7406\u627e\u5230\u5230\u8fbe\u76ee\u6807\u7684\u77ed\u8def\u5f84\u7684\u80fd\u529b\uff09\u6709\u6240\u6539\u5584\uff0c\u4f46\u89c2\u5bdf\u5e73\u5747\u8def\u5f84\u957f\u5ea6\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\u4e5f\u5f88\u6709\u8da3\uff1a</p> <p></p> <p>\u5b66\u4e60\u5185\u5bb9\u53ef\u4ee5\u6982\u62ec\u4e3a\uff1a</p> <ul> <li> <p>\u5e73\u5747\u8def\u5f84\u957f\u5ea6\u589e\u52a0\u3002\u6211\u4eec\u5728\u8fd9\u91cc\u770b\u5230\u7684\u662f\uff0c\u8d77\u521d\uff0c\u5e73\u5747\u8def\u5f84\u957f\u5ea6\u589e\u52a0\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u5f53\u6211\u4eec\u5bf9\u73af\u5883\u4e00\u65e0\u6240\u77e5\u65f6\uff0c\u6211\u4eec\u5f88\u53ef\u80fd\u4f1a\u9677\u5165\u7cdf\u7cd5\u7684\u72b6\u6001\uff0c\u6c34\u6216\u72fc\u3002\u968f\u7740\u6211\u4eec\u5b66\u4e60\u66f4\u591a\u5e76\u5f00\u59cb\u4f7f\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u957f\u65f6\u95f4\u5730\u63a2\u7d22\u73af\u5883\uff0c\u4f46\u6211\u4eec\u4ecd\u7136\u4e0d\u77e5\u9053\u82f9\u679c\u5728\u54ea\u91cc\u3002</p> </li> <li> <p>\u968f\u7740\u6211\u4eec\u4e86\u89e3\u66f4\u591a\uff0c\u8def\u5f84\u957f\u5ea6\u51cf\u5c11\u3002\u4e00\u65e6\u6211\u4eec\u5b66\u4e60\u5f97\u8db3\u591f\u591a\uff0c\u4ee3\u7406\u5c31\u66f4\u5bb9\u6613\u5b9e\u73b0\u76ee\u6807\uff0c\u8def\u5f84\u957f\u5ea6\u5f00\u59cb\u51cf\u5c11\u3002\u7136\u800c\uff0c\u6211\u4eec\u4ecd\u7136\u5bf9\u63a2\u7d22\u6301\u5f00\u653e\u6001\u5ea6\uff0c\u56e0\u6b64\u6211\u4eec\u7ecf\u5e38\u504f\u79bb\u6700\u4f73\u8def\u5f84\uff0c\u5e76\u63a2\u7d22\u65b0\u7684\u9009\u62e9\uff0c\u4f7f\u8def\u5f84\u6bd4\u6700\u4f73\u8def\u5f84\u66f4\u957f\u3002</p> </li> <li> <p>\u957f\u5ea6\u7a81\u7136\u589e\u52a0\u3002\u6211\u4eec\u5728\u8fd9\u5f20\u56fe\u4e0a\u8fd8\u89c2\u5bdf\u5230\uff0c\u5728\u67d0\u4e2a\u65f6\u523b\uff0c\u957f\u5ea6\u7a81\u7136\u589e\u52a0\u3002\u8fd9\u8868\u660e\u8be5\u8fc7\u7a0b\u7684\u968f\u673a\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u67d0\u4e2a\u65f6\u5019\u901a\u8fc7\u7528\u65b0\u503c\u8986\u76d6 Q-Table \u7cfb\u6570\u6765\"\u7834\u574f\" Q-Table \u7cfb\u6570\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u8fd9\u5e94\u8be5\u901a\u8fc7\u964d\u4f4e\u5b66\u4e60\u7387\u6765\u6700\u5c0f\u5316\uff08\u4f8b\u5982\uff0c\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\uff0c\u6211\u4eec\u53ea\u8c03\u6574 Q-Table \u5f88\u5c0f\u7684\u4e00\u4e2a\u5c0f\u503c\uff09\u3002</p> </li> </ul> <p>\u603b\u7684\u6765\u8bf4\uff0c\u91cd\u8981\u7684\u662f\u8981\u8bb0\u4f4f\u5b66\u4e60\u8fc7\u7a0b\u7684\u6210\u529f\u548c\u8d28\u91cf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u53c2\u6570\uff0c\u4f8b\u5982\u5b66\u4e60\u7387\u3001\u5b66\u4e60\u7387\u8870\u51cf\u548c\u6298\u6263\u56e0\u5b50\u3002\u8fd9\u4e9b\u901a\u5e38\u79f0\u4e3a\u8d85\u53c2\u6570\uff0c\u4ee5\u533a\u522b\u4e8e\u6211\u4eec\u5728\u8bad\u7ec3\u671f\u95f4\u4f18\u5316\u7684\u53c2\u6570\uff08\u4f8b\u5982\uff0cQ-Table \u7cfb\u6570\uff09\u3002\u5bfb\u627e\u6700\u4f73\u8d85\u53c2\u6570\u503c\u7684\u8fc7\u7a0b\u79f0\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5b83\u503c\u5f97\u4e00\u4e2a\u5355\u72ec\u7684\u8bdd\u9898\u6765\u4ecb\u7ecd\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_14","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"8-Reinforcement/1-QLearning/README.zh-cn/#_15","title":"\u4f5c\u4e1a\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684\u4e16\u754c","text":""},{"location":"8-Reinforcement/1-QLearning/assignment/","title":"A More Realistic World","text":"<p>In our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:</p> <ol> <li>By moving from one place to another, Peter loses energy and gains some fatigue.</li> <li>Peter can gain more energy by eating apples.</li> <li>Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)</li> <li>Peter needs to find and kill the wolf</li> <li>In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.</li> </ol>"},{"location":"8-Reinforcement/1-QLearning/assignment/#instructions","title":"Instructions","text":"<p>Use the original notebook.ipynb notebook as a starting point for your solution.</p> <p>Modify the reward function above according to the rules of the game, run the reinforcement learning algorithm to learn the best strategy for winning the game, and compare the results of random walk with your algorithm in terms of number of games won and lost.</p> <p>Note: In your new world, the state is more complex, and in addition to human position also includes fatigue and energy levels. You may chose to represent the state as a tuple (Board,energy,fatigue), or define a class for the state (you may also want to derive it from <code>Board</code>), or even modify the original <code>Board</code> class inside rlboard.py.</p> <p>In your solution, please keep the code responsible for random walk strategy, and compare the results of your algorithm with random walk at the end.</p> <p>Note: You may need to adjust hyperparameters to make it work, especially the number of epochs. Because the success of the game (fighting the wolf) is a rare event, you can expect much longer training time.</p>"},{"location":"8-Reinforcement/1-QLearning/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A notebook is presented with the definition of new world rules, Q-Learning algorithm and some textual explanations. Q-Learning is able to significantly improve the results comparing to random walk. Notebook is presented, Q-Learning is implemented and improves results comparing to random walk, but not significantly; or notebook is poorly documented and code is not well-structured Some attempt to re-define the rules of the world are made, but Q-Learning algorithm does not work, or reward function is not fully defined"},{"location":"8-Reinforcement/1-QLearning/assignment.zh-cn/","title":"\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684\u4e16\u754c","text":"<p>\u6211\u4eec\u5047\u60f3\uff0c\u5f7c\u5f97\u51e0\u4e4e\u53ef\u4ee5\u4e00\u76f4\u8d70\u52a8\u800c\u4e0d\u4f1a\u611f\u5230\u75b2\u5026\u6216\u9965\u997f\u3002\u4f46\u5728\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684\u4e16\u754c\u91cc\uff0c\u6211\u4eec\u9700\u8981\u65f6\u4e0d\u65f6\u5730\u5750\u4e0b\u6765\u4f11\u606f\uff0c\u4e5f\u8981\u5403\u4e1c\u897f\u3002\u8ba9\u6211\u4eec\u52a0\u5165\u4ee5\u4e0b\u89c4\u5219\u8ba9\u6211\u4eec\u7684\u4e16\u754c\u66f4\u771f\u5b9e\uff1a</p> <ol> <li>\u4ece\u4e00\u4e2a\u5730\u65b9\u8d70\u5230\u53e6\u4e00\u4e2a\u5730\u65b9\uff0c\u5f7c\u5f97\u5931\u53bb\u4e86\u80fd\u91cf\u5e76\u83b7\u5f97\u4e86\u4e00\u4e9b\u75b2\u60eb\u3002</li> <li>\u5f7c\u5f97\u53ef\u4ee5\u901a\u8fc7\u5403\u82f9\u679c\u6765\u83b7\u5f97\u66f4\u591a\u7684\u80fd\u91cf\u3002</li> <li>\u5f7c\u5f97\u53ef\u4ee5\u901a\u8fc7\u5728\u6811\u4e0b\u6216\u8349\u5730\u4e0a\u4f11\u606f\u6765\u6d88\u9664\u75b2\u60eb\uff08\u5373\u8d70\u8fdb\u6709\u6811\u548c\u8349\u7684\u68cb\u76d8\u4f4d\u7f6e\u2014\u2014\u7eff\u8272\u7684\u683c\u5b50\uff09</li> <li>\u5f7c\u5f97\u9700\u8981\u627e\u5230\u5e76\u6740\u6b7b\u72fc</li> <li>\u4e3a\u4e86\u6740\u6b7b\u72fc\uff0c\u5f7c\u5f97\u9700\u8981\u6709\u4e00\u5b9a\u7ea7\u522b\u7684\u80fd\u91cf\u548c\u75b2\u60eb\uff0c\u5426\u5219\u4ed6\u4f1a\u8f93\u6389\u8fd9\u573a\u6218\u6597\u3002</li> </ol>"},{"location":"8-Reinforcement/1-QLearning/assignment.zh-cn/#_2","title":"\u8bf4\u660e","text":"<p>\u4f7f\u7528\u539f\u59cb notebook.ipynb \u7b14\u8bb0\u672c\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u7684\u8d77\u70b9\u3002</p> <p>\u6839\u636e\u6e38\u620f\u89c4\u5219\u4fee\u6539\u4e0a\u9762\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8fd0\u884c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u5b66\u4e60\u8d62\u5f97\u6e38\u620f\u7684\u6700\u4f73\u7b56\u7565\uff0c\u5e76\u5728\u6e38\u620f\u8d62/\u8f93\u7684\u6570\u91cf\u4e0a\u5c06\u4f60\u7684\u7b97\u6cd5\u548c\u968f\u673a\u8d70\u52a8\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002</p> <p>\u6ce8\u610f\uff1a\u5728\u4f60\u7684\u65b0\u4e16\u754c\u4e2d\uff0c\u72b6\u6001\u66f4\u52a0\u590d\u6742\uff0c\u9664\u4e86\u4eba\u4f53\u4f4d\u7f6e\u8fd8\u5305\u62ec\u75b2\u60eb\u548c\u80fd\u91cf\u6c34\u5e73\u3002\u4f60\u53ef\u4ee5\u9009\u62e9\u5c06\u72b6\u6001\u8868\u793a\u4e3a\u4e00\u4e2a\u5143\u7ec4\uff08Board\u3001energy\u3001fatigue\uff09\uff0c\u6216\u8005\u4e3a\u72b6\u6001\u5b9a\u4e49\u4e00\u4e2a\u7c7b\uff08\u4f60\u53ef\u80fd\u8fd8\u60f3\u4ece <code>Board</code> \u6d3e\u751f\u5b83\uff09\uff0c\u751a\u81f3\u5728 rlboard.py\u4e2d\u4fee\u6539<code>Board</code>\u7684\u6e90\u7801\u3002</p> <p>\u5728\u4f60\u7684\u89e3\u51b3\u65b9\u6848\u4e2d\uff0c\u8bf7\u4fdd\u7559\u8d1f\u8d23\u968f\u673a\u8d70\u52a8\u7b56\u7565\u7684\u4ee3\u7801\uff0c\u5e76\u5728\u6700\u540e\u5c06\u4f60\u7684\u7b97\u6cd5\u4e0e\u968f\u673a\u8d70\u52a8\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002</p> <p>\u6ce8\u610f\uff1a\u4f60\u53ef\u80fd\u9700\u8981\u8c03\u6574\u8d85\u53c2\u6570\u624d\u80fd\u4f7f\u5176\u5de5\u4f5c\uff0c\u5c24\u5176\u662f epoch \u6570\u3002\u56e0\u4e3a\u6e38\u620f\u7684\u6210\u529f\uff08\u4e0e\u72fc\u640f\u6597\uff09\u662f\u4e00\u4e2a\u7f55\u89c1\u7684\u4e8b\u4ef6\uff0c\u4f60\u9700\u8981\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002</p>"},{"location":"8-Reinforcement/1-QLearning/assignment.zh-cn/#_3","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b \u7b14\u8bb0\u672c\u4e0a\u6709\u65b0\u4e16\u754c\u89c4\u5219\u7684\u5b9a\u4e49\u3001Q-Learning \u7b97\u6cd5\u548c\u4e00\u4e9b\u6587\u5b57\u89e3\u91ca\u3002\u4e0e\u968f\u673a\u6e38\u8d70\u76f8\u6bd4\uff0cQ-Learning \u80fd\u591f\u663e\u8457\u6539\u5584\u7ed3\u679c \u4ecb\u7ecd\u4e86 Notebook\uff0c\u5b9e\u73b0\u4e86 Q-Learning \u5e76\u4e0e\u968f\u673a\u8d70\u52a8\u7b97\u6cd5\u76f8\u6bd4\u63d0\u9ad8\u4e86\u7ed3\u679c\uff0c\u4f46\u4e0d\u663e\u8457\uff1b\u6216\u8005 notebook \u7684\u6587\u6863\u4e0d\u5b8c\u5584\uff0c\u4ee3\u7801\u7ed3\u6784\u4e0d\u5408\u7406 \u4e00\u4e9b\u8bd5\u56fe\u91cd\u65b0\u5b9a\u4e49\u4e16\u754c\u89c4\u5219\u7684\u5c1d\u8bd5\uff0c\u4f46 Q-Learning \u7b97\u6cd5\u4e0d\u8d77\u4f5c\u7528\uff0c\u6216\u8005\u5956\u52b1\u51fd\u6570\u6ca1\u6709\u5b8c\u5168\u5b9a\u4e49"},{"location":"8-Reinforcement/1-QLearning/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"8-Reinforcement/1-QLearning/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"8-Reinforcement/2-Gym/","title":"CartPole Skating","text":"<p>The problem we have been solving in the previous lesson might seem like a toy problem, not really applicable for real life scenarios. This is not the case, because many real world problems also share this scenario - including playing Chess or Go. They are similar, because we also have a board with given rules and a discrete state.</p>"},{"location":"8-Reinforcement/2-Gym/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"8-Reinforcement/2-Gym/#introduction","title":"Introduction","text":"<p>In this lesson we will apply the same principles of Q-Learning to a problem with continuous state, i.e. a state that is given by one or more real numbers. We will deal with the following problem:</p> <p>Problem: If Peter wants to escape from the wolf, he needs to be able to move faster. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.</p> <p></p> <p>Peter and his friends get creative to escape the wolf! Image by Jen Looper</p> <p>We will use a simplified version of balancing known as a CartPole problem. In the cartpole world, we have a horizontal slider that can move left or right, and the goal is to balance a vertical pole on top of the slider.</p> <p></p>"},{"location":"8-Reinforcement/2-Gym/#prerequisites","title":"Prerequisites","text":"<p>In this lesson, we will be using a library called OpenAI Gym to simulate different environments. You can run this lesson's code locally (eg. from Visual Studio Code), in which case the simulation will open in a new window. When running the code online, you may need to make some tweaks to the code, as described here.</p>"},{"location":"8-Reinforcement/2-Gym/#openai-gym","title":"OpenAI Gym","text":"<p>In the previous lesson, the rules of the game and the state were given by the <code>Board</code> class which we defined ourselves. Here we will use a special simulation environment, which will simulate the physics behind the balancing pole. One of the most popular simulation environments for training reinforcement learning algorithms is called a Gym, which is maintained by OpenAI. By using this gym we can create difference environments from a cartpole simulation to Atari games.</p> <p>Note: You can see other environments available from OpenAI Gym here. </p> <p>First, let's install the gym and import required libraries (code block 1):</p> <pre><code>import sys\n!{sys.executable} -m pip install gym \n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n</code></pre>"},{"location":"8-Reinforcement/2-Gym/#exercise-initialize-a-cartpole-environment","title":"Exercise - initialize a cartpole environment","text":"<p>To work with a cartpole balancing problem, we need to initialize corresponding environment. Each environment is associated with an:</p> <ul> <li> <p>Observation space that defines the structure of information that we receive from the environment. For cartpole problem, we receive position of the pole, velocity and some other values.</p> </li> <li> <p>Action space that defines possible actions. In our case the action space is discrete, and consists of two actions - left and right. (code block 2)</p> </li> <li> <p>To initialize, type the following code:</p> <pre><code>env = gym.make(\"CartPole-v1\")\nprint(env.action_space)\nprint(env.observation_space)\nprint(env.action_space.sample())\n</code></pre> </li> </ul> <p>To see how the environment works, let's run a short simulation for 100 steps. At each step, we provide one of the actions to be taken - in this simulation we just randomly select an action from <code>action_space</code>. </p> <ol> <li> <p>Run the code below and see what it leads to.</p> <p>\u2705 Remember that it is preferred to run this code on local Python installation! (code block 3)</p> <pre><code>env.reset()\n\nfor i in range(100):\n   env.render()\n   env.step(env.action_space.sample())\nenv.close()\n</code></pre> <p>You should be seeing something similar to this image:</p> <p></p> </li> <li> <p>During simulation, we need to get observations in order to decide how to act. In fact, the step function returns current observations, a reward function, and the done flag that indicates whether it makes sense to continue the simulation or not: (code block 4)</p> <pre><code>env.reset()\n\ndone = False\nwhile not done:\n   env.render()\n   obs, rew, done, info = env.step(env.action_space.sample())\n   print(f\"{obs} -&gt; {rew}\")\nenv.close()\n</code></pre> <p>You will end up seeing something like this in the notebook output:</p> <pre><code>[ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -&gt; 1.0\n[ 0.02917248 -0.04828055  0.03248977  0.00543839] -&gt; 1.0\n[ 0.02820687  0.14636075  0.03259854 -0.27681916] -&gt; 1.0\n[ 0.03113408  0.34100283  0.02706215 -0.55904489] -&gt; 1.0\n[ 0.03795414  0.53573468  0.01588125 -0.84308041] -&gt; 1.0\n...\n[ 0.17299878  0.15868546 -0.20754175 -0.55975453] -&gt; 1.0\n[ 0.17617249  0.35602306 -0.21873684 -0.90998894] -&gt; 1.0\n</code></pre> <p>The observation vector that is returned at each step of the simulation contains the following values: - Position of cart - Velocity of cart - Angle of pole - Rotation rate of pole</p> </li> <li> <p>Get min and max value of those numbers: (code block 5)</p> <pre><code>print(env.observation_space.low)\nprint(env.observation_space.high)\n</code></pre> <p>You may also notice that reward value on each simulation step is always 1. This is because our goal is to survive as long as possible, i.e. keep the pole to a reasonably vertical position for the longest period of time.</p> <p>\u2705 In fact, the CartPole simulation is considered solved if we manage to get the average reward of 195 over 100 consecutive trials.</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/#state-discretization","title":"State discretization","text":"<p>In Q-Learning, we need to build Q-Table that defines what to do at each state. To be able to do this, we need state to be discreet, more precisely, it should contain finite number of discrete values. Thus, we need somehow to discretize our observations, mapping them to  a finite set of states.</p> <p>There are a few ways we can do this:</p> <ul> <li>Divide into bins. If we know the interval of a certain value, we can divide this interval into a number of bins, and then replace the value by the bin number that it belongs to. This can be done using the numpy <code>digitize</code> method. In this case, we will precisely know the state size, because it will depend on the number of bins we select for digitalization.</li> </ul> <p>\u2705 We can use linear interpolation to bring values to some finite interval (say, from -20 to 20), and then convert numbers to integers by rounding them. This gives us a bit less control on the size of the state, especially if we do not know the exact ranges of input values. For example, in our case 2 out of 4 values do not have upper/lower bounds on their values, which may result in the infinite number of states.</p> <p>In our example, we will go with the second approach. As you may notice later, despite undefined upper/lower bounds, those value rarely take values outside of certain finite intervals, thus those states with extreme values will be very rare.</p> <ol> <li> <p>Here is the function that will take the observation from our model and produce a tuple of 4 integer values: (code block 6)</p> <pre><code>def discretize(x):\n    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))\n</code></pre> </li> <li> <p>Let's also explore another discretization method using bins: (code block 7)</p> <pre><code>def create_bins(i,num):\n    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n\nprint(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n\nints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\nnbins = [20,20,10,10] # number of bins for each parameter\nbins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n\ndef discretize_bins(x):\n    return tuple(np.digitize(x[i],bins[i]) for i in range(4))\n</code></pre> </li> <li> <p>Let's now run a short simulation and observe those discrete environment values. Feel free to try both <code>discretize</code> and <code>discretize_bins</code> and see if there is a difference.</p> <p>\u2705 discretize_bins returns the bin number, which is 0-based. Thus for values of input variable around 0 it returns the number from the middle of the interval (10). In discretize, we did not care about the range of output values, allowing them to be negative, thus the state values are not shifted, and 0 corresponds to 0. (code block 8)</p> <pre><code>env.reset()\n\ndone = False\nwhile not done:\n   #env.render()\n   obs, rew, done, info = env.step(env.action_space.sample())\n   #print(discretize_bins(obs))\n   print(discretize(obs))\nenv.close()\n</code></pre> <p>\u2705 Uncomment the line starting with env.render if you want to see how the environment executes. Otherwise you can execute it in the background, which is faster. We will use this \"invisible\" execution during our Q-Learning process.</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/#the-q-table-structure","title":"The Q-Table structure","text":"<p>In our previous lesson, the state was a simple pair of numbers from 0 to 8, and thus it was convenient to represent Q-Table by a numpy tensor with a shape of 8x8x2. If we use bins discretization, the size of our state vector is also known, so we can use the same approach and represent state by an array of shape 20x20x10x10x2 (here 2 is the dimension of action space, and first dimensions correspond to the number of bins we have selected to use for each of the parameters in observation space).</p> <p>However, sometimes precise dimensions of the observation space are not known. In case of the <code>discretize</code> function, we may never be sure that our state stays within certain limits, because some of the original values are not bound. Thus, we will use a slightly different approach and represent Q-Table by a dictionary. </p> <ol> <li> <p>Use the pair (state,action) as the dictionary key, and the value would correspond to Q-Table entry value. (code block 9)</p> <pre><code>Q = {}\nactions = (0,1)\n\ndef qvalues(state):\n    return [Q.get((state,a),0) for a in actions]\n</code></pre> <p>Here we also define a function <code>qvalues()</code>, which returns a list of Q-Table values for a given state that corresponds to all possible actions. If the entry is not present in the Q-Table, we will return 0 as the default.</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/#lets-start-q-learning","title":"Let's start Q-Learning","text":"<p>Now we are ready to teach Peter to balance!</p> <ol> <li> <p>First, let's set some hyperparameters: (code block 10)</p> <pre><code># hyperparameters\nalpha = 0.3\ngamma = 0.9\nepsilon = 0.90\n</code></pre> <p>Here, <code>alpha</code> is the learning rate that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased <code>alpha</code> to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting <code>alpha</code> values later.</p> <p><code>gamma</code> is the discount factor that shows to which extent we should prioritize future reward over current reward.</p> <p><code>epsilon</code> is the exploration/exploitation factor that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in <code>epsilon</code> percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. </p> <p>\u2705 In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those \"mistakes\"</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/#improve-the-algorithm","title":"Improve the algorithm","text":"<p>We can also make two improvements to our algorithm from the previous lesson:</p> <ul> <li> <p>Calculate average cumulative reward, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.</p> </li> <li> <p>Calculate maximum average cumulative result, <code>Qmax</code>, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.</p> </li> <li> <p>Collect all cumulative rewards at each simulation at <code>rewards</code> vector for further plotting. (code block  11)</p> <pre><code>def probs(v,eps=1e-4):\n    v = v-v.min()+eps\n    v = v/v.sum()\n    return v\n\nQmax = 0\ncum_rewards = []\nrewards = []\nfor epoch in range(100000):\n    obs = env.reset()\n    done = False\n    cum_reward=0\n    # == do the simulation ==\n    while not done:\n        s = discretize(obs)\n        if random.random()&lt;epsilon:\n            # exploitation - chose the action according to Q-Table probabilities\n            v = probs(np.array(qvalues(s)))\n            a = random.choices(actions,weights=v)[0]\n        else:\n            # exploration - randomly chose the action\n            a = np.random.randint(env.action_space.n)\n\n        obs, rew, done, info = env.step(a)\n        cum_reward+=rew\n        ns = discretize(obs)\n        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n    cum_rewards.append(cum_reward)\n    rewards.append(cum_reward)\n    # == Periodically print results and calculate average reward ==\n    if epoch%5000==0:\n        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n        if np.average(cum_rewards) &gt; Qmax:\n            Qmax = np.average(cum_rewards)\n            Qbest = Q\n        cum_rewards=[]\n</code></pre> </li> </ul> <p>What you may notice from those results:</p> <ul> <li> <p>Close to our goal. We are very close to achieving the goal of getting 195 cumulative rewards over 100+ consecutive runs of the simulation, or we may have actually achieved it! Even if we get smaller numbers, we still do not know, because we average over 5000 runs, and only 100 runs is required in the formal criteria.</p> </li> <li> <p>Reward starts to drop. Sometimes the reward start to drop, which means that we can \"destroy\" already learnt values in the Q-Table with the ones that make the situation worse.</p> </li> </ul> <p>This observation is more clearly visible if we plot training progress.</p>"},{"location":"8-Reinforcement/2-Gym/#plotting-training-progress","title":"Plotting Training Progress","text":"<p>During training, we have collected the cumulative reward value at each of the iterations into <code>rewards</code> vector. Here is how it looks when we plot it against the iteration number:</p> <pre><code>plt.plot(rewards)\n</code></pre> <p></p> <p>From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate the running average over a series of experiments, let's say 100. This can be done conveniently using <code>np.convolve</code>: (code block 12)</p> <pre><code>def running_average(x,window):\n    return np.convolve(x,np.ones(window)/window,mode='valid')\n\nplt.plot(running_average(rewards,100))\n</code></pre> <p></p>"},{"location":"8-Reinforcement/2-Gym/#varying-hyperparameters","title":"Varying hyperparameters","text":"<p>To make learning more stable, it makes sense to adjust some of our hyperparameters during training. In particular:</p> <ul> <li> <p>For learning rate, <code>alpha</code>, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.</p> </li> <li> <p>Increase epsilon. We may want to increase the <code>epsilon</code> slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of <code>epsilon</code>, and move up to almost 1.</p> </li> </ul> <p>Task 1: Play with hyperparameter values and see if you can achieve higher cumulative reward. Are you getting above 195?</p> <p>Task 2: To formally solve the problem, you need to get 195 average reward across 100 consecutive runs. Measure that during training and make sure that you have formally solved the problem!</p>"},{"location":"8-Reinforcement/2-Gym/#seeing-the-result-in-action","title":"Seeing the result in action","text":"<p>It would be interesting to actually see how the trained model behaves. Let's run the simulation and follow the same action selection strategy as during training, sampling according to the probability distribution in Q-Table: (code block 13)</p> <pre><code>obs = env.reset()\ndone = False\nwhile not done:\n   s = discretize(obs)\n   env.render()\n   v = probs(np.array(qvalues(s)))\n   a = random.choices(actions,weights=v)[0]\n   obs,_,done,_ = env.step(a)\nenv.close()\n</code></pre> <p>You should see something like this:</p> <p></p>"},{"location":"8-Reinforcement/2-Gym/#challenge","title":"\ud83d\ude80Challenge","text":"<p>Task 3: Here, we were using the final copy of Q-Table, which may not be the best one. Remember that we have stored the best-performing Q-Table into <code>Qbest</code> variable! Try the same example with the best-performing Q-Table by copying <code>Qbest</code> over to <code>Q</code> and see if you notice the difference.</p> <p>Task 4: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using <code>np.argmax</code> function to find out the action number corresponding to highers Q-Table value. Implement this strategy and see if it improves the balancing.</p>"},{"location":"8-Reinforcement/2-Gym/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"8-Reinforcement/2-Gym/#assignment","title":"Assignment","text":"<p>Train a Mountain Car</p>"},{"location":"8-Reinforcement/2-Gym/#conclusion","title":"Conclusion","text":"<p>We have now learned how to train agents to achieve good results just by providing them a reward function that defines the desired state of the game, and by giving them an opportunity to intelligently explore the search space. We have successfully applied the Q-Learning algorithm in the cases of discrete and continuous environments, but with discrete actions.</p> <p>It's important to also study situations where action state is also continuous, and when observation space is much more complex, such as the image from the Atari game screen. In those problems we often need to use more powerful machine learning techniques, such as neural networks, in order to achieve good results. Those more advanced topics are the subject of our forthcoming more advanced AI course.</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/","title":"CartPole Skating","text":"<p>\u6211\u4eec\u5728\u4e0a\u4e00\u8bfe\u4e2d\u4e00\u76f4\u5728\u89e3\u51b3\u7684\u95ee\u9898\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u4e00\u4e2a\u73a9\u5177\u95ee\u9898\uff0c\u5e76\u4e0d\u771f\u6b63\u9002\u7528\u4e8e\u73b0\u5b9e\u751f\u6d3b\u573a\u666f\u3002\u4e8b\u5b9e\u5e76\u975e\u5982\u6b64\uff0c\u56e0\u4e3a\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\u4e5f\u6709\u8fd9\u79cd\u60c5\u51b5\u2014\u2014\u5305\u62ec\u4e0b\u56fd\u9645\u8c61\u68cb\u6216\u56f4\u68cb\u3002\u5b83\u4eec\u5f88\u76f8\u4f3c\uff0c\u56e0\u4e3a\u6211\u4eec\u4e5f\u6709\u4e00\u4e2a\u5177\u6709\u7ed9\u5b9a\u89c4\u5219\u548c\u79bb\u6563\u72b6\u6001\u7684\u677f\u3002 https://white-water-09ec41f0f.azurestaticapps.net/</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_1","title":"\u8bfe\u524d\u6d4b\u9a8c","text":""},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_2","title":"\u4ecb\u7ecd","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u628a Q-Learning \u7684\u76f8\u540c\u539f\u7406\u5e94\u7528\u5230\u5177\u6709\u8fde\u7eed\u72b6\u6001\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u5b9e\u6570\u7ed9\u51fa\u7684\u72b6\u6001\u3002\u6211\u4eec\u5c06\u5904\u7406\u4ee5\u4e0b\u95ee\u9898\uff1a</p> <p>\u95ee\u9898\uff1a\u5982\u679c\u5f7c\u5f97\u60f3\u8981\u9003\u79bb\u72fc\u7fa4\uff0c\u4ed6\u9700\u8981\u80fd\u591f\u79fb\u52a8\u5f97\u66f4\u5feb\u3002\u6211\u4eec\u5c06\u770b\u5230\u5f7c\u5f97\u5982\u4f55\u4f7f\u7528 Q-Learning \u5b66\u4e60\u6ed1\u51b0\uff0c\u7279\u522b\u662f\u4fdd\u6301\u5e73\u8861\u3002</p> <p></p> <p>\u5f7c\u5f97\u548c\u4ed6\u7684\u670b\u53cb\u4eec\u53d1\u6325\u521b\u610f\u6765\u9003\u79bb\u72fc\uff01\u56fe\u7247\u6765\u81ea Jen Looper</p> <p>\u6211\u4eec\u5c06\u4f7f\u7528\u79f0\u4e3a CartPole \u95ee\u9898\u7684\u7b80\u5316\u7248\u7248\u672c\u3002\u5728\u8fd9\u4e2a\u4e16\u754c\u4e2d\uff0c\u6211\u4eec\u6709\u4e00\u4e2a\u53ef\u4ee5\u5de6\u53f3\u79fb\u52a8\u7684\u6c34\u5e73\u6ed1\u5757\uff0c\u76ee\u6807\u662f\u5e73\u8861\u6ed1\u5757\u9876\u90e8\u7684\u5782\u76f4\u6746\u3002</p> <p></p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_3","title":"\u5148\u51b3\u6761\u4ef6","text":"<p>\u5728\u672c\u8bfe\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u540d\u4e3a OpenAI Gym \u7684\u5e93\u6765\u6a21\u62df\u4e0d\u540c\u7684 \u73af\u5883\u3002\u4f60\u53ef\u4ee5\u5728\u672c\u5730\uff08\u4f8b\u5982\u4ece Visual Studio Code\uff09\u8fd0\u884c\u672c\u8bfe\u7a0b\u7684\u4ee3\u7801\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6a21\u62df\u5c06\u5728\u65b0\u7a97\u53e3\u4e2d\u6253\u5f00\u3002\u5728\u7ebf\u8fd0\u884c\u4ee3\u7801\u65f6\uff0c\u4f60\u53ef\u80fd\u9700\u8981\u5bf9\u4ee3\u7801\u8fdb\u884c\u4e00\u4e9b\u8c03\u6574\uff0c\u5982 \u6b64\u5904\u3002</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#openai-gym","title":"OpenAI Gym","text":"<p>\u5728\u4e0a\u4e00\u8bfe\u4e2d\uff0c\u6e38\u620f\u89c4\u5219\u548c\u72b6\u6001\u662f\u7531\u6211\u4eec\u81ea\u5df1\u5b9a\u4e49\u7684\"Board\"\u7c7b\u7ed9\u51fa\u7684\u3002\u8fd9\u91cc\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u7279\u6b8a\u7684\u6a21\u62df\u73af\u5883\uff0c\u5b83\u5c06\u6a21\u62df\u5e73\u8861\u6746\u540e\u9762\u7684\u7269\u7406\u89c4\u5219\u3002\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6700\u6d41\u884c\u7684\u6a21\u62df\u73af\u5883\u4e4b\u4e00\u79f0\u4e3a Gym\uff0c\u7531 OpenAI \u7ef4\u62a4\u3002\u901a\u8fc7\u4f7f\u7528\u8fd9\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u6211\u4eec\u53ef\u4ee5\u521b\u5efa\u4e0d\u540c\u7684\u73af\u5883\uff0c\u4ece\u63a8\u8f66\u6a21\u62df\u5230 Atari \u6e38\u620f\u3002</p> <p>\u6ce8\u610f\uff1a\u4f60\u53ef\u4ee5\u5728 \u6b64\u5904 \u67e5\u770b OpenAI Gym \u63d0\u4f9b\u7684\u5176\u4ed6\u73af\u5883\u3002 </p> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5b89\u88c5 gym \u5e76\u5bfc\u5165\u6240\u9700\u7684\u5e93\uff08\u4ee3\u7801\u5757 1\uff09\uff1a</p> <pre><code>import sys\n!{sys.executable} -m pip install gym \n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n</code></pre>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#-","title":"\u7ec3\u4e60 - \u521d\u59cb\u5316\u4e00\u4e2a\u63a8\u8f66\u73af\u5883","text":"<p>\u4e3a\u4e86\u89e3\u51b3\u8f66\u6746\u5e73\u8861\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u521d\u59cb\u5316\u76f8\u5e94\u7684\u73af\u5883\u3002\u6bcf\u4e2a\u73af\u5883\u90fd\u6709\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u89c2\u5bdf\u7a7a\u95f4\uff0c\u5b9a\u4e49\u4e86\u6211\u4eec\u4ece\u73af\u5883\u4e2d\u63a5\u6536\u5230\u7684\u4fe1\u606f\u7ed3\u6784\u3002\u5bf9\u4e8e cartpole \u95ee\u9898\uff0c\u6211\u4eec\u63a5\u6536\u6746\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u5176\u4ed6\u4e00\u4e9b\u503c\u3002</p> </li> <li> <p>\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5b9a\u4e49\u53ef\u80fd\u7684\u52a8\u4f5c\u3002\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u52a8\u4f5c\u7a7a\u95f4\u662f\u79bb\u6563\u7684\uff0c\u7531\u4e24\u4e2a\u52a8\u4f5c\u7ec4\u6210 - left \u548c right\u3002\uff08\u4ee3\u7801\u5757 2\uff09</p> </li> <li> <p>\u8981\u521d\u59cb\u5316\uff0c\u8bf7\u952e\u5165\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code>env = gym.make(\"CartPole-v1\")\nprint(env.action_space)\nprint(env.observation_space)\nprint(env.action_space.sample())\n</code></pre> </li> </ul> <p>\u8981\u67e5\u770b\u73af\u5883\u5982\u4f55\u5de5\u4f5c\uff0c\u8ba9\u6211\u4eec\u8fd0\u884c 100 \u4e2a\u6b65\u9aa4\u7684\u7b80\u77ed\u6a21\u62df\u3002\u5728\u6bcf\u4e00\u6b65\uff0c\u6211\u4eec\u63d0\u4f9b\u4e00\u4e2a\u8981\u91c7\u53d6\u7684\u884c\u52a8\u2014\u2014\u5728\u8fd9\u4e2a\u6a21\u62df\u4e2d\uff0c\u6211\u4eec\u53ea\u662f\u4ece \"action_space\" \u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u884c\u52a8\u3002</p> <ol> <li> <p>\u8fd0\u884c\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u770b\u770b\u5b83\u4f1a\u5bfc\u81f4\u4ec0\u4e48\u3002</p> <p>\u2705 \u8bf7\u8bb0\u4f4f\uff0c\u6700\u597d\u5728\u672c\u5730 Python \u5b89\u88c5\u4e0a\u8fd0\u884c\u6b64\u4ee3\u7801\uff01\uff08\u4ee3\u7801\u5757 3\uff09</p> <pre><code>env.reset()\n\nfor i in range(100):\n    env.render()\n    env.step(env.action_space.sample())\nenv.close()\n</code></pre> <p>\u4f60\u5e94\u8be5\u4f1a\u770b\u5230\u4e0e\u6b64\u56fe\u50cf\u7c7b\u4f3c\u7684\u5185\u5bb9\uff1a</p> <p></p> </li> <li> <p>\u5728\u6a21\u62df\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u901a\u8fc7\u89c2\u5bdf\u6765\u51b3\u5b9a\u5982\u4f55\u884c\u52a8\u3002\u4e8b\u5b9e\u4e0a\uff0cstep \u51fd\u6570\u8fd4\u56de\u5f53\u524d\u89c2\u5bdf\u503c\u3001\u5956\u52b1\u51fd\u6570\u548c\u6307\u793a\u662f\u5426\u7ee7\u7eed\u6a21\u62df\u6709\u610f\u4e49\u7684\u5b8c\u6210\u6807\u5fd7\uff1a\uff08\u4ee3\u7801\u5757 4\uff09</p> <pre><code>env.reset()\n\ndone = False\nwhile not done:\n    env.render()\n    obs, rew, done, info = env.step(env.action_space.sample())\n    print(f\"{obs} -&gt; {rew}\")\nenv.close()\n</code></pre> <p>\u4f60\u6700\u7ec8\u4f1a\u5728\u7b14\u8bb0\u672c\u8f93\u51fa\u4e2d\u770b\u5230\u7c7b\u4f3c\u7684\u5185\u5bb9\uff1a</p> <pre><code>[ 0.03403272 -0.24301182 0.02669811 0.2895829 ] -&gt; 1.0\n[ 0.02917248 -0.04828055 0.03248977 0.00543839] -&gt; 1.0\n[ 0.02820687 0.14636075 0.03259854 -0.27681916] -&gt; 1.0\n[ 0.03113408 0.34100283 0.02706215 -0.55904489] -&gt; 1.0\n[ 0.03795414 0.53573468 0.01588125 -0.84308041] -&gt; 1.0\n...\n[ 0.17299878 0.15868546 -0.20754175 -0.55975453] -&gt; 1.0\n[ 0.17617249 0.35602306 -0.21873684 -0.90998894] -&gt; 1.0\n</code></pre> <p>\u5728\u6a21\u62df\u7684\u6bcf\u4e00\u6b65\u8fd4\u56de\u7684\u89c2\u5bdf\u5411\u91cf\u5305\u542b\u4ee5\u4e0b\u503c\uff1a - \u63a8\u8f66\u7684\u4f4d\u7f6e - \u63a8\u8f66\u901f\u5ea6 - \u6746\u7684\u89d2\u5ea6 - \u6746\u7684\u8f6c\u901f</p> </li> <li> <p>\u83b7\u53d6\u8fd9\u4e9b\u6570\u5b57\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\uff1a\uff08\u4ee3\u7801\u5757 5\uff09</p> <pre><code>print(env.observation_space.low)\nprint(env.observation_space.high)\n</code></pre> <p>\u4f60\u53ef\u80fd\u8fd8\u6ce8\u610f\u5230\uff0c\u6bcf\u4e2a\u6a21\u62df\u6b65\u9aa4\u7684\u5956\u52b1\u503c\u59cb\u7ec8\u4e3a 1\u3002\u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u7684\u76ee\u6807\u662f\u5c3d\u53ef\u80fd\u957f\u65f6\u95f4\u5730\u751f\u5b58\uff0c\u5373\u5728\u6700\u957f\u7684\u65f6\u95f4\u5185\u5c06\u6746\u4fdd\u6301\u5728\u5408\u7406\u7684\u5782\u76f4\u4f4d\u7f6e\u3002</p> <p>\u2705 \u4e8b\u5b9e\u4e0a\uff0c\u5982\u679c\u6211\u4eec\u8bbe\u6cd5\u5728 100 \u6b21\u8fde\u7eed\u8bd5\u9a8c\u4e2d\u83b7\u5f97 195 \u7684\u5e73\u5747\u5956\u52b1\uff0c\u5219\u8ba4\u4e3a CartPole \u95ee\u9898\u5df2\u89e3\u51b3\u3002</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_4","title":"\u72b6\u6001\u79bb\u6563\u5316","text":"<p>\u5728 Q-Learning \u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6784\u5efa Q-Table \u6765\u5b9a\u4e49\u5728\u6bcf\u4e2a\u72b6\u6001\u4e0b\u8981\u505a\u4ec0\u4e48\u3002\u4e3a\u4e86\u80fd\u591f\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u9700\u8981\u72b6\u6001 discreet\uff0c\u66f4\u51c6\u786e\u5730\u8bf4\uff0c\u5b83\u5e94\u8be5\u5305\u542b\u6709\u9650\u6570\u91cf\u7684\u79bb\u6563\u503c\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u79bb\u6563\u6211\u4eec\u7684\u89c2\u5bdf\uff0c\u5c06\u5b83\u4eec\u6620\u5c04\u5230\u4e00\u7ec4\u6709\u9650\u7684\u72b6\u6001\u3002</p> <p>\u6211\u4eec\u6709\u51e0\u79cd\u65b9\u6cd5\u53ef\u4ee5\u505a\u5230\u8fd9\u4e00\u70b9\uff1a</p> <ul> <li>\u62c6\u5206\u88c5\u7bb1\u3002\u5982\u679c\u6211\u4eec\u77e5\u9053\u67d0\u4e2a\u503c\u7684\u533a\u95f4\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e2a\u533a\u95f4\u5206\u6210\u82e5\u5e72\u4e2abins\uff0c\u7136\u540e\u7528\u5b83\u6240\u5c5e\u7684\u7bb1\u5b50\u5e8f\u53f7\u66ff\u6362\u8fd9\u4e2a\u503c\u3002\u8fd9\u53ef\u4ee5\u4f7f\u7528 <code>digitize</code> \u65b9\u6cd5\u6765\u5b8c\u6210\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5c06\u7cbe\u786e\u5730\u77e5\u9053\u72b6\u6001\u5927\u5c0f\uff0c\u56e0\u4e3a\u5b83\u53d6\u51b3\u4e8e\u6211\u4eec\u9009\u62e9\u7684\u7bb1\u5b50\u6570\u91cf\u3002</li> </ul> <p>\u2705 \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7ebf\u6027\u63d2\u503c\u5c06\u503c\u5e26\u5165\u67d0\u4e2a\u6709\u9650\u533a\u95f4\uff08\u4f8b\u5982\uff0c\u4ece -20 \u5230 20\uff09\uff0c\u7136\u540e\u901a\u8fc7\u56db\u820d\u4e94\u5165\u5c06\u6570\u5b57\u8f6c\u6362\u4e3a\u6574\u6570\u3002\u8fd9\u4f7f\u6211\u4eec\u5bf9\u72b6\u6001\u5927\u5c0f\u7684\u63a7\u5236\u51cf\u5f31\u4e86\u4e00\u70b9\uff0c\u5c24\u5176\u662f\u5f53\u6211\u4eec\u4e0d\u77e5\u9053\u8f93\u5165\u503c\u7684\u786e\u5207\u8303\u56f4\u65f6\u3002\u4f8b\u5982\uff0c\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c4 \u4e2a\u503c\u4e2d\u6709 2 \u4e2a\u503c\u7684\u503c\u6ca1\u6709\u4e0a\u9650/\u4e0b\u9650\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u65e0\u9650\u6570\u91cf\u7684\u72b6\u6001\u3002</p> <p>\u5728\u6211\u4eec\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u91c7\u7528\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u3002\u7a0d\u540e\u4f60\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\uff0c\u5c3d\u7ba1\u6709\u672a\u5b9a\u4e49\u7684\u4e0a\u9650/\u4e0b\u9650\uff0c\u4f46\u8fd9\u4e9b\u503c\u5f88\u5c11\u91c7\u7528\u67d0\u4e9b\u6709\u9650\u533a\u95f4\u4e4b\u5916\u7684\u503c\uff0c\u56e0\u6b64\u5177\u6709\u6781\u503c\u7684\u72b6\u6001\u5c06\u975e\u5e38\u7f55\u89c1\u3002</p> <ol> <li> <p>\u8fd9\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u5b83\u5c06\u4ece\u6211\u4eec\u7684\u6a21\u578b\u4e2d\u83b7\u53d6\u89c2\u5bdf\u7ed3\u679c\u5e76\u751f\u6210\u4e00\u4e2a\u5305\u542b 4 \u4e2a\u6574\u6570\u503c\u7684\u5143\u7ec4\uff1a\uff08\u4ee3\u7801\u5757 6\uff09</p> <pre><code>def discretize(x):\n    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))\n</code></pre> </li> <li> <p>\u8ba9\u6211\u4eec\u4e5f\u63a2\u7d22\u53e6\u4e00\u79cd\u4f7f\u7528 bins \u7684\u79bb\u6563\u5316\u65b9\u6cd5\uff1a\uff08\u4ee3\u7801\u5757 7\uff09</p> <pre><code>def create_bins(i,num):\n    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n\nprint(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n\nints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # \u6bcf\u4e2a\u53c2\u6570\u7684\u503c\u95f4\u9694\nnbins = [20,20,10,10] # \u6bcf\u4e2a\u53c2\u6570\u7684 bin \u6570\u91cf\nbins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n\ndef discretize_bins(x):\n    return tuple(np.digitize(x[i],bins[i]) for i in range(4))\n</code></pre> </li> <li> <p>\u73b0\u5728\u8ba9\u6211\u4eec\u8fd0\u884c\u4e00\u4e2a\u7b80\u77ed\u7684\u6a21\u62df\u5e76\u89c2\u5bdf\u90a3\u4e9b\u79bb\u6563\u7684\u73af\u5883\u503c\u3002\u968f\u610f\u5c1d\u8bd5 <code>discretize</code> \u548c <code>discretize_bins</code> \uff0c\u770b\u770b\u662f\u5426\u6709\u533a\u522b\u3002</p> <p>\u2705 discretize_bins \u8fd4\u56de bin \u7f16\u53f7\uff0c\u4ece 0 \u5f00\u59cb\u3002\u56e0\u6b64\uff0c\u5bf9\u4e8e0\u9644\u8fd1 \u7684\u8f93\u5165\u53d8\u91cf\u503c\uff0c\u5b83\u8fd4\u56de\u533a\u95f4 (10) \u4e2d\u95f4\u7684\u6570\u5b57\u3002\u5728\u79bb\u6563\u5316\u4e2d\uff0c\u6211\u4eec\u4e0d\u5173\u5fc3\u8f93\u51fa\u503c\u7684\u8303\u56f4\uff0c\u5141\u8bb8\u5b83\u4eec\u4e3a\u8d1f\uff0c\u56e0\u6b64\u72b6\u6001\u503c\u4e0d\u4f1a\u79fb\u4f4d\uff0c0 \u5bf9\u5e94\u4e8e 0\u3002\uff08\u4ee3\u7801\u5757 8\uff09</p> <pre><code>env.reset()\n\ndone = False\nwhile not done:\n    #env.render()\n    obs, rew, done, info = env.step(env.action_space.sample())\n    #print(discretize_bins(obs))\n    print(discretize(obs))\nenv.close()\n</code></pre> <p>\u2705 \u5982\u679c\u4f60\u60f3\u67e5\u770b\u73af\u5883\u5982\u4f55\u6267\u884c\uff0c\u8bf7\u53d6\u6d88\u4ee5 env.render \u5f00\u5934\u884c\u7684\u6ce8\u91ca\u3002\u6216\u8005\u4f60\u53ef\u4ee5\u5728\u540e\u53f0\u6267\u884c\u5b83\uff0c\u8fd9\u6837\u4f1a\u66f4\u5feb\u3002\u6211\u4eec\u5c06\u5728 Q-Learning \u8fc7\u7a0b\u4e2d\u4f7f\u7528\u8fd9\u79cd\"\u9690\u5f62\"\u6267\u884c\u3002</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#q-table","title":"Q-Table \u7ed3\u6784","text":"<p>\u5728\u6211\u4eec\u4e0a\u4e00\u8bfe\u4e2d\uff0c\u72b6\u6001\u662f\u4ece 0 \u5230 8 \u7684\u4e00\u5bf9\u7b80\u5355\u6570\u5b57\uff0c\u56e0\u6b64\u7528\u5f62\u72b6\u4e3a 8x8x2 \u7684 numpy \u5f20\u91cf\u6765\u8868\u793a Q-Table \u5f88\u65b9\u4fbf\u3002\u5982\u679c\u6211\u4eec\u4f7f\u7528 bins \u79bb\u6563\u5316\uff0c\u6211\u4eec\u7684\u72b6\u6001\u5411\u91cf\u7684\u5927\u5c0f\u4e5f\u662f\u5df2\u77e5\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u6cd5\uff0c\u7528\u4e00\u4e2a\u5f62\u72b6\u4e3a 20x20x10x10x2 \u7684\u6570\u7ec4\u6765\u8868\u793a\u72b6\u6001\uff08\u8fd9\u91cc 2 \u662f\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ef4\u5ea6\uff0c\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5bf9\u5e94\u4e8e\u6211\u4eec\u9009\u62e9\u7528\u4e8e\u89c2\u5bdf\u7a7a\u95f4\u4e2d\u7684\u6bcf\u4e2a\u53c2\u6570\u7684\u7bb1\uff09\u3002</p> <p>\u7136\u800c\uff0c\u6709\u65f6\u4e0d\u77e5\u9053\u89c2\u5bdf\u7a7a\u95f4\u7684\u7cbe\u786e\u5c3a\u5bf8\u3002\u5728 <code>discretize</code> \u51fd\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u80fd\u6c38\u8fdc\u65e0\u6cd5\u786e\u5b9a\u6211\u4eec\u7684\u72b6\u6001\u662f\u5426\u4fdd\u6301\u5728\u67d0\u4e9b\u9650\u5236\u5185\uff0c\u56e0\u4e3a\u4e00\u4e9b\u539f\u59cb\u503c\u4e0d\u53d7\u7ea6\u675f\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u7a0d\u5fae\u4e0d\u540c\u7684\u65b9\u6cd5\u5e76\u7528\u5b57\u5178\u8868\u793a Q-Table\u3002</p> <ol> <li> <p>\u4f7f\u7528 (state,action) \u5bf9\u4f5c\u4e3a\u5b57\u5178\u952e\uff0c\u8be5\u503c\u5c06\u5bf9\u5e94\u4e8e Q-Table \u6761\u76ee\u503c\u3002\uff08\u4ee3\u7801\u5757 9\uff09</p> <pre><code>Q = {}\nactions = (0,1)\n\ndef qvalues(state):\n    return [Q.get((state,a),0) for a in actions]\n</code></pre> <p>\u8fd9\u91cc\u6211\u4eec\u8fd8\u5b9a\u4e49\u4e86\u4e00\u4e2a\u51fd\u6570 <code>qvalues()</code>\uff0c\u5b83\u8fd4\u56de\u5bf9\u5e94\u4e8e\u6240\u6709\u53ef\u80fd\u64cd\u4f5c\u7684\u7ed9\u5b9a\u72b6\u6001\u7684 Q-Table \u503c\u5217\u8868\u3002\u5982\u679c\u8be5\u6761\u76ee\u4e0d\u5b58\u5728\u4e8e Q-Table \u4e2d\uff0c\u6211\u4eec\u5c06\u8fd4\u56de 0 \u4f5c\u4e3a\u9ed8\u8ba4\u503c\u3002</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#q-learning","title":"\u8ba9\u6211\u4eec\u5f00\u59cb Q-Learning","text":"<p>\u73b0\u5728\u6211\u4eec\u51c6\u5907\u6559\u5f7c\u5f97\u5e73\u8861\uff01</p> <ol> <li> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u8bbe\u7f6e\u4e00\u4e9b\u8d85\u53c2\u6570\uff1a\uff08\u4ee3\u7801\u5757 10\uff09</p> <pre><code># hyperparameters\nalpha = 0.3\ngamma = 0.9\nepsilon = 0.90\n</code></pre> <p>\u8fd9\u91cc\uff0c<code>alpha</code> \u662f\u5b66\u4e60\u7387\uff0c\u5b83\u5b9a\u4e49\u4e86\u6211\u4eec\u5e94\u8be5\u5728\u6bcf\u4e00\u6b65\u8c03\u6574 Q-Table \u7684\u5f53\u524d\u503c\u7684\u7a0b\u5ea6\u3002\u5728\u4e0a\u4e00\u8bfe\u4e2d\uff0c\u6211\u4eec\u4ece 1 \u5f00\u59cb\uff0c\u7136\u540e\u5728\u8bad\u7ec3\u671f\u95f4\u5c06 <code>alpha</code> \u51cf\u5c0f\u5230\u8f83\u4f4e\u7684\u503c\u3002\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u4fdd\u6301\u5b83\u4e0d\u53d8\uff0c\u4f60\u53ef\u4ee5\u7a0d\u540e\u5c1d\u8bd5\u8c03\u6574 <code>alpha</code> \u503c\u3002</p> <p><code>gamma</code> \u662f\u6298\u6263\u56e0\u7d20\uff0c\u5b83\u8868\u660e\u6211\u4eec\u5e94\u8be5\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4f18\u5148\u8003\u8651\u672a\u6765\u7684\u5956\u52b1\u800c\u4e0d\u662f\u5f53\u524d\u7684\u5956\u52b1\u3002</p> <p><code>epsilon</code> \u662f\u63a2\u7d22/\u5f00\u53d1\u56e0\u7d20\uff0c\u5b83\u51b3\u5b9a\u4e86\u6211\u4eec\u662f\u5426\u5e94\u8be5\u66f4\u559c\u6b22\u63a2\u7d22\u800c\u4e0d\u662f\u5f00\u53d1\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u5728\u6211\u4eec\u7684\u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u5c06\u5728 <code>epsilon</code> \u767e\u5206\u6bd4\u7684\u60c5\u51b5\u4e0b\u6839\u636e Q-Table \u503c\u9009\u62e9\u4e0b\u4e00\u4e2a\u52a8\u4f5c\uff0c\u5728\u5269\u4f59\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5c06\u6267\u884c\u968f\u673a\u52a8\u4f5c\u3002\u8fd9\u5c06\u4f7f\u6211\u4eec\u80fd\u591f\u63a2\u7d22\u6211\u4eec\u4ee5\u524d\u4ece\u672a\u89c1\u8fc7\u7684\u641c\u7d22\u7a7a\u95f4\u533a\u57df\u3002</p> <p>\u2705 \u5728\u5e73\u8861\u65b9\u9762 - \u9009\u62e9\u968f\u673a\u52a8\u4f5c\uff08\u63a2\u7d22\uff09\u5c06\u4f5c\u4e3a\u671d\u7740\u9519\u8bef\u65b9\u5411\u7684\u968f\u673a\u4e00\u62f3\uff0c\u6746\u5b50\u5fc5\u987b\u5b66\u4e60\u5982\u4f55\u4ece\u8fd9\u4e9b\"\u9519\u8bef\"\u4e2d\u6062\u590d\u5e73\u8861</p> </li> </ol>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_5","title":"\u6539\u8fdb\u7b97\u6cd5","text":"<p>\u6211\u4eec\u8fd8\u53ef\u4ee5\u5bf9\u4e0a\u4e00\u8bfe\u7684\u7b97\u6cd5\u8fdb\u884c\u4e24\u9879\u6539\u8fdb\uff1a</p> <ul> <li> <p>\u8ba1\u7b97\u5e73\u5747\u7d2f\u79ef\u5956\u52b1\uff0c\u7ecf\u8fc7\u591a\u6b21\u6a21\u62df\u3002\u6211\u4eec\u6bcf 5000 \u6b21\u8fed\u4ee3\u6253\u5370\u4e00\u6b21\u8fdb\u5ea6\uff0c\u5e76\u8ba1\u7b97\u8fd9\u6bb5\u65f6\u95f4\u5185\u7d2f\u79ef\u5956\u52b1\u7684\u5e73\u5747\u503c\u3002\u8fd9\u610f\u5473\u7740\u5982\u679c\u6211\u4eec\u5f97\u5230\u8d85\u8fc7 195 \u5206\u2014\u2014\u6211\u4eec\u53ef\u4ee5\u8ba4\u4e3a\u95ee\u9898\u5df2\u7ecf\u89e3\u51b3\uff0c\u751a\u81f3\u6bd4\u8981\u6c42\u7684\u8d28\u91cf\u66f4\u9ad8\u3002</p> </li> <li> <p>\u8ba1\u7b97\u6700\u5927\u5e73\u5747\u7d2f\u79ef\u7ed3\u679c\uff0c<code>Qmax</code>\uff0c\u6211\u4eec\u5c06\u5b58\u50a8\u4e0e\u8be5\u7ed3\u679c\u5bf9\u5e94\u7684 Q-Table\u3002\u5f53\u4f60\u8fd0\u884c\u8bad\u7ec3\u65f6\uff0c\u4f60\u4f1a\u6ce8\u610f\u5230\u6709\u65f6\u5e73\u5747\u7d2f\u79ef\u7ed3\u679c\u5f00\u59cb\u4e0b\u964d\uff0c\u6211\u4eec\u5e0c\u671b\u4fdd\u7559\u4e0e\u8bad\u7ec3\u671f\u95f4\u89c2\u5bdf\u5230\u7684\u6700\u4f73\u6a21\u578b\u76f8\u5bf9\u5e94\u7684 Q-Table \u503c\u3002</p> </li> <li> <p>\u5728 <code>rewards</code> \u5411\u91cf\u5904\u6536\u96c6\u6bcf\u6b21\u6a21\u62df\u7684\u6240\u6709\u7d2f\u79ef\u5956\u52b1\uff0c\u7528\u4e8e\u8fdb\u4e00\u6b65\u7ed8\u56fe\u3002\uff08\u4ee3\u7801\u5757 11\uff09</p> <pre><code>def probs(v,eps=1e-4):\n    v = vv.min()+eps\n    v = v/v.sum()\n    return v\n\nQmax = 0\ncum_rewards = []\nrewards = []\nfor epoch in range(100000):\n    obs = env.reset()\n    done = False\n    cum_reward=0\n    # == do the simulation ==\n    while not done:\n        s = discretize(obs)\n        if random.random()&lt;epsilon:\n            # exploitation - chose the action according to Q-Table probabilities\n            v = probs(np.array(qvalues(s)))\n            a = random.choices(actions,weights=v)[0]\n        else:\n            # exploration - randomly chose the action\n            a = np.random.randint(env.action_space.n)\n\n        obs, rew, done, info = env.step(a)\n        cum_reward+=rew\n        ns = discretize(obs)\n        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n    cum_rewards.append(cum_reward)\n    rewards.append(cum_reward)\n    # == Periodically print results and calculate average reward ==\n    if epoch%5000==0:\n        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n        if np.average(cum_rewards) &gt; Qmax:\n            Qmax = np.average(cum_rewards)\n            Qbest = Q\n        cum_rewards=[]\n</code></pre> </li> </ul> <p>\u4f60\u53ef\u80fd\u4f1a\u4ece\u8fd9\u4e9b\u7ed3\u679c\u4e2d\u6ce8\u610f\u5230\uff1a</p> <ul> <li> <p>\u63a5\u8fd1\u6211\u4eec\u7684\u76ee\u6807\u3002\u6211\u4eec\u975e\u5e38\u63a5\u8fd1\u5b9e\u73b0\u5728\u8fde\u7eed 100 \u591a\u6b21\u6a21\u62df\u8fd0\u884c\u4e2d\u83b7\u5f97 195 \u4e2a\u7d2f\u79ef\u5956\u52b1\u7684\u76ee\u6807\uff0c\u6216\u8005\u6211\u4eec\u53ef\u80fd\u771f\u7684\u5b9e\u73b0\u4e86\uff01\u5373\u4f7f\u6211\u4eec\u5f97\u5230\u66f4\u5c0f\u7684\u6570\u5b57\uff0c\u6211\u4eec\u4ecd\u7136\u4e0d\u77e5\u9053\uff0c\u56e0\u4e3a\u6211\u4eec\u5e73\u5747\u8d85\u8fc7 5000 \u6b21\u8fd0\u884c\uff0c\u800c\u5728\u6b63\u5f0f\u6807\u51c6\u4e2d\u53ea\u9700\u8981 100 \u6b21\u8fd0\u884c\u3002</p> </li> <li> <p>\u5956\u52b1\u5f00\u59cb\u4e0b\u964d\u3002\u6709\u65f6\u5956\u52b1\u5f00\u59cb\u4e0b\u964d\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u53ef\u4ee5\u201c\u7834\u574f\u201d Q-Table \u4e2d\u5df2\u7ecf\u5b66\u4e60\u5230\u7684\u503c\uff0c\u8fd9\u4e9b\u503c\u4f1a\u4f7f\u60c5\u51b5\u53d8\u5f97\u66f4\u7cdf\u3002</p> </li> </ul> <p>\u5982\u679c\u6211\u4eec\u7ed8\u5236\u8bad\u7ec3\u8fdb\u5ea6\u56fe\uff0c\u5219\u8fd9\u79cd\u89c2\u5bdf\u4f1a\u66f4\u52a0\u6e05\u6670\u53ef\u89c1\u3002</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_6","title":"\u7ed8\u5236\u8bad\u7ec3\u8fdb\u5ea6","text":"<p>\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u6211\u4eec\u5c06\u6bcf\u6b21\u8fed\u4ee3\u7684\u7d2f\u79ef\u5956\u52b1\u503c\u6536\u96c6\u5230 <code>rewards</code> \u5411\u91cf\u4e2d\u3002\u4ee5\u4e0b\u662f\u6211\u4eec\u6839\u636e\u8fed\u4ee3\u6b21\u6570\u7ed8\u5236\u5b83\u65f6\u7684\u6837\u5b50\uff1a</p> <pre><code>plt.plot(reawrd)\n</code></pre> <p></p> <p>\u4ece\u8fd9\u5f20\u56fe\u4e2d\uff0c\u65e0\u6cd5\u8bf4\u660e\u4efb\u4f55\u4e8b\u60c5\uff0c\u56e0\u4e3a\u7531\u4e8e\u968f\u673a\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6027\u8d28\uff0c\u8bad\u7ec3\u8bfe\u7a0b\u7684\u957f\u5ea6\u5dee\u5f02\u5f88\u5927\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e2a\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u4e00\u7cfb\u5217\u5b9e\u9a8c\u7684 running average\uff0c\u5047\u8bbe\u4e3a 100\u3002\u8fd9\u53ef\u4ee5\u4f7f\u7528 <code>np.convolve</code> \u65b9\u4fbf\u5730\u5b8c\u6210\uff1a\uff08\u4ee3\u7801\u5757 12\uff09</p> <pre><code>def running_average(x,window):\n    return np.convolve(x,np.ones(window)/window,mode='valid')\n\nplt.plot(running_average(rewards,100))\n</code></pre> <p></p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_7","title":"\u4e0d\u540c\u7684\u8d85\u53c2\u6570","text":"<p>\u4e3a\u4e86\u8ba9\u5b66\u4e60\u66f4\u7a33\u5b9a\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u8c03\u6574\u6211\u4eec\u7684\u4e00\u4e9b\u8d85\u53c2\u6570\u662f\u6709\u610f\u4e49\u7684\u3002\u7279\u522b\u662f\uff1a</p> <ul> <li> <p>\u5bf9\u4e8e\u5b66\u4e60\u7387\uff0c<code>alpha</code>\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece\u63a5\u8fd1 1 \u7684\u503c\u5f00\u59cb\uff0c\u7136\u540e\u4e0d\u65ad\u51cf\u5c0f\u53c2\u6570\u3002\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u6211\u4eec\u5c06\u5728 Q-Table \u4e2d\u83b7\u5f97\u826f\u597d\u7684\u6982\u7387\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u5e94\u8be5\u7a0d\u5fae\u8c03\u6574\u5b83\u4eec\uff0c\u800c\u4e0d\u662f\u7528\u65b0\u503c\u5b8c\u5168\u8986\u76d6\u3002</p> </li> <li> <p>\u589e\u52a0 epsilon\u3002\u6211\u4eec\u53ef\u80fd\u5e0c\u671b\u7f13\u6162\u589e\u52a0 <code>epsilon</code>\uff0c\u4ee5\u4fbf\u63a2\u7d22\u66f4\u5c11\uff0c\u5f00\u53d1\u66f4\u591a\u3002\u4ece <code>epsilon</code> \u7684\u8f83\u4f4e\u503c\u5f00\u59cb\uff0c\u7136\u540e\u4e0a\u5347\u5230\u63a5\u8fd1 1 \u53ef\u80fd\u662f\u6709\u610f\u4e49\u7684\u3002</p> </li> </ul> <p>\u4efb\u52a1 1\uff1a\u73a9\u8f6c\u8d85\u53c2\u6570\u503c\uff0c\u770b\u770b\u662f\u5426\u53ef\u4ee5\u83b7\u5f97\u66f4\u9ad8\u7684\u7d2f\u79ef\u5956\u52b1\u3002\u4f60\u8d85\u8fc7 195 \u4e86\u5417\uff1f</p> <p>\u4efb\u52a1 2\uff1a\u8981\u6b63\u5f0f\u89e3\u51b3\u95ee\u9898\uff0c\u4f60\u9700\u8981\u5728 100 \u6b21\u8fde\u7eed\u8fd0\u884c\u4e2d\u83b7\u5f97 195 \u7684\u5e73\u5747\u5956\u52b1\u3002\u5728\u57f9\u8bad\u671f\u95f4\u8861\u91cf\u5e76\u786e\u4fdd\u4f60\u5df2\u7ecf\u6b63\u5f0f\u89e3\u51b3\u4e86\u95ee\u9898\uff01</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_8","title":"\u5728\u884c\u52a8\u4e2d\u770b\u5230\u7ed3\u679c","text":"<p>\u5b9e\u9645\u770b\u770b\u53d7\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u884c\u4e3a\u4f1a\u5f88\u6709\u8da3\u3002\u8ba9\u6211\u4eec\u8fd0\u884c\u6a21\u62df\u5e76\u9075\u5faa\u4e0e\u8bad\u7ec3\u671f\u95f4\u76f8\u540c\u7684\u52a8\u4f5c\u9009\u62e9\u7b56\u7565\uff0c\u6839\u636e Q-Table \u4e2d\u7684\u6982\u7387\u5206\u5e03\u8fdb\u884c\u91c7\u6837\uff1a\uff08\u4ee3\u7801\u5757 13\uff09</p> <pre><code>obs = env.reset()\ndone = False\nwhile not done:\n   s = discretize(obs)\n   env.render()\n   v = probs(np.array(qvalues(s)))\n   a = random.choices(actions,weights=v)[0]\n   obs,_,done,_ = env.step(a)\nenv.close()\n</code></pre> <p>\u4f60\u5e94\u8be5\u4f1a\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a</p> <p></p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_9","title":"\ud83d\ude80\u6311\u6218","text":"<p>\u4efb\u52a1 3\uff1a\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u662f Q-Table \u7684\u6700\u7ec8\u526f\u672c\uff0c\u5b83\u53ef\u80fd\u4e0d\u662f\u6700\u597d\u7684\u3002\u8bf7\u8bb0\u4f4f\uff0c\u6211\u4eec\u5df2\u5c06\u6027\u80fd\u6700\u4f73\u7684 Q-Table \u5b58\u50a8\u5230 <code>Qbest</code> \u53d8\u91cf\u4e2d\uff01\u901a\u8fc7\u5c06 <code>Qbest</code> \u590d\u5236\u5230 <code>Q</code> \u6765\u5c1d\u8bd5\u4f7f\u7528\u6027\u80fd\u6700\u4f73\u7684 Q-Table \u7684\u76f8\u540c\u793a\u4f8b\uff0c\u770b\u770b\u4f60\u662f\u5426\u6ce8\u610f\u5230\u5dee\u5f02\u3002</p> <p>\u4efb\u52a1 4\uff1a\u8fd9\u91cc\u6211\u4eec\u4e0d\u662f\u5728\u6bcf\u4e00\u6b65\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\uff0c\u800c\u662f\u7528\u76f8\u5e94\u7684\u6982\u7387\u5206\u5e03\u8fdb\u884c\u91c7\u6837\u3002\u59cb\u7ec8\u9009\u62e9\u5177\u6709\u6700\u9ad8 Q-Table \u503c\u7684\u6700\u4f73\u52a8\u4f5c\u662f\u5426\u66f4\u6709\u610f\u4e49\uff1f\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 <code>np.argmax</code> \u51fd\u6570\u627e\u51fa\u5bf9\u5e94\u4e8e\u8f83\u9ad8 Q-Table \u503c\u7684\u52a8\u4f5c\u7f16\u53f7\u6765\u5b8c\u6210\u3002\u5b9e\u65bd\u8fd9\u4e2a\u7b56\u7565\uff0c\u770b\u770b\u5b83\u662f\u5426\u80fd\u6539\u5584\u5e73\u8861\u3002</p>"},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_10","title":"\u8bfe\u540e\u6d4b\u9a8c","text":""},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_11","title":"\u4f5c\u4e1a\uff1a\u8bad\u7ec3\u5c71\u5730\u8f66","text":""},{"location":"8-Reinforcement/2-Gym/README.zh-cn/#_12","title":"\u7ed3\u8bba","text":"<p>\u6211\u4eec\u73b0\u5728\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u8bad\u7ec3\u667a\u80fd\u4f53\u4ee5\u53d6\u5f97\u826f\u597d\u7684\u7ed3\u679c\uff0c\u53ea\u9700\u4e3a\u5b83\u4eec\u63d0\u4f9b\u4e00\u4e2a\u5b9a\u4e49\u6e38\u620f\u6240\u9700\u72b6\u6001\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u4e3a\u5b83\u4eec\u63d0\u4f9b\u667a\u80fd\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u7684\u673a\u4f1a\u3002\u6211\u4eec\u5df2\u7ecf\u6210\u529f\u5730\u5c06 Q-Learning \u7b97\u6cd5\u5e94\u7528\u4e8e\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u7684\u60c5\u51b5\uff0c\u4f46\u5177\u6709\u79bb\u6563\u52a8\u4f5c\u3002</p> <p>\u7814\u7a76\u52a8\u4f5c\u72b6\u6001\u4e5f\u662f\u8fde\u7eed\u7684\u60c5\u51b5\u4ee5\u53ca\u89c2\u5bdf\u7a7a\u95f4\u590d\u6742\u5f97\u591a\u7684\u60c5\u51b5\u4e5f\u5f88\u91cd\u8981\uff0c\u4f8b\u5982\u6765\u81ea Atari \u6e38\u620f\u5c4f\u5e55\u7684\u56fe\u50cf\u3002\u5728\u90a3\u4e9b\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u5f80\u5f80\u9700\u8981\u4f7f\u7528\u66f4\u5f3a\u5927\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u6bd4\u5982\u795e\u7ecf\u7f51\u7edc\uff0c\u624d\u80fd\u53d6\u5f97\u597d\u7684\u6548\u679c\u3002\u8fd9\u4e9b\u66f4\u9ad8\u7ea7\u7684\u4e3b\u9898\u5728\u6211\u4eec\u5373\u5c06\u63a8\u51fa\u7684\u66f4\u9ad8\u7ea7 AI \u8bfe\u7a0b\u7684\u4e3b\u9898\u4e2d\u3002</p>"},{"location":"8-Reinforcement/2-Gym/assignment/","title":"Train Mountain Car","text":"<p>OpenAI Gym has been designed in such a way that all environments provide the same API - i.e. the same methods <code>reset</code>, <code>step</code> and <code>render</code>, and the same abstractions of action space and observation space. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.</p>"},{"location":"8-Reinforcement/2-Gym/assignment/#a-mountain-car-environment","title":"A Mountain Car Environment","text":"<p>Mountain Car environment contains a car stuck in a valley:</p> <p></p> <p>The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:</p> Value Meaning 0 Accelerate to the left 1 Do not accelerate 2 Accelerate to the right <p>The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.</p> <p>Observation space consists of just two values:</p> Num Observation Min Max 0 Car Position -1.2 0.6 1 Car Velocity -0.07 0.07 <p>Reward system for the mountain car is rather tricky:</p> <ul> <li>Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.</li> <li>Reward of -1 is awarded if the position of the agent is less than 0.5.</li> </ul> <p>Episode terminates if the car position is more than 0.5, or episode length is greater than 200.</p>"},{"location":"8-Reinforcement/2-Gym/assignment/#instructions","title":"Instructions","text":"<p>Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing notebook.ipynb code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.</p> <p>Note: Hyperparameters adjustment is likely to be needed to make algorithm converge. </p>"},{"location":"8-Reinforcement/2-Gym/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement Q-Learning algorithm is successfully adapted from CartPole example, with minimal code modifications, which is able to solve the problem of capturing the flag under 200 steps. A new Q-Learning algorithm has been adopted from the Internet, but is well-documented; or existing algorithm adopted, but does not reach desired results Student was not able to successfully adopt any algorithm, but has mede substantial steps towards solution (implemented state discretization, Q-Table data structure, etc.)"},{"location":"8-Reinforcement/2-Gym/assignment.zh-cn/","title":"\u8fde\u7eed\u5c71\u5730\u8f66","text":"<p>OpenAI Gym \u7684\u8bbe\u8ba1\u65b9\u5f0f\u662f\u6240\u6709\u73af\u5883\u90fd\u63d0\u4f9b\u76f8\u540c\u7684 API - \u5373\u76f8\u540c\u7684\u65b9\u6cd5 <code>reset</code>\u3001<code>step</code> \u548c <code>render</code>\uff0c\u4ee5\u53ca\u76f8\u540c\u7684\u62bd\u8c61\u52a8\u4f5c\u7a7a\u95f4\u548c\u89c2\u5bdf\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u5e94\u8be5\u53ef\u4ee5\u901a\u8fc7\u6700\u5c11\u7684\u4ee3\u7801\u66f4\u6539\u4f7f\u76f8\u540c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u9002\u5e94\u4e0d\u540c\u7684\u73af\u5883\u3002</p>"},{"location":"8-Reinforcement/2-Gym/assignment.zh-cn/#_2","title":"\u5c71\u5730\u8f66\u73af\u5883","text":"<p>\u5c71\u5730\u8f66\u73af\u5883 \u5305\u542b\u4e00\u8f86\u5361\u5728\u5c71\u8c37\u4e2d\u7684\u6c7d\u8f66\uff1a</p> <p></p> <p>\u76ee\u6807\u662f\u901a\u8fc7\u5728\u6bcf\u4e00\u6b65\u6267\u884c\u4ee5\u4e0b\u4efb\u4e00\u64cd\u4f5c\u6765\u8d70\u51fa\u5c71\u8c37\u5e76\u5f97\u5230\u65d7\u5e1c\uff1a</p> \u503c \u542b\u4e49 0 \u5411\u5de6\u52a0\u901f 1 \u4e0d\u52a0\u901f 2 \u5411\u53f3\u52a0\u901f <p>\u7136\u800c\uff0c\u8fd9\u4e2a\u95ee\u9898\u7684\u4e3b\u8981\u6280\u5de7\u662f\uff0c\u6c7d\u8f66\u7684\u5f15\u64ce\u4e0d\u591f\u5f3a\u5927\uff0c\u65e0\u6cd5\u4e00\u6b21\u6027\u7ffb\u8d8a\u8fd9\u5ea7\u5c71\u3002\u56e0\u6b64\uff0c\u6210\u529f\u7684\u552f\u4e00\u65b9\u6cd5\u662f\u6765\u56de\u9a71\u52a8\u4ee5\u79ef\u805a\u52a8\u529b\u3002</p> <p>\u89c2\u5bdf\u7a7a\u95f4\u4ec5\u5305\u542b\u4e24\u4e2a\u503c\uff1a</p> \u6570\u91cf \u89c2\u5bdf \u6700\u5c0f \u6700\u5927 0 \u8f66\u4f4d\u7f6e -1.2 0.6 1 \u8f66\u901f\u5ea6 -0.07 0.07 <p>\u5c71\u5730\u8f66\u7684\u5956\u52b1\u7cfb\u7edf\u76f8\u5f53\u68d8\u624b\uff1a</p> <ul> <li>\u5982\u679c\u5c71\u5730\u8f66\u5230\u8fbe\u5c71\u9876\u7684\u6807\u5fd7\uff08\u4f4d\u7f6e = 0.5\uff09\u5219\u5956\u52b1 0\u3002</li> <li>\u5982\u679c\u4ee3\u7406\u7684\u4f4d\u7f6e\u5c0f\u4e8e 0.5\uff0c\u5219\u5956\u52b1 -1\u3002</li> </ul> <p>\u5982\u679c\u6c7d\u8f66\u4f4d\u7f6e\u5927\u4e8e 0.5 \u6216\u6b65\u9aa4\u957f\u5ea6\u5927\u4e8e 200\uff0c\u5219\u6e38\u620f\u7ec8\u6b62\u3002</p>"},{"location":"8-Reinforcement/2-Gym/assignment.zh-cn/#_3","title":"\u8bf4\u660e","text":"<p>\u8c03\u6574\u6211\u4eec\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u89e3\u51b3\u5c71\u5730\u8f66\u95ee\u9898\u3002\u4ece\u73b0\u6709\u7684 notebook.ipynb\u4ee3\u7801\u5f00\u59cb\uff0c\u66ff\u6362\u65b0\u73af\u5883\uff0c\u66f4\u6539\u72b6\u6001\u79bb\u6563\u5316\u51fd\u6570\uff0c\u5e76\u5c1d\u8bd5\u4f7f\u73b0\u6709\u7b97\u6cd5\u4ee5\u6700\u5c11\u7684\u4ee3\u7801\u4fee\u6539\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\u6765\u4f18\u5316\u7ed3\u679c\u3002</p> <p>\u6ce8\u610f\uff1a\u53ef\u80fd\u9700\u8981\u8c03\u6574\u8d85\u53c2\u6570\u4ee5\u4f7f\u7b97\u6cd5\u6536\u655b\u3002</p>"},{"location":"8-Reinforcement/2-Gym/assignment.zh-cn/#_4","title":"\u8bc4\u5224\u6807\u51c6","text":"\u6807\u51c6 \u4f18\u79c0 \u4e2d\u89c4\u4e2d\u77e9 \u4ecd\u9700\u52aa\u529b Q-Learning \u7b97\u6cd5\u6210\u529f\u5730\u6539\u7f16\u81ea CartPole \u793a\u4f8b\uff0c\u4ee3\u7801\u4fee\u6539\u6700\u5c11\uff0c\u80fd\u591f\u89e3\u51b3 200 \u6b65\u4ee5\u4e0b\u5f97\u5230\u65d7\u5e1c\u7684\u95ee\u9898\u3002 \u4e00\u79cd\u65b0\u7684 Q-Learning \u7b97\u6cd5\u5df2\u4ece Internet \u4e0a\u91c7\u7528\uff0c\u4f46\u6709\u636e\u53ef\u67e5\uff1b\u6216\u91c7\u7528\u73b0\u6709\u7b97\u6cd5\uff0c\u4f46\u672a\u8fbe\u5230\u9884\u671f\u6548\u679c \u5b66\u751f\u65e0\u6cd5\u6210\u529f\u91c7\u7528\u4efb\u4f55\u7b97\u6cd5\uff0c\u4f46\u5df2\u7ecf\u8fc8\u51fa\u4e86\u89e3\u51b3\u95ee\u9898\u7684\u5b9e\u8d28\u6027\u6b65\u9aa4\uff08\u5b9e\u73b0\u4e86\u72b6\u6001\u79bb\u6563\u5316\u3001Q-Table \u6570\u636e\u7ed3\u6784\u7b49\uff09"},{"location":"8-Reinforcement/2-Gym/solution/Julia/","title":"Index","text":"<p>This is a temporary placeholder</p>"},{"location":"8-Reinforcement/2-Gym/solution/R/","title":"Index","text":"<p>this is a temporary placeholder</p>"},{"location":"9-Real-World/","title":"Postscript: Real world applications of classic machine learning","text":"<p>In this section of the curriculum, you will be introduced to some real-world applications of classical ML. We have scoured the internet to find whitepapers and articles about applications that have used these strategies, avoiding neural networks, deep learning and AI as much as possible. Learn about how ML is used in business systems, ecological applications, finance, arts and culture, and more.</p> <p></p> <p>Photo by Alexis Fauvet on Unsplash</p>"},{"location":"9-Real-World/#lesson","title":"Lesson","text":"<ol> <li>Real-World Applications for ML</li> <li>Model Debugging in Machine Learning using Responsible AI dashboard components</li> </ol>"},{"location":"9-Real-World/#credits","title":"Credits","text":"<p>\"Real-World Applications\" was written by a team of folks, including Jen Looper and Ornella Altunyan.</p> <p>\"Model Debugging in Machine Learning using Responsible AI dashboard components\" was written by Ruth Yakubu</p>"},{"location":"9-Real-World/README.zh-cn/","title":"\u9644\u8a00\uff1a\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u5e94\u7528","text":"<p>\u5728\u8bfe\u7a0b\u7684\u8fd9\u4e00\u7ae0\u8282\u4e2d\uff0c\u4f60\u5c06\u4f1a\u4e86\u89e3\u4e00\u4e9b\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u73b0\u5b9e\u5e94\u7528\u3002\u6211\u4eec\u5728\u7f51\u7edc\u4e0a\u627e\u904d\u4e86\u6d89\u53ca\u8bfe\u7a0b\u4e2d\u8fd9\u4e9b\u6280\u672f\u7684\u5e94\u7528\u7684\u767d\u76ae\u4e66\u548c\u6587\u7ae0\uff0c\u4ece\u4e2d\u5c3d\u529b\u6392\u9664\u4e86\u795e\u7ecf\u7f51\u7edc\u3001\u6df1\u5ea6\u5b66\u4e60\u548cAI\u3002\u8ba9\u6211\u4eec\u4e00\u8d77\u6765\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u662f\u5982\u4f55\u88ab\u5e94\u7528\u5728\u5546\u4e1a\u7cfb\u7edf\u3001\u751f\u6001\u5e94\u7528\u3001\u91d1\u878d\u3001\u827a\u672f\u6587\u5316\u548c\u5176\u4ed6\u9886\u57df\u4e2d\u5427\u3002</p> <p></p> <p>\u7167\u7247\u7531 Alexis Fauvet \u62cd\u6444\u5e76\u53d1\u5e03\u5728 Unsplash \u5e73\u53f0</p>"},{"location":"9-Real-World/README.zh-cn/#_2","title":"\u8bfe\u7a0b\u5b89\u6392","text":"<ol> <li>\u673a\u5668\u5b66\u4e60\u7684\u73b0\u5b9e\u5e94\u7528</li> </ol>"},{"location":"9-Real-World/README.zh-cn/#_3","title":"\u81f4\u8c22","text":"<p>\"\u673a\u5668\u5b66\u4e60\u7684\u73b0\u5b9e\u5e94\u7528\" \u7531 Jen Looper \u548c Ornella Altunyan \u4e24\u4eba\u7684\u56e2\u961f\u5171\u540c\u64b0\u5199.</p>"},{"location":"9-Real-World/1-Applications/","title":"Postscript: Machine learning in the real world","text":"<p>Sketchnote by Tomomi Imura</p> <p>In this curriculum, you have learned many ways to prepare data for training and create machine learning models. You built a series of classic regression, clustering, classification, natural language processing, and time series models. Congratulations! Now, you might be wondering what it's all for... what are the real world applications for these models?</p> <p>While a lot of interest in industry has been garnered by AI, which usually leverages deep learning, there are still valuable applications for classical machine learning models. You might even use some of these applications today! In this lesson, you'll explore how eight different industries and subject-matter domains use these types of models to make their applications more performant, reliable, intelligent, and valuable to users.</p>"},{"location":"9-Real-World/1-Applications/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"9-Real-World/1-Applications/#finance","title":"\ud83d\udcb0 Finance","text":"<p>The finance sector offers many opportunities for machine learning. Many problems in this area lend themselves to be modeled and solved by using ML.</p>"},{"location":"9-Real-World/1-Applications/#credit-card-fraud-detection","title":"Credit card fraud detection","text":"<p>We learned about k-means clustering earlier in the course, but how can it be used to solve problems related to credit card fraud?</p> <p>K-means clustering comes in handy during a credit card fraud detection technique called outlier detection. Outliers, or deviations in observations about a set of data, can tell us if a credit card is being used in a normal capacity or if something unusual is going on. As shown in the paper linked below, you can sort credit card data using a k-means clustering algorithm and assign each transaction to a cluster based on how much of an outlier it appears to be. Then, you can evaluate the riskiest clusters for fraudulent versus legitimate transactions. Reference</p>"},{"location":"9-Real-World/1-Applications/#wealth-management","title":"Wealth management","text":"<p>In wealth management, an individual or firm handles investments on behalf of their clients. Their job is to sustain and grow wealth in the long-term, so it is essential to choose investments that perform well.</p> <p>One way to evaluate how a particular investment performs is through statistical regression. Linear regression is a valuable tool for understanding how a fund performs relative to some benchmark. We can also deduce whether or not the results of the regression are statistically significant, or how much they would affect a client's investments. You could even further expand your analysis using multiple regression, where additional risk factors can be taken into account. For an example of how this would work for a specific fund, check out the paper below on evaluating fund performance using regression. Reference</p>"},{"location":"9-Real-World/1-Applications/#education","title":"\ud83c\udf93 Education","text":"<p>The educational sector is also a very interesting area where ML can be applied. There are interesting problems to be tackled such as detecting cheating on tests or essays or managing bias, unintentional or not, in the correction process.</p>"},{"location":"9-Real-World/1-Applications/#predicting-student-behavior","title":"Predicting student behavior","text":"<p>Coursera, an online open course provider, has a great tech blog where they discuss many engineering decisions. In this case study, they plotted a regression line to try to explore any correlation between a low NPS (Net Promoter Score) rating and course retention or drop-off. Reference</p>"},{"location":"9-Real-World/1-Applications/#mitigating-bias","title":"Mitigating bias","text":"<p>Grammarly, a writing assistant that checks for spelling and grammar errors, uses sophisticated natural language processing systems throughout its products. They published an interesting case study in their tech blog about how they dealt with gender bias in machine learning, which you learned about in our introductory fairness lesson. Reference</p>"},{"location":"9-Real-World/1-Applications/#retail","title":"\ud83d\udc5c Retail","text":"<p>The retail sector can definitely benefit from the use of ML, with everything from creating a better customer journey to stocking inventory in an optimal way.</p>"},{"location":"9-Real-World/1-Applications/#personalizing-the-customer-journey","title":"Personalizing the customer journey","text":"<p>At Wayfair, a company that sells home goods like furniture, helping customers find the right products for their taste and needs is paramount. In this article, engineers from the company describe how they use ML and NLP to \"surface the right results for customers\". Notably, their Query Intent Engine has been built to use entity extraction, classifier training, asset and opinion extraction, and sentiment tagging on customer reviews. This is a classic use case of how NLP works in online retail. Reference</p>"},{"location":"9-Real-World/1-Applications/#inventory-management","title":"Inventory management","text":"<p>Innovative, nimble companies like StitchFix, a box service that ships clothing to consumers, rely heavily on ML for recommendations and inventory management. Their styling teams work together with their merchandising teams, in fact: \"one of our data scientists tinkered with a genetic algorithm and applied it to apparel to predict what would be a successful piece of clothing that doesn't exist today. We brought that to the merchandise team and now they can use that as a tool.\" Reference</p>"},{"location":"9-Real-World/1-Applications/#health-care","title":"\ud83c\udfe5 Health Care","text":"<p>The health care sector can leverage ML to optimize research tasks and also logistic problems like readmitting patients or stopping diseases from spreading.</p>"},{"location":"9-Real-World/1-Applications/#managing-clinical-trials","title":"Managing clinical trials","text":"<p>Toxicity in clinical trials is a major concern to drug makers. How much toxicity is tolerable? In this study, analyzing various clinical trial methods led to the development of a new approach for predicting the odds of clinical trial outcomes. Specifically, they were able to use random forest to produce a classifier that is able to distinguish between groups of drugs. Reference</p>"},{"location":"9-Real-World/1-Applications/#hospital-readmission-management","title":"Hospital readmission management","text":"<p>Hospital care is costly, especially when patients have to be readmitted. This paper discusses a company that uses ML to predict readmission potential using clustering algorithms. These clusters help analysts to \"discover groups of readmissions that may share a common cause\". Reference</p>"},{"location":"9-Real-World/1-Applications/#disease-management","title":"Disease management","text":"<p>The recent pandemic has shone a bright light on the ways that machine learning can aid in stopping the spread of disease. In this article, you'll recognize the use of ARIMA, logistic curves, linear regression, and SARIMA. \"This work is an attempt to calculate the rate of spread of this virus and thus to predict the deaths, recoveries, and confirmed cases, so that it may help us to prepare better and survive.\" Reference</p>"},{"location":"9-Real-World/1-Applications/#ecology-and-green-tech","title":"\ud83c\udf32 Ecology and Green Tech","text":"<p>Nature and ecology consists of many sensitive systems where the interplay between animals and nature come into focus. It's important to be able to measure these systems accurately and act appropriately if something happens, like a forest fire or a drop in the animal population.</p>"},{"location":"9-Real-World/1-Applications/#forest-management","title":"Forest management","text":"<p>You learned about Reinforcement Learning in previous lessons. It can be very useful when trying to predict patterns in nature. In particular, it can be used to track ecological problems like forest fires and the spread of invasive species. In Canada, a group of researchers used Reinforcement Learning to build forest wildfire dynamics models from satellite images. Using an innovative \"spatially spreading process (SSP)\", they envisioned a forest fire as \"the agent at any cell in the landscape.\" \"The set of actions the fire can take from a location at any point in time includes spreading north, south, east, or west or not spreading.</p> <p>This approach inverts the usual RL setup since the dynamics of the corresponding Markov Decision Process (MDP) is a known function for immediate wildfire spread.\" Read more about the classic algorithms used by this group at the link below. Reference</p>"},{"location":"9-Real-World/1-Applications/#motion-sensing-of-animals","title":"Motion sensing of animals","text":"<p>While deep learning has created a revolution in visually tracking animal movements (you can build your own polar bear tracker here), classic ML still has a place in this task.</p> <p>Sensors to track movements of farm animals and IoT make use of this type of visual processing, but more basic ML techniques are useful to preprocess data. For example, in this paper, sheep postures were monitored and analyzed using various classifier algorithms. You might recognize the ROC curve on page 335. Reference</p>"},{"location":"9-Real-World/1-Applications/#energy-management","title":"\u26a1\ufe0f Energy Management","text":"<p>In our lessons on time series forecasting, we invoked the concept of smart parking meters to generate revenue for a town based on understanding supply and demand. This article discusses in detail how clustering, regression and time series forecasting combined to help predict future energy use in Ireland, based off of smart metering. Reference</p>"},{"location":"9-Real-World/1-Applications/#insurance","title":"\ud83d\udcbc Insurance","text":"<p>The insurance sector is another sector that uses ML to construct and optimize viable financial and actuarial models. </p>"},{"location":"9-Real-World/1-Applications/#volatility-management","title":"Volatility Management","text":"<p>MetLife, a life insurance provider, is forthcoming with the way they analyze and mitigate volatility in their financial models. In this article you'll notice binary and ordinal classification visualizations. You'll also discover forecasting visualizations. Reference</p>"},{"location":"9-Real-World/1-Applications/#arts-culture-and-literature","title":"\ud83c\udfa8 Arts, Culture, and Literature","text":"<p>In the arts, for example in journalism, there are many interesting problems. Detecting fake news is a huge problem as it has been proven to influence the opinion of people and even to topple democracies. Museums can also benefit from using ML in everything from finding links between artifacts to resource planning.</p>"},{"location":"9-Real-World/1-Applications/#fake-news-detection","title":"Fake news detection","text":"<p>Detecting fake news has become a game of cat and mouse in today's media. In this article, researchers suggest that a system combining several of the ML techniques we have studied can be tested and the best model deployed: \"This system is based on natural language processing to extract features from the data and then these features are used for the training of machine learning classifiers such as Naive Bayes,  Support Vector Machine (SVM), Random Forest (RF), Stochastic Gradient Descent (SGD), and Logistic Regression(LR).\" Reference</p> <p>This article shows how combining different ML domains can produce interesting results that can help stop fake news from spreading and creating real damage; in this case, the impetus was the spread of rumors about COVID treatments that incited mob violence.</p>"},{"location":"9-Real-World/1-Applications/#museum-ml","title":"Museum ML","text":"<p>Museums are at the cusp of an AI revolution in which cataloging and digitizing collections and finding links between artifacts is becoming easier as technology advances. Projects such as In Codice Ratio are helping unlock the mysteries of inaccessible collections such as the Vatican Archives. But, the business aspect of museums benefits from ML models as well.</p> <p>For example, the Art Institute of Chicago built models to predict what audiences are interested in and when they will attend expositions. The goal is to create individualized and optimized visitor experiences each time the user visits the museum. \"During fiscal 2017, the model predicted attendance and admissions within 1 percent of accuracy, says Andrew Simnick, senior vice president at the Art Institute.\" Reference</p>"},{"location":"9-Real-World/1-Applications/#marketing","title":"\ud83c\udff7 Marketing","text":""},{"location":"9-Real-World/1-Applications/#customer-segmentation","title":"Customer segmentation","text":"<p>The most effective marketing strategies target customers in different ways based on various groupings. In this article, the uses of Clustering algorithms are discussed to support differentiated marketing. Differentiated marketing helps companies improve brand recognition, reach more customers, and make more money. Reference</p>"},{"location":"9-Real-World/1-Applications/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>Identify another sector that benefits from some of the techniques you learned in this curriculum, and discover how it uses ML.</p>"},{"location":"9-Real-World/1-Applications/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"9-Real-World/1-Applications/#review-self-study","title":"Review &amp; Self Study","text":"<p>The Wayfair data science team has several interesting videos on how they use ML at their company. It's worth taking a look!</p>"},{"location":"9-Real-World/1-Applications/#assignment","title":"Assignment","text":"<p>A ML scavenger hunt</p>"},{"location":"9-Real-World/1-Applications/assignment/","title":"A ML Scavenger Hunt","text":""},{"location":"9-Real-World/1-Applications/assignment/#instructions","title":"Instructions","text":"<p>In this lesson, you learned about many real-life use cases that were solved using classical ML. While the use of deep learning, new techniques and tools in AI, and leveraging neural networks has helped speed up the production of tools to help in these sectors, classic ML using the techniques in this curriculum still hold great value.</p> <p>In this assignment, imagine that you are participating in a hackathon. Use what you learned in the curriculum to propose a solution using classic ML to solve a problem in one of the sectors discussed in this lesson. Create a presentation where you discuss how you will implement your idea. Bonus points if you can gather sample data and build a ML model to support your concept!</p>"},{"location":"9-Real-World/1-Applications/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A PowerPoint presentation is presented - bonus for building a model A non-innovative, basic presentation is presented The work is incomplete"},{"location":"9-Real-World/2-Debugging-ML-Models/","title":"Postscript: Model Debugging in Machine Learning using Responsible AI dashboard components","text":""},{"location":"9-Real-World/2-Debugging-ML-Models/#pre-lecture-quiz","title":"Pre-lecture quiz","text":""},{"location":"9-Real-World/2-Debugging-ML-Models/#introduction","title":"Introduction","text":"<p>Machine learning impacts our everyday lives. AI is finding its way into some of the most important systems that affect us as individuals as well as our society, from healthcare, finance, education, and employment. For instance, systems and models are involved in daily decision-making tasks, such as health care diagnoses or detecting fraud. Consequentially, the advancements in AI along with the accelerated adoption are being met with evolving societal expectations and growing regulation in response. We constantly see areas where AI systems continue to miss expectations; they expose new challenges; and governments are starting to regulate AI solutions. So, it is important that these models are analyzed to provide fair, reliable, inclusive, transparent, and accountable outcomes for everyone.</p> <p>In this curriculum, we will look at practical tools that can be used to assess if a model has responsible AI issues. Traditional machine learning debugging techniques tend to be based on quantitative calculations such as aggregated accuracy or average error loss. Imagine what can happen when the data you are using to build these models lacks certain demographics, such as race, gender, political view, religion, or disproportionally represents such demographics. What about when the model's output is interpreted to favor some demographic? This can introduce an over or under representation of these sensitive feature groups resulting in fairness, inclusiveness, or reliability issues from the model. Another factor is, machine learning models are considered black boxes, which makes it hard to understand and explain what drives a model\u2019s prediction. All of these are challenges data scientists and AI developers face when they do not have adequate tools to debug and assess the fairness or trustworthiness of a model.</p> <p>In this lesson, you will learn about debugging your models using:</p> <ul> <li>Error Analysis: identify where in your data distribution the model has high error rates.</li> <li>Model Overview: perform comparative analysis across different data cohorts to discover disparities in your model\u2019s performance metrics.</li> <li>Data Analysis: investigate where there could be over or under representation of your data that can skew your model to favor one data demographic vs another.</li> <li>Feature Importance: understand which features are driving your model\u2019s predictions on a global level or local level.</li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#prerequisite","title":"Prerequisite","text":"<p>As a prerequisite, please take the review Responsible AI tools for developers</p> <p></p>"},{"location":"9-Real-World/2-Debugging-ML-Models/#error-analysis","title":"Error Analysis","text":"<p>Traditional model performance metrics used for measuring accuracy are mostly calculations based on correct vs incorrect predictions. For example, determining that a model is accurate 89% of time with an error loss of 0.001 can be considered a good performance. Errors are often not distributed uniformly in your underlying dataset. You may get an 89% model accuracy score but discover that there are different regions of your data for which the model is failing 42% of the time. The consequence of these failure patterns with certain data groups can lead to fairness or reliability issues. It is essential to understand areas where the model is performing well or not. The data regions where there are a high number of inaccuracies in your model may turn out to be an important data demographic.  </p> <p></p> <p>The Error Analysis component on the RAI dashboard illustrates how model failure is distributed across various cohorts with a tree visualization. This is useful in identifying features or areas where there is a high error rate with your dataset. By seeing where most of the model\u2019s inaccuracies are coming from, you can start investigating the root cause. You can also create cohorts of data to perform analysis on. These data cohorts help in the debugging process to determine why the model performance is good in one cohort, but erroneous in another.   </p> <p></p> <p>The visual indicators on the tree map help in locating the problem areas quicker. For instance, the darker shade of red color a tree node has, the higher the error rate.  </p> <p>Heat map is another visualization functionality that users can use in investigating the error rate using one or two features to find a contributor to the model errors across an entire dataset or cohorts.</p> <p></p> <p>Use error analysis when you need to:</p> <ul> <li>Gain a deep understanding of how model failures are distributed across a dataset and across several input and feature dimensions.</li> <li>Break down the aggregate performance metrics to automatically discover erroneous cohorts to inform your targeted mitigation steps.</li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#model-overview","title":"Model Overview","text":"<p>Evaluating the performance of a machine learning model requires getting a holistic understanding of its behavior. This can be achieved by reviewing more than one metric such as error rate, accuracy, recall, precision, or MAE (Mean Absolute Err) to find disparities among performance metrics.  One performance metric may look great, but inaccuracies can be exposed in another metric. In addition, comparing the metrics for disparities across the entire dataset or cohorts helps shed light on where the model is performing well or not. This is especially important in seeing the model\u2019s performance among sensitive vs insensitive features (e.g., patient race, gender, or age) to uncover potential unfairness the model may have. For example, discovering that the model is more erroneous in a cohort that has sensitive features can reveal potential unfairness the model may have.</p> <p>The Model Overview component of the RAI dashboard helps not just in analyzing the performance metrics of the data representation in a cohort, but it gives users the ability to compare the model\u2019s behavior across different cohorts.</p> <p></p> <p>The component's feature-based analysis functionality allows users to narrow down data subgroups within a particular feature to identify anomalies on a granular level. For example, the dashboard has built-in intelligence to automatically generate cohorts for a user-selected feature (eg., \"time_in_hospital &lt; 3\" or \"time_in_hospital &gt;= 7\"). This enables a user to isolate a particular feature from a larger data group to see if it is a key influencer of the model's erroneous outcomes.</p> <p></p> <p>The Model Overview component supports two classes of disparity metrics:</p> <p>Disparity in model performance: These sets of metrics calculate the disparity (difference) in the values of the selected performance metric across subgroups of data. Here are a few examples:</p> <ul> <li>Disparity in accuracy rate</li> <li>Disparity in error rate</li> <li>Disparity in precision</li> <li>Disparity in recall</li> <li>Disparity in mean absolute error (MAE)</li> </ul> <p>Disparity in selection rate: This metric contains the difference in selection rate (favorable prediction) among subgroups. An example of this is the disparity in loan approval rates. Selection rate means the fraction of data points in each class classified as 1 (in binary classification) or distribution of prediction values (in regression).</p>"},{"location":"9-Real-World/2-Debugging-ML-Models/#data-analysis","title":"Data Analysis","text":"<p>\"If you torture the data long enough, it will confess to anything\" - Ronald Coase</p> <p>This statement sounds extreme, but it is true that data can be manipulated to support any conclusion. Such manipulation can sometimes happen unintentionally. As humans, we all have bias, and it is often difficult to consciously know when you are introducing bias in data. Guaranteeing fairness in AI and machine learning remains a complex challenge. </p> <p>Data is a huge blind spot for traditional model performance metrics. You may have high accuracy scores, but this does not always reflect the underlining data bias that could be in your dataset. For example, if a dataset of employees has 27% of women in executive positions in a company and 73% of men at the same level, a job advertising AI model trained on this data may target mostly a male audience for senior level job positions. Having this imbalance in data skewed the model\u2019s prediction to favor one gender. This reveals a fairness issue where there is a gender bias in the AI model.  </p> <p>The Data Analysis component on the RAI dashboard helps to identify areas where there\u2019s an over- and under-representation in the dataset. It helps users diagnose the root cause of errors and fairness issues introduced from data imbalances or lack of representation of a particular data group. This gives users the ability to visualize datasets based on predicted and actual outcomes, error groups, and specific features. Sometimes discovering an underrepresented data group can also uncover that the model is not learning well, hence the high inaccuracies. Having a model that has data bias is not just a fairness issue but shows that the model is not inclusive or reliable.</p> <p></p> <p>Use data analysis when you need to:</p> <ul> <li>Explore your dataset statistics by selecting different filters to slice your data into different dimensions (also known as cohorts).</li> <li>Understand the distribution of your dataset across different cohorts and feature groups.</li> <li>Determine whether your findings related to fairness, error analysis, and causality (derived from other dashboard components) are a result of your dataset's distribution.</li> <li>Decide in which areas to collect more data to mitigate errors that come from representation issues, label noise, feature noise, label bias, and similar factors.</li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#model-interpretability","title":"Model Interpretability","text":"<p>Machine learning models tend to be black boxes. Understanding which key data features drive a model\u2019s prediction can be challenging.  It is important to provide transparency as to why a model makes a certain prediction. For example, if an AI system predicts that a diabetic patient is at risk of being readmitted back to a hospital in less than 30 days, it should be able to provide supporting data that led to its prediction. Having supporting data indicators brings transparency to help clinicians or hospitals to be able to make well-informed decisions. In addition, being able to explain why a model made a prediction for an individual patient enables accountability with health regulations. When you are using machine learning models in ways that affect people\u2019s lives, it is crucial to understand and explain what influences the behavior of a model. Model explainability and interpretability helps answer questions in scenarios such as:</p> <ul> <li>Model debugging: Why did my model make this mistake? How can I improve my model?</li> <li>Human-AI collaboration: How can I understand and trust the model\u2019s decisions?</li> <li>Regulatory compliance: Does my model satisfy legal requirements?</li> </ul> <p>The Feature Importance component of the RAI dashboard helps you to debug and get a comprehensive understanding of how a model makes predictions. It is also a useful tool for machine learning professionals and decision-makers to explain and show evidence of features influencing a model's behavior for regulatory compliance. Next, users can explore both global and local explanations validate which features drive a model\u2019s prediction. Global explanations lists the top features that affected a model\u2019s overall prediction. Local explanations display which features led to a model\u2019s prediction for an individual case. The ability to evaluate local explanations is also helpful in debugging or auditing a specific case to better understand and interpret why a model made an accurate or inaccurate prediction. </p> <p></p> <ul> <li>Global explanations: For example, what features affect the overall behavior of a diabetes hospital readmission model?</li> <li>Local explanations: For example, why was a diabetic patient over 60 years old with prior hospitalizations predicted to be readmitted or not readmitted within 30 days back to a hospital?</li> </ul> <p>In the debugging process of examining a model\u2019s performance across different cohorts, Feature Importance shows what level of impact a feature has across the cohorts. It helps reveal anomalies when comparing the level of influence the feature has in driving a model\u2019s erroneous predictions. The Feature Importance component can show which values in a feature positively or negatively influenced the model\u2019s outcome. For instance, if a model made an inaccurate prediction, the component gives you the ability to drill down and pinpoint what features or feature values drove the prediction. This level of detail helps not just in debugging but provides transparency and accountability in auditing situations. Finally, the component can help you to identify fairness issues. To illustrate, if a sensitive feature such as ethnicity or gender is highly influential in driving a model\u2019s prediction, this could be a sign of race or gender bias in the model.</p> <p></p> <p>Use interpretability when you need to:</p> <ul> <li>Determine how trustworthy your AI system\u2019s predictions are by understanding what features are most important for the predictions.</li> <li>Approach the debugging of your model by understanding it first and identifying whether the model is using healthy features or merely false correlations.</li> <li>Uncover potential sources of unfairness by understanding whether the model is basing predictions on sensitive features or on features that are highly correlated with them.</li> <li>Build user trust in your model\u2019s decisions by generating local explanations to illustrate their outcomes.</li> <li>Complete a regulatory audit of an AI system to validate models and monitor the impact of model decisions on humans.</li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#conclusion","title":"Conclusion","text":"<p>All the RAI dashboard components are practical tools to help you build machine learning models that are less harmful and more trustworthy to society. It improves the prevention of treats to human rights; discriminating or excluding certain groups to life opportunities; and the risk of physical or psychological injury. It also helps to build trust in your model\u2019s decisions by generating local explanations to illustrate their outcomes. Some of the potential harms can be classified as:</p> <ul> <li>Allocation, if a gender or ethnicity for example is favored over another.</li> <li>Quality of service. If you train the data for one specific scenario but the reality is much more complex, it leads to a poor performing service.</li> <li>Stereotyping. Associating a given group with pre-assigned attributes.</li> <li>Denigration. To unfairly criticize and label something or someone.</li> <li>Over- or under- representation. The idea is that a certain group is not seen in a certain profession, and any service or function that keeps promoting that is contributing to harm.</li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#azure-rai-dashboard","title":"Azure RAI dashboard","text":"<p>Azure RAI dashboard is built on open-source tools developed by the leading academic institutions and organizations including Microsoft are instrumental for data scientists and AI developers to better understand model behavior, discover and mitigate undesirable issues from AI models.</p> <ul> <li> <p>Learn how to use the different components by checking out the RAI dashboard docs.</p> </li> <li> <p>Check out some RAI dashboard sample notebooks for debugging more responsible AI scenarios in Azure Machine Learning. </p> </li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#challenge","title":"\ud83d\ude80 Challenge","text":"<p>To prevent statistical or data biases from being introduced in the first place, we should: </p> <ul> <li>have a diversity of backgrounds and perspectives among the people working on systems </li> <li>invest in datasets that reflect the diversity of our society </li> <li>develop better methods for detecting and correcting bias when it occurs </li> </ul> <p>Think about real-life scenarios where unfairness is evident in model-building and usage. What else should we consider? </p>"},{"location":"9-Real-World/2-Debugging-ML-Models/#post-lecture-quiz","title":"Post-lecture quiz","text":""},{"location":"9-Real-World/2-Debugging-ML-Models/#review-self-study","title":"Review &amp; Self Study","text":"<p>In this lesson, you have learned some of the practical tools of incorporating responsible AI in machine learning.  </p> <p>Watch this workshop to dive deeper into the topics: </p> <ul> <li>Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice by Besmira Nushi and Mehrnoosh Sameki</li> </ul> <p></p> <p>\ud83c\udfa5 Click the image above for a video: Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice by Besmira Nushi and Mehrnoosh Sameki</p> <p>Reference the following materials to learn more about responsible AI and how to build more trustworthy models: </p> <ul> <li> <p>Microsoft\u2019s RAI dashboard tools for debugging ML models: Responsible AI tools resources</p> </li> <li> <p>Explore the Responsible AI toolkit: Github</p> </li> <li> <p>Microsoft\u2019s RAI resource center: Responsible AI Resources \u2013 Microsoft AI </p> </li> <li> <p>Microsoft\u2019s FATE research group: FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research </p> </li> </ul>"},{"location":"9-Real-World/2-Debugging-ML-Models/#assignment","title":"Assignment","text":"<p>Explore RAI dashboard </p>"},{"location":"9-Real-World/2-Debugging-ML-Models/assignment/","title":"Explore Responsible AI (RAI) dashboard","text":""},{"location":"9-Real-World/2-Debugging-ML-Models/assignment/#instructions","title":"Instructions","text":"<p>In this lesson you learned about the RAI dashboard, a suite of components built on \"open-source\" tools  to help data scientists perform error analysis, data exploration, fairness assessment, model interpretability, counterfact/what-if assesments and causal analysis on AI systems.\" For this assignment, explore some of RAI dashboard's sample notebooks and report your findings in a paper or presentation.</p>"},{"location":"9-Real-World/2-Debugging-ML-Models/assignment/#rubric","title":"Rubric","text":"Criteria Exemplary Adequate Needs Improvement A paper or powerpoint presentation is presented discussing RAI dashboard's components, the notebook that was run, and the conclusions drawn from running it A paper is presented without conclusions No paper is presented"}]}